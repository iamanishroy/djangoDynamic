stories = [
    {
        "author": "Aman Dalmia",
        "claps": "2.3K",
        "reading_time": 17,
        "link": "https://blog.usejournal.com/what-i-learned-from-interviewing-at-multiple-ai-companies-and-start-ups-a9620415e4cc?source=---------9----------------",
        "title": "What I learned from interviewing at multiple AI companies and start-ups",
        "text": "Over the past 8 months, I‚Äôve been interviewing at various companies like Google‚Äôs DeepMind, Wadhwani Institute of AI, Microsoft, Ola, Fractal Analytics, and a few others primarily for the roles ‚Äî Data Scientist, Software Engineer & Research Engineer. In the process, not only did I get an opportunity to interact with many great minds, but also had a peek at myself along with a sense of what people really look for when interviewing someone. I believe that if I‚Äôd had this knowledge before, I could have avoided many mistakes and have prepared in a much better manner, which is what the motivation behind this post is, to be able to help someone bag their dream place of work.\nThis post arose from a discussion with one of my juniors on the lack of really fulfilling job opportunities offered through campus placements for people working in AI. Also, when I was preparing, I noticed people using a lot of resources but as per my experience over the past months, I realised that one can do away with a few minimal ones for most roles in AI, all of which I‚Äôm going to mention at the end of the post. I begin with How to get noticed a.k.a. the interview. Then I provide a List of companies and start-ups to apply, which is followed by How to ace that interview. Based on whatever experience I‚Äôve had, I add a section on What we should strive to work for. I conclude with Minimal Resources you need for preparation.\nNOTE: For people who are sitting for campus placements, there are two things I‚Äôd like to add. Firstly, most of what I‚Äôm going to say (except for the last one maybe) is not going to be relevant to you for placements. But, and this is my second point, as I mentioned before, opportunities on campus are mostly in software engineering roles having no intersection with AI. So, this post is specifically meant for people who want to work on solving interesting problems using AI. Also, I want to add that I haven‚Äôt cleared all of these interviews but I guess that‚Äôs the essence of failure ‚Äî it‚Äôs the greatest teacher! The things that I mention here may not all be useful but these are things that I did and there‚Äôs no way for me to know what might have ended up making my case stronger.\nTo be honest, this step is the most important one. What makes off-campus placements so tough and exhausting is getting the recruiter to actually go through your profile among the plethora of applications that they get. Having a contact inside the organisation place a referral for you would make it quite easy, but, in general, this part can be sub-divided into three keys steps:\na) Do the regulatory preparation and do that well: So, with regulatory preparation, I mean ‚Äîa LinkedIn profile, a Github profile, a portfolio website and a well-polished CV. Firstly, your CV should be really neat and concise. Follow this guide by Udacity for cleaning up your CV ‚Äî Resume Revamp. It has everything that I intend to say and I‚Äôve been using it as a reference guide myself. As for the CV template, some of the in-built formats on Overleaf are quite nice. I personally use deedy-resume. Here‚Äôs a preview:\nAs it can be seen, a lot of content can be fit into one page. However, if you really do need more than that, then the format linked above would not work directly. Instead, you can find a modified multi-page format of the same here. The next most important thing to mention is your Github profile. A lot of people underestimate the potential of this, just because unlike LinkedIn, it doesn‚Äôt have a ‚ÄúWho Viewed Your Profile‚Äù option. People DO go through your Github because that‚Äôs the only way they have to validate what you have mentioned in your CV, given that there‚Äôs a lot of noise today with people associating all kinds of buzzwords with their profile. Especially for data science, open-source has a big role to play too with majority of the tools, implementations of various algorithms, lists of learning resources, all being open-sourced. I discuss the benefits of getting involved in Open-Source and how one can start from scratch in an earlier post here. The bare minimum for now should be:\n‚Ä¢ Create a Github account if you don‚Äôt already have one.‚Ä¢ Create a repository for each of the projects that you have done.‚Ä¢ Add documentation with clear instructions on how to run the code‚Ä¢ Add documentation for each file mentioning the role of each function, the meaning of each parameter, proper formatting (e.g. PEP8 for Python) along with a script to automate the previous step (Optional).\nMoving on, the third step is what most people lack, which is having a portfolio website demonstrating their experience and personal projects. Making a portfolio indicates that you are really serious about getting into the field and adds a lot of points to the authenticity factor. Also, you generally have space constraints on your CV and tend to miss out on a lot of details. You can use your portfolio to really delve deep into the details if you want to and it‚Äôs highly recommended to include some sort of visualisation or demonstration of the project/idea. It‚Äôs really easy to create one too as there are a lot of free platforms with drag-and-drop features making the process really painless. I personally use Weebly which is a widely used tool. It‚Äôs better to have a reference to begin with. There are a lot of awesome ones out there but I referred to Deshraj Yadav‚Äôs personal website to begin with making mine:\nFinally, a lot of recruiters and start-ups have nowadays started using LinkedIn as their go-to platform for hiring. A lot of good jobs get posted there. Apart from recruiters, the people working at influential positions are quite active there as well. So, if you can grab their attention, you have a good chance of getting in too. Apart from that, maintaining a clean profile is necessary for people to have the will to connect with you. An important part of LinkedIn is their search tool and for you to show up, you must have the relevant keywords interspersed over your profile. It took me a lot of iterations and re-evaluations to finally have a decent one. Also, you should definitely ask people with or under whom you‚Äôve worked with to endorse you for your skills and add a recommendation talking about their experience of working with you. All of this increases your chance of actually getting noticed. I‚Äôll again point towards Udacity‚Äôs guide for LinkedIn and Github profiles.\nAll this might seem like a lot, but remember that you don‚Äôt need to do it in a single day or even a week or a month. It‚Äôs a process, it never ends. Setting up everything at first would definitely take some effort but once it‚Äôs there and you keep updating it regularly as events around you keep happening, you‚Äôll not only find it to be quite easy, but also you‚Äôll be able to talk about yourself anywhere anytime without having to explicitly prepare for it because you become so aware about yourself.\nb) Stay authentic: I‚Äôve seen a lot of people do this mistake of presenting themselves as per different job profiles. According to me, it‚Äôs always better to first decide what actually interests you, what would you be happy doing and then search for relevant opportunities; not the other way round. The fact that the demand for AI talent surpasses the supply for the same gives you this opportunity. Spending time on your regulatory preparation mentioned above would give you an all-around perspective on yourself and help make this decision easier. Also, you won‚Äôt need to prepare answers to various kinds of questions that you get asked during an interview. Most of them would come out naturally as you‚Äôd be talking about something you really care about.\nc) Networking: Once you‚Äôre done with a), figured out b), Networking is what will actually help you get there. If you don‚Äôt talk to people, you miss out on hearing about many opportunities that you might have a good shot at. It‚Äôs important to keep connecting with new people each day, if not physically, then on LinkedIn, so that upon compounding it after many days, you have a large and strong network. Networking is NOT messaging people to place a referral for you. When I was starting off, I did this mistake way too often until I stumbled upon this excellent article by Mark Meloon, where he talks about the importance of building a real connection with people by offering our help first. Another important step in networking is to get your content out. For example, if you‚Äôre good at something, blog about it and share that blog on Facebook and LinkedIn. Not only does this help others, it helps you as well. Once you have a good enough network, your visibility increases multi-fold. You never know how one person from your network liking or commenting on your posts, may help you reach out to a much broader audience including people who might be looking for someone of your expertise.\nI‚Äôm presenting this list in alphabetical order to avoid the misinterpretation of any specific preference. However, I do place a ‚Äú*‚Äù on the ones that I‚Äôd personally recommend. This recommendation is based on either of the following: mission statement, people, personal interaction or scope of learning. More than 1 ‚Äú*‚Äù is purely based on the 2nd and 3rd factors.\nYour interview begins the moment you have entered the room and a lot of things can happen between that moment and the time when you‚Äôre asked to introduce yourself ‚Äî your body language and the fact that you‚Äôre smiling while greeting them plays a big role, especially when you‚Äôre interviewing for a start-up as culture-fit is something that they extremely care about. You need to understand that as much as the interviewer is a stranger to you, you‚Äôre a stranger to him/her too. So, they‚Äôre probably just as nervous as you are.\nIt‚Äôs important to view the interview as more of a conversation between yourself and the interviewer. Both of you are looking for a mutual fit ‚Äî you are looking for an awesome place to work at and the interviewer is looking for an awesome person (like you) to work with. So, make sure that you‚Äôre feeling good about yourself and that you take the charge of making the initial moments of your conversation pleasant for them. And the easiest way I know how to make that happen is to smile.\nThere are mostly two types of interviews ‚Äî one, where the interviewer has come with come prepared set of questions and is going to just ask you just that irrespective of your profile and the second, where the interview is based on your CV. I‚Äôll start with the second one.\nThis kind of interview generally begins with a ‚ÄúCan you tell me a bit about yourself?‚Äù. At this point, 2 things are a big NO ‚Äî talking about your GPA in college and talking about your projects in detail. An ideal statement should be about a minute or two long, should give a good idea on what have you been doing till now, and it‚Äôs not restricted to academics. You can talk about your hobbies like reading books, playing sports, meditation, etc ‚Äî basically, anything that contributes to defining you. The interviewer will then take something that you talk about here as a cue for his next question, and then the technical part of the interview begins. The motive of this kind of interview is to really check whether whatever you have written on your CV is true or not:\nThere would be a lot of questions on what could be done differently or if ‚ÄúX‚Äù was used instead of ‚ÄúY‚Äù, what would have happened. At this point, it‚Äôs important to know the kind of trade-offs that is usually made during implementation, for e.g. if the interviewer says that using a more complex model would have given better results, then you might say that you actually had less data to work with and that would have lead to overfitting. In one of the interviews, I was given a case-study to work on and it involved designing algorithms for a real-world use case. I‚Äôve noticed that once I‚Äôve been given the green flag to talk about a project, the interviewers really like it when I talk about it in the following flow:\nProblem > 1 or 2 previous approaches > Our approach > Result > Intuition\nThe other kind of interview is really just to test your basic knowledge. Don‚Äôt expect those questions to be too hard. But they would definitely scratch every bit of the basics that you should be having, mainly based around Linear Algebra, Probability, Statistics, Optimisation, Machine Learning and/or Deep Learning. The resources mentioned in the Minimal Resources you need for preparation section should suffice, but make sure that you don‚Äôt miss out one bit among them. The catch here is the amount of time you take to answer those questions. Since these cover the basics, they expect that you should be answering them almost instantly. So, do your preparation accordingly.\nThroughout the process, it‚Äôs important to be confident and honest about what you know and what you don‚Äôt know. If there‚Äôs a question that you‚Äôre certain you have no idea about, say it upfront rather than making ‚ÄúAah‚Äù, ‚ÄúUm‚Äù sounds. If some concept is really important but you are struggling with answering it, the interviewer would generally (depending on how you did in the initial parts) be happy to give you a hint or guide you towards the right solution. It‚Äôs a big plus if you manage to pick their hints and arrive at the correct solution. Try to not get nervous and the best way to avoid that is by, again, smiling.\nNow we come to the conclusion of the interview where the interviewer would ask you if you have any questions for them. It‚Äôs really easy to think that your interview is done and just say that you have nothing to ask. I know many people who got rejected just because of failing at this last question. As I mentioned before, it‚Äôs not only you who is being interviewed. You are also looking for a mutual fit with the company itself. So, it‚Äôs quite obvious that if you really want to join a place, you must have many questions regarding the work culture there or what kind of role are they seeing you in. It can be as simple as being curious about the person interviewing you. There‚Äôs always something to learn from everything around you and you should make sure that you leave the interviewer with the impression that you‚Äôre truly interested in being a part of their team. A final question that I‚Äôve started asking all my interviewers, is for a feedback on what they might want me to improve on. This has helped me tremendously and I still remember every feedback that I‚Äôve gotten which I‚Äôve incorporated into my daily life.\nThat‚Äôs it. Based on my experience, if you‚Äôre just honest about yourself, are competent, truly care about the company you‚Äôre interviewing for and have the right mindset, you should have ticked all the right boxes and should be getting a congratulatory mail soon üòÑ\nWe live in an era full of opportunities and that applies to anything that you love. You just need to strive to become the best at it and you will find a way to monetise it. As Gary Vaynerchuk (just follow him already) says:\nThis is a great time to be working in AI and if you‚Äôre truly passionate about it, you have so much that you can do with AI. You can empower so many people that have always been under-represented. We keep nagging about the problems surrounding us, but there‚Äôs been never such a time where common people like us can actually do something about those problems, rather than just complaining. Jeffrey Hammerbacher (Founder, Cloudera) had famously said:\nWe can do so much with AI than we can ever imagine. There are many extremely challenging problems out there which require incredibly smart people like you to put your head down on and solve. You can make many lives better. Time to let go of what is ‚Äúcool‚Äù, or what would ‚Äúlook good‚Äù. THINK and CHOOSE wisely.\nAny Data Science interview comprises of questions mostly of a subset of the following four categories: Computer Science, Math, Statistics and Machine Learning.\nIf you‚Äôre not familiar with the math behind Deep Learning, then you should consider going over my last post for resources to understand them. However, if you are comfortable, I‚Äôve found that the chapters 2, 3 and 4 of the Deep Learning Book are enough to prepare/revise for theoretical questions during such interviews. I‚Äôve been preparing summaries for a few chapters which you can refer to where I‚Äôve tried to even explain a few concepts that I found challenging to understand at first, in case you are not willing to go through the entire chapters. And if you‚Äôve already done a course on probability, you should be comfortable answering a few numerical as well. For stats, covering these topics should be enough.\nNow, the range of questions here can vary depending on the type of position you are applying for. If it‚Äôs a more traditional Machine Learning based interview where they want to check your basic knowledge in ML, you can complete any one of the following courses:- Machine Learning by Andrew Ng ‚Äî CS 229- Machine Learning course by Caltech Professor Yaser Abu-Mostafa\nImportant topics are: Supervised Learning (Classification, Regression, SVM, Decision Tree, Random Forests, Logistic Regression, Multi-layer Perceptron, Parameter Estimation, Bayes‚Äô Decision Rule), Unsupervised Learning (K-means Clustering, Gaussian Mixture Models), Dimensionality Reduction (PCA).\nNow, if you‚Äôre applying for a more advanced position, there‚Äôs a high chance that you might be questioned on Deep Learning. In that case, you should be very comfortable with Convolutional Neural Networks (CNNs) and/or (depending upon what you‚Äôve worked on) Recurrent Neural Networks (RNNs) and their variants. And by being comfortable, you must know what is the fundamental idea behind Deep Learning, how CNNs/RNNs actually worked, what kind of architectures have been proposed and what has been the motivation behind those architectural changes. Now, there‚Äôs no shortcut for this. Either you understand them or you put enough time to understand them. For CNNs, the recommended resource is Stanford‚Äôs CS 231N and CS 224N for RNNs. I found this Neural Network class by Hugo Larochelle to be really enlightening too. Refer this for a quick refresher too. Udacity coming to the aid here too. By now, you should have figured out that Udacity is a really important place for an ML practitioner. There are not a lot of places working on Reinforcement Learning (RL) in India and I too am not experienced in RL as of now. So, that‚Äôs one thing to add to this post sometime in the future.\nGetting placed off-campus is a long journey of self-realisation. I realise that this has been another long post and I‚Äôm again extremely grateful to you for valuing my thoughts. I hope that this post finds a way of being useful to you and that it helped you in some way to prepare for your next Data Science interview better. If it did, I request you to really think about what I talk about in What we should strive to work for.\nI‚Äôm very thankful to my friends from IIT Guwahati for their helpful feedback, especially Ameya Godbole, Kothapalli Vignesh and Prabal Jain. A majority of what I mention here, like ‚Äúviewing an interview as a conversation‚Äù and ‚Äúseeking feedback from our interviewers‚Äù, arose from multiple discussions with Prabal who has been advising me constantly on how I can improve my interviewing skills.\nThis story is published in Noteworthy, where thousands come every day to learn about the people & ideas shaping the products we love.\nFollow our publication to see more product & design stories featured by the Journal team.\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\nAI Fanatic ‚Ä¢ Math Lover ‚Ä¢ Dreamer\nThe official Journal blog"
    },
    {
        "author": "Sophia Arakelyan",
        "claps": 7,
        "reading_time": 4,
        "link": "https://buzzrobot.com/from-ballerina-to-ai-researcher-part-i-46fce67f809b?source=---------1----------------",
        "title": "From Ballerina to AI Researcher: Part I ‚Äì buZZrobot",
        "text": "Last year, I published the article ‚ÄúFrom Ballerina to AI writer‚Äù where I described how I embraced the technical part of AI without a technical background. But having love and passion for AI, I educated myself and was able to build a neural net classifier and do projects in Deep RL.\nRecently, I‚Äôve become a participant in the OpenAI Scholarship Program (OpenAI is a non-profit that gathers top AI researchers to ensure the safety of AI to benefit humanity). Every week for the next three months I‚Äôll publish blog posts sharing my story of transformation from a person dedicated to 15 years of professional dancing and then writing about tech and AI to actually conducting AI research.\nFinding your true calling ‚Äî the key component of happiness\nMy primary goal with the series of blog posts ‚ÄúFrom Ballerina to AI researcher‚Äù is to show that it‚Äôs never too late to embrace a new field, start over again, and find your true calling. Finding work you love is one of the most important components of happiness - ‚Äî something that you do every day and invest your time in to grow; that makes you feel fulfilled, gives you energy; something that is a refuge for your soul.\nGreat things never come easy. We have to be able to fight to make great things happen. But you can‚Äôt fight for something you don‚Äôt believe in, especially if you don‚Äôt feel like it‚Äôs really important for you and humanity. Finding that thing is a real challenge. I feel lucky that I found my true passion ‚Äî AI. To me, the technology itself and the AI community ‚Äî researchers, scientists, people who dedicate their lives to building the most powerful technology of all time with the mission to benefit humanity and make it safe for us ‚Äî is a great source of energy.\nThe structure of the blog post series\nToday, I‚Äôm giving an overall intro of what I‚Äôm going to cover in my ‚ÄúFrom Ballerina to AI Researcher‚Äù series.\nI‚Äôll dedicate the sequence of blog posts during the OpenAI Scholars program to several aspects of AI technology. I‚Äôll cover those areas that concern me a lot, like AI and automation, bias in ML, dual use of AI, etc.\nAlso, the structure of my posts will include some insights on what I‚Äôm working on right now (the final technical project will be available by the end of August and will be open-sourced).\nI feel very lucky to have Alec Radford, an experienced researcher, as my mentor who guides me in the NLP and NLU research area.\nFirst week of my scholarship\nI‚Äôve dedicated my first week within the program to learning about the Transformer architecture that performs much better on sequential data compared to RNNs, LSTMs.\nThe novelty of the architecture is its multi-head self-attention mechanism. According to the original paper, experiments with the transformer on two machine translation tasks showed the model to be superior in quality while being more parallelizable and requiring significantly less time to train.\nMore concretely, when RNNs or CNNs take a sequence as an input, it goes through sentences word by word, which is a huge obstacle toward parallelization of the process (takes more time to train models). Moreover, if sequences are too long, the model tends to forget the content of distant positions in sequence or mixes it with the following positions‚Äô content ‚Äî this is the fundamental problem in dealing with sequential data. The transformer architecture reduced this problem thanks to the multi-head self-attention mechanism.\nI digged into RNN, LSTM models to catch up with the background information. To that end, I‚Äôve found Andrew Ng‚Äôs course on Deep Learning along with the papers extremely useful. To develop insights regarding the transformer, I went through the following resources: the video by ≈Åukasz Kaiser from Google Brain, one of the model‚Äôs creators; a blog post with very well elaborated content re: the model, ran the code tensor2tensor and the code using the PyTorch framework from this paper to ‚Äúfeel‚Äù the difference between the TF and PyTorch frameworks.\nOverall, the goal within the program is to develop deep comprehension of the NLU research area: challenges, current state of the art; and to formulate and test hypotheses that tackle the most important problems of the field.\nI‚Äôll share more on what I‚Äôm working on in my future articles. Meanwhile, if you have questions/feedback, please leave a comment.\nIf you want to learn more about me, here are my Facebook and Twitter accounts.\nI‚Äôd appreciate your feedback on my posts, such as what topics are most interesting to you that I should consider further coverage on.\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\nFormer ballerina turned AI writer. Fan of sci-fi, astrophysics. Consciousness is the key. Founder of buZZrobot.com\nThe publication aims to cover practical aspects of AI technology, use cases along with interviews with notable people in the AI field."
    },
    {
        "author": "Dr. GP Pulipaka",
        "claps": 2,
        "reading_time": 6,
        "link": "https://medium.com/@gp_pulipaka/3-ways-to-apply-latent-semantic-analysis-on-large-corpus-text-on-macos-terminal-jupyterlab-colab-7b4dc3e1622?source=---------9----------------",
        "title": "3 Ways to Apply Latent Semantic Analysis on Large-Corpus Text on macOS Terminal, JupyterLab, and...",
        "text": "Latent semantic analysis works on large-scale datasets to generate representations to discover the insights through natural language processing. There are different approaches to perform the latent semantic analysis at multiple levels such as document level, phrase level, and sentence level. Primarily semantic analysis can be summarized into lexical semantics and the study of combining individual words into paragraphs or sentences. The lexical semantics classifies and decomposes the lexical items. Applying lexical semantic structures has different contexts to identify the differences and similarities between the words. A generic term in a paragraph or a sentence is hypernym and hyponymy provides the meaning of the relationship between instances of the hyponyms. Homonyms contain similar syntax or similar spelling with similar structuring with different meanings. Homonyms are not related to each other. Book is an example for homonym. It can mean for someone to read something or an act of making a reservation with similar spelling, form, and syntax. However, the definition is different. Polysemy is another phenomenon of the words where a single word could be associated with multiple related senses and distinct meanings. The word polysemy is a Greek word which means many signs. Python provides NLTK library to perform tokenization of the words by chopping the words in larger chunks into phrases or meaningful strings. Processing words through tokenization produce tokens. Word lemmatization converts words from the current inflected form into the base form.\nLatent semantic analysis\nApplying latent semantic analysis on large datasets of text and documents represents the contextual meaning through mathematical and statistical computation methods on large corpus of text. Many times, latent semantic analysis overtook human scores and subject matter tests conducted by humans. The accuracy of latent semantic analysis is high as it reads through machine readable documents and texts at a web scale. Latent semantic analysis is a technique that applies singular value decomposition and principal component analysis (PCA). The document can be represented with Z x Y Matrix A, the rows of the matrix represent the document in the collection. The matrix A can represent numerous hundred thousands of rows and columns on a typical large-corpus text document. Applying singular value decomposition develops a set of operations dubbed matrix decomposition. Natural language processing in Python with NLTK library applies a low-rank approximation to the term-document matrix. Later, the low-rank approximation aids in indexing and retrieving the document known as latent semantic indexing by clustering the number of words in the document.\nBrief overview of linear algebra\nThe A with Z x Y matrix contains the real-valued entries with non-negative values for the term-document matrix. Determining the rank of the matrix comes with the number of linearly independent columns or rows in the the matrix. The rank of A ‚â§ {Z,Y}. A square c x c represented as diagonal matrix where off-diagonal entries are zero. Examining the matrix, if all the c diagonal matrices are one, the identity matrix of the dimension c represented by Ic. For the square Z x Z matrix, A with a vector k which contains not all zeroes, for Œª. The matrix decomposition applies on the square matrix factored into the product of matrices from eigenvectors. This allows to reduce the dimensionality of the words from multi-dimensions to two dimensions to view on the plot. The dimensionality reduction techniques with principal component analysis and singular value decomposition holds critical relevance in natural language processing. The Zipfian nature of the frequency of the words in a document makes it difficult to determine the similarity of the words in a static stage. Hence, eigen decomposition is a by-product of singular value decomposition as the input of the document is highly asymmetrical. The latent semantic analysis is a particular technique in semantic space to parse through the document and identify the words with polysemy with NLKT library. The resources such as punkt and wordnet have to be downloaded from NLTK.\nDeep Learning at scale with Google Colab notebooks\nTraining machine learning or deep learning models on CPUs could take hours and could be pretty expensive in terms of the programming language efficiency with time and energy of the computer resources. Google built Colab Notebooks environment for research and development purposes. It runs entirely on the cloud without requiring any additional hardware or software setup for each machine. It‚Äôs entirely equivalent of a Jupyter notebook that aids the data scientists to share the colab notebooks by storing on Google drive just like any other Google Sheets or documents in a collaborative environment. There are no additional costs associated with enabling GPU at runtime for acceleration on the runtime. There are some challenges of uploading the data into Colab, unlike Jupyter notebook that can access the data directly from the local directory of the machine. In Colab, there are multiple options to upload the files from the local file system or a drive can be mounted to load the data through drive FUSE wrapper.\nOnce this step is complete, it shows the following log without errors:\nThe next step would be generating the authentication tokens to authenticate the Google credentials for the drive and Colab\nIf it shows successful retrieval of access token, then Colab is all set.\nAt this stage, the drive is not mounted yet, it will show false when accessing the contents of the text file.\nOnce the drive is mounted, Colab has access to the datasets from Google drive.\nOnce the files are accessible, the Python can be executed similar to executing in Jupyter environment. Colab notebook also displays the results similar to what we see on Jupyter notebook.\nPyCharm IDE\nThe program can be run compiled on PyCharm IDE environment and run on PyCharm or can be executed from OSX Terminal.\nResults from OSX Terminal\nJupyter Notebook on standalone machine\nJupyter Notebook gives a similar output running the latent semantic analysis on the local machine:\nReferences\nGorrell, G. (2006). Generalized Hebbian Algorithm for Incremental Singular Value Decomposition in Natural Language Processing. Retrieved from https://www.aclweb.org/anthology/E06-1013\nHardeniya, N. (2016). Natural Language Processing: Python and NLTK . Birmingham, England: Packt Publishing.\nLandauer, T. K., Foltz, P. W., Laham, D., & University of Colorado at Boulder (1998). An Introduction to Latent Semantic Analysis. Retrieved from http://lsa.colorado.edu/papers/dp1.LSAintro.pdf\nStackoverflow (2018). Mounting Google Drive on Google Colab. Retrieved from https://stackoverflow.com/questions/50168315/mounting-google-drive-on-google-colab\nStanford University (2009). Matrix decompositions and latent semantic indexing. Retrieved from https://nlp.stanford.edu/IR-book/html/htmledition/matrix-decompositions-and-latent-semantic-indexing-1.html\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\nGanapathi Pulipaka | Founder and CEO @deepsingularity | Bestselling Author | Big data | IoT | Startups | SAP | MachineLearning | DeepLearning | DataScience"
    },
    {
        "author": "Scott Santens",
        "claps": "7.3K",
        "reading_time": 14,
        "link": "https://medium.com/basic-income/deep-learning-is-going-to-teach-us-all-the-lesson-of-our-lives-jobs-are-for-machines-7c6442e37a49?source=tag_archive---------0----------------",
        "title": "Deep Learning Is Going to Teach Us All the Lesson of Our Lives: Jobs Are for Machines",
        "text": "(An alternate version of this article was originally published in the Boston Globe)\nOn December 2nd, 1942, a team of scientists led by Enrico Fermi came back from lunch and watched as humanity created the first self-sustaining nuclear reaction inside a pile of bricks and wood underneath a football field at the University of Chicago. Known to history as Chicago Pile-1, it was celebrated in silence with a single bottle of Chianti, for those who were there understood exactly what it meant for humankind, without any need for words.\nNow, something new has occurred that, again, quietly changed the world forever. Like a whispered word in a foreign language, it was quiet in that you may have heard it, but its full meaning may not have been comprehended. However, it‚Äôs vital we understand this new language, and what it‚Äôs increasingly telling us, for the ramifications are set to alter everything we take for granted about the way our globalized economy functions, and the ways in which we as humans exist within it.\nThe language is a new class of machine learning known as deep learning, and the ‚Äúwhispered word‚Äù was a computer‚Äôs use of it to seemingly out of nowhere defeat three-time European Go champion Fan Hui, not once but five times in a row without defeat. Many who read this news, considered that as impressive, but in no way comparable to a match against Lee Se-dol instead, who many consider to be one of the world‚Äôs best living Go players, if not the best. Imagining such a grand duel of man versus machine, China‚Äôs top Go player predicted that Lee would not lose a single game, and Lee himself confidently expected to possibly lose one at the most.\nWhat actually ended up happening when they faced off? Lee went on to lose all but one of their match‚Äôs five games. An AI named AlphaGo is now a better Go player than any human and has been granted the ‚Äúdivine‚Äù rank of 9 dan. In other words, its level of play borders on godlike. Go has officially fallen to machine, just as Jeopardy did before it to Watson, and chess before that to Deep Blue.\nSo, what is Go? Very simply, think of Go as Super Ultra Mega Chess. This may still sound like a small accomplishment, another feather in the cap of machines as they continue to prove themselves superior in the fun games we play, but it is no small accomplishment, and what‚Äôs happening is no game.\nAlphaGo‚Äôs historic victory is a clear signal that we‚Äôve gone from linear to parabolic. Advances in technology are now so visibly exponential in nature that we can expect to see a lot more milestones being crossed long before we would otherwise expect. These exponential advances, most notably in forms of artificial intelligence limited to specific tasks, we are entirely unprepared for as long as we continue to insist upon employment as our primary source of income.\nThis may all sound like exaggeration, so let‚Äôs take a few decade steps back, and look at what computer technology has been actively doing to human employment so far:\nLet the above chart sink in. Do not be fooled into thinking this conversation about the automation of labor is set in the future. It‚Äôs already here. Computer technology is already eating jobs and has been since 1990.\nAll work can be divided into four types: routine and nonroutine, cognitive and manual. Routine work is the same stuff day in and day out, while nonroutine work varies. Within these two varieties, is the work that requires mostly our brains (cognitive) and the work that requires mostly our bodies (manual). Where once all four types saw growth, the stuff that is routine stagnated back in 1990. This happened because routine labor is easiest for technology to shoulder. Rules can be written for work that doesn‚Äôt change, and that work can be better handled by machines.\nDistressingly, it‚Äôs exactly routine work that once formed the basis of the American middle class. It‚Äôs routine manual work that Henry Ford transformed by paying people middle class wages to perform, and it‚Äôs routine cognitive work that once filled US office spaces. Such jobs are now increasingly unavailable, leaving only two kinds of jobs with rosy outlooks: jobs that require so little thought, we pay people little to do them, and jobs that require so much thought, we pay people well to do them.\nIf we can now imagine our economy as a plane with four engines, where it can still fly on only two of them as long as they both keep roaring, we can avoid concerning ourselves with crashing. But what happens when our two remaining engines also fail? That‚Äôs what the advancing fields of robotics and AI represent to those final two engines, because for the first time, we are successfully teaching machines to learn.\nI‚Äôm a writer at heart, but my educational background happens to be in psychology and physics. I‚Äôm fascinated by both of them so my undergraduate focus ended up being in the physics of the human brain, otherwise known as cognitive neuroscience. I think once you start to look into how the human brain works, how our mass of interconnected neurons somehow results in what we describe as the mind, everything changes. At least it did for me.\nAs a quick primer in the way our brains function, they‚Äôre a giant network of interconnected cells. Some of these connections are short, and some are long. Some cells are only connected to one other, and some are connected to many. Electrical signals then pass through these connections, at various rates, and subsequent neural firings happen in turn. It‚Äôs all kind of like falling dominoes, but far faster, larger, and more complex. The result amazingly is us, and what we‚Äôve been learning about how we work, we‚Äôve now begun applying to the way machines work.\nOne of these applications is the creation of deep neural networks - kind of like pared-down virtual brains. They provide an avenue to machine learning that‚Äôs made incredible leaps that were previously thought to be much further down the road, if even possible at all. How? It‚Äôs not just the obvious growing capability of our computers and our expanding knowledge in the neurosciences, but the vastly growing expanse of our collective data, aka big data.\nBig data isn‚Äôt just some buzzword. It‚Äôs information, and when it comes to information, we‚Äôre creating more and more of it every day. In fact we‚Äôre creating so much that a 2013 report by SINTEF estimated that 90% of all information in the world had been created in the prior two years. This incredible rate of data creation is even doubling every 1.5 years thanks to the Internet, where in 2015 every minute we were liking 4.2 million things on Facebook, uploading 300 hours of video to YouTube, and sending 350,000 tweets. Everything we do is generating data like never before, and lots of data is exactly what machines need in order to learn to learn. Why?\nImagine programming a computer to recognize a chair. You‚Äôd need to enter a ton of instructions, and the result would still be a program detecting chairs that aren‚Äôt, and not detecting chairs that are. So how did we learn to detect chairs? Our parents pointed at a chair and said, ‚Äúchair.‚Äù Then we thought we had that whole chair thing all figured out, so we pointed at a table and said ‚Äúchair‚Äù, which is when our parents told us that was ‚Äútable.‚Äù This is called reinforcement learning. The label ‚Äúchair‚Äù gets connected to every chair we see, such that certain neural pathways are weighted and others aren‚Äôt. For ‚Äúchair‚Äù to fire in our brains, what we perceive has to be close enough to our previous chair encounters. Essentially, our lives are big data filtered through our brains.\nThe power of deep learning is that it‚Äôs a way of using massive amounts of data to get machines to operate more like we do without giving them explicit instructions. Instead of describing ‚Äúchairness‚Äù to a computer, we instead just plug it into the Internet and feed it millions of pictures of chairs. It can then have a general idea of ‚Äúchairness.‚Äù Next we test it with even more images. Where it‚Äôs wrong, we correct it, which further improves its ‚Äúchairness‚Äù detection. Repetition of this process results in a computer that knows what a chair is when it sees it, for the most part as well as we can. The important difference though is that unlike us, it can then sort through millions of images within a matter of seconds.\nThis combination of deep learning and big data has resulted in astounding accomplishments just in the past year. Aside from the incredible accomplishment of AlphaGo, Google‚Äôs DeepMind AI learned how to read and comprehend what it read through hundreds of thousands of annotated news articles. DeepMind also taught itself to play dozens of Atari 2600 video games better than humans, just by looking at the screen and its score, and playing games repeatedly. An AI named Giraffe taught itself how to play chess in a similar manner using a dataset of 175 million chess positions, attaining International Master level status in just 72 hours by repeatedly playing itself. In 2015, an AI even passed a visual Turing test by learning to learn in a way that enabled it to be shown an unknown character in a fictional alphabet, then instantly reproduce that letter in a way that was entirely indistinguishable from a human given the same task. These are all major milestones in AI.\nHowever, despite all these milestones, when asked to estimate when a computer would defeat a prominent Go player, the answer even just months prior to the announcement by Google of AlphaGo‚Äôs victory, was by experts essentially, ‚ÄúMaybe in another ten years.‚Äù A decade was considered a fair guess because Go is a game so complex I‚Äôll just let Ken Jennings of Jeopardy fame, another former champion human defeated by AI, describe it:\nSuch confounding complexity makes impossible any brute-force approach to scan every possible move to determine the next best move. But deep neural networks get around that barrier in the same way our own minds do, by learning to estimate what feels like the best move. We do this through observation and practice, and so did AlphaGo, by analyzing millions of professional games and playing itself millions of times. So the answer to when the game of Go would fall to machines wasn‚Äôt even close to ten years. The correct answer ended up being, ‚ÄúAny time now.‚Äù\nAny time now. That‚Äôs the new go-to response in the 21st century for any question involving something new machines can do better than humans, and we need to try to wrap our heads around it.\nWe need to recognize what it means for exponential technological change to be entering the labor market space for nonroutine jobs for the first time ever. Machines that can learn mean nothing humans do as a job is uniquely safe anymore. From hamburgers to healthcare, machines can be created to successfully perform such tasks with no need or less need for humans, and at lower costs than humans.\nAmelia is just one AI out there currently being beta-tested in companies right now. Created by IPsoft over the past 16 years, she‚Äôs learned how to perform the work of call center employees. She can learn in seconds what takes us months, and she can do it in 20 languages. Because she‚Äôs able to learn, she‚Äôs able to do more over time. In one company putting her through the paces, she successfully handled one of every ten calls in the first week, and by the end of the second month, she could resolve six of ten calls. Because of this, it‚Äôs been estimated that she can put 250 million people out of a job, worldwide.\nViv is an AI coming soon from the creators of Siri who‚Äôll be our own personal assistant. She‚Äôll perform tasks online for us, and even function as a Facebook News Feed on steroids by suggesting we consume the media she‚Äôll know we‚Äôll like best. In doing all of this for us, we‚Äôll see far fewer ads, and that means the entire advertising industry ‚Äî that industry the entire Internet is built upon ‚Äî stands to be hugely disrupted.\nA world with Amelia and Viv ‚Äî and the countless other AI counterparts coming online soon ‚Äî in combination with robots like Boston Dynamics‚Äô next generation Atlas portends, is a world where machines can do all four types of jobs and that means serious societal reconsiderations. If a machine can do a job instead of a human, should any human be forced at the threat of destitution to perform that job? Should income itself remain coupled to employment, such that having a job is the only way to obtain income, when jobs for many are entirely unobtainable? If machines are performing an increasing percentage of our jobs for us, and not getting paid to do them, where does that money go instead? And what does it no longer buy? Is it even possible that many of the jobs we‚Äôre creating don‚Äôt need to exist at all, and only do because of the incomes they provide? These are questions we need to start asking, and fast.\nFortunately, people are beginning to ask these questions, and there‚Äôs an answer that‚Äôs building up momentum. The idea is to put machines to work for us, but empower ourselves to seek out the forms of remaining work we as humans find most valuable, by simply providing everyone a monthly paycheck independent of work. This paycheck would be granted to all citizens unconditionally, and its name is universal basic income. By adopting UBI, aside from immunizing against the negative effects of automation, we‚Äôd also be decreasing the risks inherent in entrepreneurship, and the sizes of bureaucracies necessary to boost incomes. It‚Äôs for these reasons, it has cross-partisan support, and is even now in the beginning stages of possible implementation in countries like Switzerland, Finland, the Netherlands, and Canada.\nThe future is a place of accelerating changes. It seems unwise to continue looking at the future as if it were the past, where just because new jobs have historically appeared, they always will. The WEF started 2016 off by estimating the creation by 2020 of 2 million new jobs alongside the elimination of 7 million. That‚Äôs a net loss, not a net gain of 5 million jobs. In a frequently cited paper, an Oxford study estimated the automation of about half of all existing jobs by 2033. Meanwhile self-driving vehicles, again thanks to machine learning, have the capability of drastically impacting all economies ‚Äî especially the US economy as I wrote last year about automating truck driving ‚Äî by eliminating millions of jobs within a short span of time.\nAnd now even the White House, in a stunning report to Congress, has put the probability at 83 percent that a worker making less than $20 an hour in 2010 will eventually lose their job to a machine. Even workers making as much as $40 an hour face odds of 31 percent. To ignore odds like these is tantamount to our now laughable ‚Äúduck and cover‚Äù strategies for avoiding nuclear blasts during the Cold War.\nAll of this is why it‚Äôs those most knowledgeable in the AI field who are now actively sounding the alarm for basic income. During a panel discussion at the end of 2015 at Singularity University, prominent data scientist Jeremy Howard asked ‚ÄúDo you want half of people to starve because they literally can‚Äôt add economic value, or not?‚Äù before going on to suggest, ‚ÄùIf the answer is not, then the smartest way to distribute the wealth is by implementing a universal basic income.‚Äù\nAI pioneer Chris Eliasmith, director of the Centre for Theoretical Neuroscience, warned about the immediate impacts of AI on society in an interview with Futurism, ‚ÄúAI is already having a big impact on our economies... My suspicion is that more countries will have to follow Finland‚Äôs lead in exploring basic income guarantees for people.‚Äù\nMoshe Vardi expressed the same sentiment after speaking at the 2016 annual meeting of the American Association for the Advancement of Science about the emergence of intelligent machines, ‚Äúwe need to rethink the very basic structure of our economic system... we may have to consider instituting a basic income guarantee.‚Äù\nEven Baidu‚Äôs chief scientist and founder of Google‚Äôs ‚ÄúGoogle Brain‚Äù deep learning project, Andrew Ng, during an onstage interview at this year‚Äôs Deep Learning Summit, expressed the shared notion that basic income must be ‚Äúseriously considered‚Äù by governments, citing ‚Äúa high chance that AI will create massive labor displacement.‚Äù\nWhen those building the tools begin warning about the implications of their use, shouldn‚Äôt those wishing to use those tools listen with the utmost attention, especially when it‚Äôs the very livelihoods of millions of people at stake? If not then, what about when Nobel prize winning economists begin agreeing with them in increasing numbers?\nNo nation is yet ready for the changes ahead. High labor force non-participation leads to social instability, and a lack of consumers within consumer economies leads to economic instability. So let‚Äôs ask ourselves, what‚Äôs the purpose of the technologies we‚Äôre creating? What‚Äôs the purpose of a car that can drive for us, or artificial intelligence that can shoulder 60% of our workload? Is it to allow us to work more hours for even less pay? Or is it to enable us to choose how we work, and to decline any pay/hours we deem insufficient because we‚Äôre already earning the incomes that machines aren‚Äôt?\nWhat‚Äôs the big lesson to learn, in a century when machines can learn?\nI offer it‚Äôs that jobs are for machines, and life is for people.\nThis article was written on a crowdfunded monthly basic income. If you found value in this article, you can support it along with all my advocacy for basic income with a monthly patron pledge of $1+.\nSpecial thanks to Arjun Banker, Steven Grimm, Larry Cohen, Topher Hunt, Aaron Marcus-Kubitza, Andrew Stern, Keith Davis, Albert Wenger, Richard Just, Chris Smothers, Mark Witham, David Ihnen, Danielle Texeira, Katie Doemland, Paul Wicks, Jan Smole, Joe Esposito, Jack Wagner, Joe Ballou, Stuart Matthews, Natalie Foster, Chris McCoy, Michael Honey, Gary Aranovich, Kai Wong, John David Hodge, Louise Whitmore, Dan O‚ÄôSullivan, Harish Venkatesan, Michiel Dral, Gerald Huff, Susanne Berg, Cameron Ottens, Kian Alavi, Gray Scott, Kirk Israel, Robert Solovay, Jeff Schulman, Andrew Henderson, Robert F. Greene, Martin Jordo, Victor Lau, Shane Gordon, Paolo Narciso, Johan Grahn, Tony DeStefano, Erhan Altay, Bryan Herdliska, Stephane Boisvert, Dave Shelton, Rise & Shine PAC, Luke Sampson, Lee Irving, Kris Roadruck, Amy Shaffer, Thomas Welsh, Olli NiinimaÃàki, Casey Young, Elizabeth Balcar, Masud Shah, Allen Bauer, all my other funders for their support, and my amazing partner, Katie Smith.\nScott Santens writes about basic income on his blog. You can also follow him here on Medium, on Twitter, on Facebook, or on Reddit where he is a moderator for the /r/BasicIncome community of over 30,000 subscribers.\nIf you feel others would appreciate this article, please click the green heart.\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\nNew Orleans writer focused on the potential for human civilization to gets its act together in the 21st century. Moderator of /r/BasicIncome on Reddit.\nArticles discussing the concept of the universal basic income"
    },
    {
        "author": "Adam Geitgey",
        "claps": "35K",
        "reading_time": 15,
        "link": "https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471?source=tag_archive---------1----------------",
        "title": "Machine Learning is Fun! ‚Äì Adam Geitgey ‚Äì Medium",
        "text": "Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8! You can also read this article in Êó•Êú¨Ë™û, PortugueÃÇs, PortugueÃÇs (alternate), TuÃàrkcÃße, FrancÃßais, ·Ñí·Ö°·Ü´·ÑÄ·ÖÆ·Ü®·Ñã·Ö• , ÿßŸÑÿπŸéÿ±Ÿéÿ®ŸêŸäŸéŸëÿ©‚Äé‚Äé, EspanÃÉol (MeÃÅxico), EspanÃÉol (EspanÃÉa), Polski, Italiano, ÊôÆÈÄöËØù, –†—É—Å—Å–∫–∏–∏ÃÜ, ·Ñí·Ö°·Ü´·ÑÄ·ÖÆ·Ü®·Ñã·Ö• , TieÃÇÃÅng VieÃ£ÃÇt or ŸÅÿßÿ±ÿ≥€å.\nBigger update: The content of this article is now available as a full-length video course that walks you through every step of the code. You can take the course for free (and access everything else on Lynda.com free for 30 days) if you sign up with this link.\nHave you heard people talking about machine learning but only have a fuzzy idea of what that means? Are you tired of nodding your way through conversations with co-workers? Let‚Äôs change that!\nThis guide is for anyone who is curious about machine learning but has no idea where to start. I imagine there are a lot of people who tried reading the wikipedia article, got frustrated and gave up wishing someone would just give them a high-level explanation. That‚Äôs what this is.\nThe goal is be accessible to anyone ‚Äî which means that there‚Äôs a lot of generalizations. But who cares? If this gets anyone more interested in ML, then mission accomplished.\nMachine learning is the idea that there are generic algorithms that can tell you something interesting about a set of data without you having to write any custom code specific to the problem. Instead of writing code, you feed data to the generic algorithm and it builds its own logic based on the data.\nFor example, one kind of algorithm is a classification algorithm. It can put data into different groups. The same classification algorithm used to recognize handwritten numbers could also be used to classify emails into spam and not-spam without changing a line of code. It‚Äôs the same algorithm but it‚Äôs fed different training data so it comes up with different classification logic.\n‚ÄúMachine learning‚Äù is an umbrella term covering lots of these kinds of generic algorithms.\nYou can think of machine learning algorithms as falling into one of two main categories ‚Äî supervised learning and unsupervised learning. The difference is simple, but really important.\nLet‚Äôs say you are a real estate agent. Your business is growing, so you hire a bunch of new trainee agents to help you out. But there‚Äôs a problem ‚Äî you can glance at a house and have a pretty good idea of what a house is worth, but your trainees don‚Äôt have your experience so they don‚Äôt know how to price their houses.\nTo help your trainees (and maybe free yourself up for a vacation), you decide to write a little app that can estimate the value of a house in your area based on it‚Äôs size, neighborhood, etc, and what similar houses have sold for.\nSo you write down every time someone sells a house in your city for 3 months. For each house, you write down a bunch of details ‚Äî number of bedrooms, size in square feet, neighborhood, etc. But most importantly, you write down the final sale price:\nUsing that training data, we want to create a program that can estimate how much any other house in your area is worth:\nThis is called supervised learning. You knew how much each house sold for, so in other words, you knew the answer to the problem and could work backwards from there to figure out the logic.\nTo build your app, you feed your training data about each house into your machine learning algorithm. The algorithm is trying to figure out what kind of math needs to be done to make the numbers work out.\nThis kind of like having the answer key to a math test with all the arithmetic symbols erased:\nFrom this, can you figure out what kind of math problems were on the test? You know you are supposed to ‚Äúdo something‚Äù with the numbers on the left to get each answer on the right.\nIn supervised learning, you are letting the computer work out that relationship for you. And once you know what math was required to solve this specific set of problems, you could answer to any other problem of the same type!\nLet‚Äôs go back to our original example with the real estate agent. What if you didn‚Äôt know the sale price for each house? Even if all you know is the size, location, etc of each house, it turns out you can still do some really cool stuff. This is called unsupervised learning.\nThis is kind of like someone giving you a list of numbers on a sheet of paper and saying ‚ÄúI don‚Äôt really know what these numbers mean but maybe you can figure out if there is a pattern or grouping or something ‚Äî good luck!‚Äù\nSo what could do with this data? For starters, you could have an algorithm that automatically identified different market segments in your data. Maybe you‚Äôd find out that home buyers in the neighborhood near the local college really like small houses with lots of bedrooms, but home buyers in the suburbs prefer 3-bedroom houses with lots of square footage. Knowing about these different kinds of customers could help direct your marketing efforts.\nAnother cool thing you could do is automatically identify any outlier houses that were way different than everything else. Maybe those outlier houses are giant mansions and you can focus your best sales people on those areas because they have bigger commissions.\nSupervised learning is what we‚Äôll focus on for the rest of this post, but that‚Äôs not because unsupervised learning is any less useful or interesting. In fact, unsupervised learning is becoming increasingly important as the algorithms get better because it can be used without having to label the data with the correct answer.\nSide note: There are lots of other types of machine learning algorithms. But this is a pretty good place to start.\nAs a human, your brain can approach most any situation and learn how to deal with that situation without any explicit instructions. If you sell houses for a long time, you will instinctively have a ‚Äúfeel‚Äù for the right price for a house, the best way to market that house, the kind of client who would be interested, etc. The goal of Strong AI research is to be able to replicate this ability with computers.\nBut current machine learning algorithms aren‚Äôt that good yet ‚Äî they only work when focused a very specific, limited problem. Maybe a better definition for ‚Äúlearning‚Äù in this case is ‚Äúfiguring out an equation to solve a specific problem based on some example data‚Äù.\nUnfortunately ‚ÄúMachine Figuring out an equation to solve a specific problem based on some example data‚Äù isn‚Äôt really a great name. So we ended up with ‚ÄúMachine Learning‚Äù instead.\nOf course if you are reading this 50 years in the future and we‚Äôve figured out the algorithm for Strong AI, then this whole post will all seem a little quaint. Maybe stop reading and go tell your robot servant to go make you a sandwich, future human.\nSo, how would you write the program to estimate the value of a house like in our example above? Think about it for a second before you read further.\nIf you didn‚Äôt know anything about machine learning, you‚Äôd probably try to write out some basic rules for estimating the price of a house like this:\nIf you fiddle with this for hours and hours, you might end up with something that sort of works. But your program will never be perfect and it will be hard to maintain as prices change.\nWouldn‚Äôt it be better if the computer could just figure out how to implement this function for you? Who cares what exactly the function does as long is it returns the correct number:\nOne way to think about this problem is that the price is a delicious stew and the ingredients are the number of bedrooms, the square footage and the neighborhood. If you could just figure out how much each ingredient impacts the final price, maybe there‚Äôs an exact ratio of ingredients to stir in to make the final price.\nThat would reduce your original function (with all those crazy if‚Äôs and else‚Äôs) down to something really simple like this:\nNotice the magic numbers in bold ‚Äî .841231951398213, 1231.1231231, 2.3242341421, and 201.23432095. These are our weights. If we could just figure out the perfect weights to use that work for every house, our function could predict house prices!\nA dumb way to figure out the best weights would be something like this:\nStart with each weight set to 1.0:\nRun every house you know about through your function and see how far off the function is at guessing the correct price for each house:\nFor example, if the first house really sold for $250,000, but your function guessed it sold for $178,000, you are off by $72,000 for that single house.\nNow add up the squared amount you are off for each house you have in your data set. Let‚Äôs say that you had 500 home sales in your data set and the square of how much your function was off for each house was a grand total of $86,123,373. That‚Äôs how ‚Äúwrong‚Äù your function currently is.\nNow, take that sum total and divide it by 500 to get an average of how far off you are for each house. Call this average error amount the cost of your function.\nIf you could get this cost to be zero by playing with the weights, your function would be perfect. It would mean that in every case, your function perfectly guessed the price of the house based on the input data. So that‚Äôs our goal ‚Äî get this cost to be as low as possible by trying different weights.\nRepeat Step 2 over and over with every single possible combination of weights. Whichever combination of weights makes the cost closest to zero is what you use. When you find the weights that work, you‚Äôve solved the problem!\nThat‚Äôs pretty simple, right? Well think about what you just did. You took some data, you fed it through three generic, really simple steps, and you ended up with a function that can guess the price of any house in your area. Watch out, Zillow!\nBut here‚Äôs a few more facts that will blow your mind:\nPretty crazy, right?\nOk, of course you can‚Äôt just try every combination of all possible weights to find the combo that works the best. That would literally take forever since you‚Äôd never run out of numbers to try.\nTo avoid that, mathematicians have figured out lots of clever ways to quickly find good values for those weights without having to try very many. Here‚Äôs one way:\nFirst, write a simple equation that represents Step #2 above:\nNow let‚Äôs re-write exactly the same equation, but using a bunch of machine learning math jargon (that you can ignore for now):\nThis equation represents how wrong our price estimating function is for the weights we currently have set.\nIf we graph this cost equation for all possible values of our weights for number_of_bedrooms and sqft, we‚Äôd get a graph that might look something like this:\nIn this graph, the lowest point in blue is where our cost is the lowest ‚Äî thus our function is the least wrong. The highest points are where we are most wrong. So if we can find the weights that get us to the lowest point on this graph, we‚Äôll have our answer!\nSo we just need to adjust our weights so we are ‚Äúwalking down hill‚Äù on this graph towards the lowest point. If we keep making small adjustments to our weights that are always moving towards the lowest point, we‚Äôll eventually get there without having to try too many different weights.\nIf you remember anything from Calculus, you might remember that if you take the derivative of a function, it tells you the slope of the function‚Äôs tangent at any point. In other words, it tells us which way is downhill for any given point on our graph. We can use that knowledge to walk downhill.\nSo if we calculate a partial derivative of our cost function with respect to each of our weights, then we can subtract that value from each weight. That will walk us one step closer to the bottom of the hill. Keep doing that and eventually we‚Äôll reach the bottom of the hill and have the best possible values for our weights. (If that didn‚Äôt make sense, don‚Äôt worry and keep reading).\nThat‚Äôs a high level summary of one way to find the best weights for your function called batch gradient descent. Don‚Äôt be afraid to dig deeper if you are interested on learning the details.\nWhen you use a machine learning library to solve a real problem, all of this will be done for you. But it‚Äôs still useful to have a good idea of what is happening.\nThe three-step algorithm I described is called multivariate linear regression. You are estimating the equation for a line that fits through all of your house data points. Then you are using that equation to guess the sales price of houses you‚Äôve never seen before based where that house would appear on your line. It‚Äôs a really powerful idea and you can solve ‚Äúreal‚Äù problems with it.\nBut while the approach I showed you might work in simple cases, it won‚Äôt work in all cases. One reason is because house prices aren‚Äôt always simple enough to follow a continuous line.\nBut luckily there are lots of ways to handle that. There are plenty of other machine learning algorithms that can handle non-linear data (like neural networks or SVMs with kernels). There are also ways to use linear regression more cleverly that allow for more complicated lines to be fit. In all cases, the same basic idea of needing to find the best weights still applies.\nAlso, I ignored the idea of overfitting. It‚Äôs easy to come up with a set of weights that always works perfectly for predicting the prices of the houses in your original data set but never actually works for any new houses that weren‚Äôt in your original data set. But there are ways to deal with this (like regularization and using a cross-validation data set). Learning how to deal with this issue is a key part of learning how to apply machine learning successfully.\nIn other words, while the basic concept is pretty simple, it takes some skill and experience to apply machine learning and get useful results. But it‚Äôs a skill that any developer can learn!\nOnce you start seeing how easily machine learning techniques can be applied to problems that seem really hard (like handwriting recognition), you start to get the feeling that you could use machine learning to solve any problem and get an answer as long as you have enough data. Just feed in the data and watch the computer magically figure out the equation that fits the data!\nBut it‚Äôs important to remember that machine learning only works if the problem is actually solvable with the data that you have.\nFor example, if you build a model that predicts home prices based on the type of potted plants in each house, it‚Äôs never going to work. There just isn‚Äôt any kind of relationship between the potted plants in each house and the home‚Äôs sale price. So no matter how hard it tries, the computer can never deduce a relationship between the two.\nSo remember, if a human expert couldn‚Äôt use the data to solve the problem manually, a computer probably won‚Äôt be able to either. Instead, focus on problems where a human could solve the problem, but where it would be great if a computer could solve it much more quickly.\nIn my mind, the biggest problem with machine learning right now is that it mostly lives in the world of academia and commercial research groups. There isn‚Äôt a lot of easy to understand material out there for people who would like to get a broad understanding without actually becoming experts. But it‚Äôs getting a little better every day.\nIf you want to try out what you‚Äôve learned in this article, I made a course that walks you through every step of this article, including writing all the code. Give it a try!\nIf you want to go deeper, Andrew Ng‚Äôs free Machine Learning class on Coursera is pretty amazing as a next step. I highly recommend it. It should be accessible to anyone who has a Comp. Sci. degree and who remembers a very minimal amount of math.\nAlso, you can play around with tons of machine learning algorithms by downloading and installing SciKit-Learn. It‚Äôs a python framework that has ‚Äúblack box‚Äù versions of all the standard algorithms.\nIf you liked this article, please consider signing up for my Machine Learning is Fun! Newsletter:\nAlso, please check out the full-length course version of this article. It covers everything in this article in more detail, including writing the actual code in Python. You can get a free 30-day trial to watch the course if you sign up with this link.\nYou can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I‚Äôd love to hear from you if I can help you or your team with machine learning.\nNow continue on to Machine Learning is Fun Part 2!\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\nInterested in computers and machine learning. Likes to write about it."
    },
    {
        "author": "Adam Geitgey",
        "claps": "14.2K",
        "reading_time": 15,
        "link": "https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721?source=tag_archive---------2----------------",
        "title": "Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks",
        "text": "Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8!\nYou can also read this article in ÊôÆÈÄöËØù, –†—É—Å—Å–∫–∏–∏ÃÜ, ·Ñí·Ö°·Ü´·ÑÄ·ÖÆ·Ü®·Ñã·Ö•, PortugueÃÇs, TieÃÇÃÅng VieÃ£ÃÇt or Italiano.\nAre you tired of reading endless news stories about deep learning and not really knowing what that means? Let‚Äôs change that!\nThis time, we are going to learn how to write programs that recognize objects in images using deep learning. In other words, we‚Äôre going to explain the black magic that allows Google Photos to search your photos based on what is in the picture:\nJust like Part 1 and Part 2, this guide is for anyone who is curious about machine learning but has no idea where to start. The goal is be accessible to anyone ‚Äî which means that there‚Äôs a lot of generalizations and we skip lots of details. But who cares? If this gets anyone more interested in ML, then mission accomplished!\n(If you haven‚Äôt already read part 1 and part 2, read them now!)\nYou might have seen this famous xkcd comic before.\nThe goof is based on the idea that any 3-year-old child can recognize a photo of a bird, but figuring out how to make a computer recognize objects has puzzled the very best computer scientists for over 50 years.\nIn the last few years, we‚Äôve finally found a good approach to object recognition using deep convolutional neural networks. That sounds like a a bunch of made up words from a William Gibson Sci-Fi novel, but the ideas are totally understandable if you break them down one by one.\nSo let‚Äôs do it ‚Äî let‚Äôs write a program that can recognize birds!\nBefore we learn how to recognize pictures of birds, let‚Äôs learn how to recognize something much simpler ‚Äî the handwritten number ‚Äú8‚Äù.\nIn Part 2, we learned about how neural networks can solve complex problems by chaining together lots of simple neurons. We created a small neural network to estimate the price of a house based on how many bedrooms it had, how big it was, and which neighborhood it was in:\nWe also know that the idea of machine learning is that the same generic algorithms can be reused with different data to solve different problems. So let‚Äôs modify this same neural network to recognize handwritten text. But to make the job really simple, we‚Äôll only try to recognize one letter ‚Äî the numeral ‚Äú8‚Äù.\nMachine learning only works when you have data ‚Äî preferably a lot of data. So we need lots and lots of handwritten ‚Äú8‚Äùs to get started. Luckily, researchers created the MNIST data set of handwritten numbers for this very purpose. MNIST provides 60,000 images of handwritten digits, each as an 18x18 image. Here are some ‚Äú8‚Äùs from the data set:\nThe neural network we made in Part 2 only took in a three numbers as the input (‚Äú3‚Äù bedrooms, ‚Äú2000‚Äù sq. feet , etc.). But now we want to process images with our neural network. How in the world do we feed images into a neural network instead of just numbers?\nThe answer is incredible simple. A neural network takes numbers as input. To a computer, an image is really just a grid of numbers that represent how dark each pixel is:\nTo feed an image into our neural network, we simply treat the 18x18 pixel image as an array of 324 numbers:\nThe handle 324 inputs, we‚Äôll just enlarge our neural network to have 324 input nodes:\nNotice that our neural network also has two outputs now (instead of just one). The first output will predict the likelihood that the image is an ‚Äú8‚Äù and thee second output will predict the likelihood it isn‚Äôt an ‚Äú8‚Äù. By having a separate output for each type of object we want to recognize, we can use a neural network to classify objects into groups.\nOur neural network is a lot bigger than last time (324 inputs instead of 3!). But any modern computer can handle a neural network with a few hundred nodes without blinking. This would even work fine on your cell phone.\nAll that‚Äôs left is to train the neural network with images of ‚Äú8‚Äùs and not-‚Äú8\"s so it learns to tell them apart. When we feed in an ‚Äú8‚Äù, we‚Äôll tell it the probability the image is an ‚Äú8‚Äù is 100% and the probability it‚Äôs not an ‚Äú8‚Äù is 0%. Vice versa for the counter-example images.\nHere‚Äôs some of our training data:\nWe can train this kind of neural network in a few minutes on a modern laptop. When it‚Äôs done, we‚Äôll have a neural network that can recognize pictures of ‚Äú8‚Äùs with a pretty high accuracy. Welcome to the world of (late 1980‚Äôs-era) image recognition!\nIt‚Äôs really neat that simply feeding pixels into a neural network actually worked to build image recognition! Machine learning is magic! ...right?\nWell, of course it‚Äôs not that simple.\nFirst, the good news is that our ‚Äú8‚Äù recognizer really does work well on simple images where the letter is right in the middle of the image:\nBut now the really bad news:\nOur ‚Äú8‚Äù recognizer totally fails to work when the letter isn‚Äôt perfectly centered in the image. Just the slightest position change ruins everything:\nThis is because our network only learned the pattern of a perfectly-centered ‚Äú8‚Äù. It has absolutely no idea what an off-center ‚Äú8‚Äù is. It knows exactly one pattern and one pattern only.\nThat‚Äôs not very useful in the real world. Real world problems are never that clean and simple. So we need to figure out how to make our neural network work in cases where the ‚Äú8‚Äù isn‚Äôt perfectly centered.\nWe already created a really good program for finding an ‚Äú8‚Äù centered in an image. What if we just scan all around the image for possible ‚Äú8‚Äùs in smaller sections, one section at a time, until we find one?\nThis approach called a sliding window. It‚Äôs the brute force solution. It works well in some limited cases, but it‚Äôs really inefficient. You have to check the same image over and over looking for objects of different sizes. We can do better than this!\nWhen we trained our network, we only showed it ‚Äú8‚Äùs that were perfectly centered. What if we train it with more data, including ‚Äú8‚Äùs in all different positions and sizes all around the image?\nWe don‚Äôt even need to collect new training data. We can just write a script to generate new images with the ‚Äú8‚Äùs in all kinds of different positions in the image:\nUsing this technique, we can easily create an endless supply of training data.\nMore data makes the problem harder for our neural network to solve, but we can compensate for that by making our network bigger and thus able to learn more complicated patterns.\nTo make the network bigger, we just stack up layer upon layer of nodes:\nWe call this a ‚Äúdeep neural network‚Äù because it has more layers than a traditional neural network.\nThis idea has been around since the late 1960s. But until recently, training this large of a neural network was just too slow to be useful. But once we figured out how to use 3d graphics cards (which were designed to do matrix multiplication really fast) instead of normal computer processors, working with large neural networks suddenly became practical. In fact, the exact same NVIDIA GeForce GTX 1080 video card that you use to play Overwatch can be used to train neural networks incredibly quickly.\nBut even though we can make our neural network really big and train it quickly with a 3d graphics card, that still isn‚Äôt going to get us all the way to a solution. We need to be smarter about how we process images into our neural network.\nThink about it. It doesn‚Äôt make sense to train a network to recognize an ‚Äú8‚Äù at the top of a picture separately from training it to recognize an ‚Äú8‚Äù at the bottom of a picture as if those were two totally different objects.\nThere should be some way to make the neural network smart enough to know that an ‚Äú8‚Äù anywhere in the picture is the same thing without all that extra training. Luckily... there is!\nAs a human, you intuitively know that pictures have a hierarchy or conceptual structure. Consider this picture:\nAs a human, you instantly recognize the hierarchy in this picture:\nMost importantly, we recognize the idea of a child no matter what surface the child is on. We don‚Äôt have to re-learn the idea of child for every possible surface it could appear on.\nBut right now, our neural network can‚Äôt do this. It thinks that an ‚Äú8‚Äù in a different part of the image is an entirely different thing. It doesn‚Äôt understand that moving an object around in the picture doesn‚Äôt make it something different. This means it has to re-learn the identify of each object in every possible position. That sucks.\nWe need to give our neural network understanding of translation invariance ‚Äî an ‚Äú8‚Äù is an ‚Äú8‚Äù no matter where in the picture it shows up.\nWe‚Äôll do this using a process called Convolution. The idea of convolution is inspired partly by computer science and partly by biology (i.e. mad scientists literally poking cat brains with weird probes to figure out how cats process images).\nInstead of feeding entire images into our neural network as one grid of numbers, we‚Äôre going to do something a lot smarter that takes advantage of the idea that an object is the same no matter where it appears in a picture.\nHere‚Äôs how it‚Äôs going to work, step by step ‚Äî\nSimilar to our sliding window search above, let‚Äôs pass a sliding window over the entire original image and save each result as a separate, tiny picture tile:\nBy doing this, we turned our original image into 77 equally-sized tiny image tiles.\nEarlier, we fed a single image into a neural network to see if it was an ‚Äú8‚Äù. We‚Äôll do the exact same thing here, but we‚Äôll do it for each individual image tile:\nHowever, there‚Äôs one big twist: We‚Äôll keep the same neural network weights for every single tile in the same original image. In other words, we are treating every image tile equally. If something interesting appears in any given tile, we‚Äôll mark that tile as interesting.\nWe don‚Äôt want to lose track of the arrangement of the original tiles. So we save the result from processing each tile into a grid in the same arrangement as the original image. It looks like this:\nIn other words, we‚Äôve started with a large image and we ended with a slightly smaller array that records which sections of our original image were the most interesting.\nThe result of Step 3 was an array that maps out which parts of the original image are the most interesting. But that array is still pretty big:\nTo reduce the size of the array, we downsample it using an algorithm called max pooling. It sounds fancy, but it isn‚Äôt at all!\nWe‚Äôll just look at each 2x2 square of the array and keep the biggest number:\nThe idea here is that if we found something interesting in any of the four input tiles that makes up each 2x2 grid square, we‚Äôll just keep the most interesting bit. This reduces the size of our array while keeping the most important bits.\nSo far, we‚Äôve reduced a giant image down into a fairly small array.\nGuess what? That array is just a bunch of numbers, so we can use that small array as input into another neural network. This final neural network will decide if the image is or isn‚Äôt a match. To differentiate it from the convolution step, we call it a ‚Äúfully connected‚Äù network.\nSo from start to finish, our whole five-step pipeline looks like this:\nOur image processing pipeline is a series of steps: convolution, max-pooling, and finally a fully-connected network.\nWhen solving problems in the real world, these steps can be combined and stacked as many times as you want! You can have two, three or even ten convolution layers. You can throw in max pooling wherever you want to reduce the size of your data.\nThe basic idea is to start with a large image and continually boil it down, step-by-step, until you finally have a single result. The more convolution steps you have, the more complicated features your network will be able to learn to recognize.\nFor example, the first convolution step might learn to recognize sharp edges, the second convolution step might recognize beaks using it‚Äôs knowledge of sharp edges, the third step might recognize entire birds using it‚Äôs knowledge of beaks, etc.\nHere‚Äôs what a more realistic deep convolutional network (like you would find in a research paper) looks like:\nIn this case, they start a 224 x 224 pixel image, apply convolution and max pooling twice, apply convolution 3 more times, apply max pooling and then have two fully-connected layers. The end result is that the image is classified into one of 1000 categories!\nSo how do you know which steps you need to combine to make your image classifier work?\nHonestly, you have to answer this by doing a lot of experimentation and testing. You might have to train 100 networks before you find the optimal structure and parameters for the problem you are solving. Machine learning involves a lot of trial and error!\nNow finally we know enough to write a program that can decide if a picture is a bird or not.\nAs always, we need some data to get started. The free CIFAR10 data set contains 6,000 pictures of birds and 52,000 pictures of things that are not birds. But to get even more data we‚Äôll also add in the Caltech-UCSD Birds-200‚Äì2011 data set that has another 12,000 bird pics.\nHere‚Äôs a few of the birds from our combined data set:\nAnd here‚Äôs some of the 52,000 non-bird images:\nThis data set will work fine for our purposes, but 72,000 low-res images is still pretty small for real-world applications. If you want Google-level performance, you need millions of large images. In machine learning, having more data is almost always more important that having better algorithms. Now you know why Google is so happy to offer you unlimited photo storage. They want your sweet, sweet data!\nTo build our classifier, we‚Äôll use TFLearn. TFlearn is a wrapper around Google‚Äôs TensorFlow deep learning library that exposes a simplified API. It makes building convolutional neural networks as easy as writing a few lines of code to define the layers of our network.\nHere‚Äôs the code to define and train the network:\nIf you are training with a good video card with enough RAM (like an Nvidia GeForce GTX 980 Ti or better), this will be done in less than an hour. If you are training with a normal cpu, it might take a lot longer.\nAs it trains, the accuracy will increase. After the first pass, I got 75.4% accuracy. After just 10 passes, it was already up to 91.7%. After 50 or so passes, it capped out around 95.5% accuracy and additional training didn‚Äôt help, so I stopped it there.\nCongrats! Our program can now recognize birds in images!\nNow that we have a trained neural network, we can use it! Here‚Äôs a simple script that takes in a single image file and predicts if it is a bird or not.\nBut to really see how effective our network is, we need to test it with lots of images. The data set I created held back 15,000 images for validation. When I ran those 15,000 images through the network, it predicted the correct answer 95% of the time.\nThat seems pretty good, right? Well... it depends!\nOur network claims to be 95% accurate. But the devil is in the details. That could mean all sorts of different things.\nFor example, what if 5% of our training images were birds and the other 95% were not birds? A program that guessed ‚Äúnot a bird‚Äù every single time would be 95% accurate! But it would also be 100% useless.\nWe need to look more closely at the numbers than just the overall accuracy. To judge how good a classification system really is, we need to look closely at how it failed, not just the percentage of the time that it failed.\nInstead of thinking about our predictions as ‚Äúright‚Äù and ‚Äúwrong‚Äù, let‚Äôs break them down into four separate categories ‚Äî\nUsing our validation set of 15,000 images, here‚Äôs how many times our predictions fell into each category:\nWhy do we break our results down like this? Because not all mistakes are created equal.\nImagine if we were writing a program to detect cancer from an MRI image. If we were detecting cancer, we‚Äôd rather have false positives than false negatives. False negatives would be the worse possible case ‚Äî that‚Äôs when the program told someone they definitely didn‚Äôt have cancer but they actually did.\nInstead of just looking at overall accuracy, we calculate Precision and Recall metrics. Precision and Recall metrics give us a clearer picture of how well we did:\nThis tells us that 97% of the time we guessed ‚ÄúBird‚Äù, we were right! But it also tells us that we only found 90% of the actual birds in the data set. In other words, we might not find every bird but we are pretty sure about it when we do find one!\nNow that you know the basics of deep convolutional networks, you can try out some of the examples that come with tflearn to get your hands dirty with different neural network architectures. It even comes with built-in data sets so you don‚Äôt even have to find your own images.\nYou also know enough now to start branching and learning about other areas of machine learning. Why not learn how to use algorithms to train computers how to play Atari games next?\nIf you liked this article, please consider signing up for my Machine Learning is Fun! email list. I‚Äôll only email you when I have something new and awesome to share. It‚Äôs the best way to find out when I write more articles like this.\nYou can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I‚Äôd love to hear from you if I can help you or your team with machine learning.\nNow continue on to Machine Learning is Fun Part 4, Part 5 and Part 6!\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\nInterested in computers and machine learning. Likes to write about it."
    },
    {
        "author": "Adam Geitgey",
        "claps": "15.2K",
        "reading_time": 13,
        "link": "https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78?source=tag_archive---------3----------------",
        "title": "Machine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning",
        "text": "Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8!\nYou can also read this article in ÊôÆÈÄöËØù, –†—É—Å—Å–∫–∏–∏ÃÜ, ·Ñí·Ö°·Ü´·ÑÄ·ÖÆ·Ü®·Ñã·Ö•, PortugueÃÇs, TieÃÇÃÅng VieÃ£ÃÇt or Italiano.\nHave you noticed that Facebook has developed an uncanny ability to recognize your friends in your photographs? In the old days, Facebook used to make you to tag your friends in photos by clicking on them and typing in their name. Now as soon as you upload a photo, Facebook tags everyone for you like magic:\nThis technology is called face recognition. Facebook‚Äôs algorithms are able to recognize your friends‚Äô faces after they have been tagged only a few times. It‚Äôs pretty amazing technology ‚Äî Facebook can recognize faces with 98% accuracy which is pretty much as good as humans can do!\nLet‚Äôs learn how modern face recognition works! But just recognizing your friends would be too easy. We can push this tech to the limit to solve a more challenging problem ‚Äî telling Will Ferrell (famous actor) apart from Chad Smith (famous rock musician)!\nSo far in Part 1, 2 and 3, we‚Äôve used machine learning to solve isolated problems that have only one step ‚Äî estimating the price of a house, generating new data based on existing data and telling if an image contains a certain object. All of those problems can be solved by choosing one machine learning algorithm, feeding in data, and getting the result.\nBut face recognition is really a series of several related problems:\nAs a human, your brain is wired to do all of this automatically and instantly. In fact, humans are too good at recognizing faces and end up seeing faces in everyday objects:\nComputers are not capable of this kind of high-level generalization (at least not yet...), so we have to teach them how to do each step in this process separately.\nWe need to build a pipeline where we solve each step of face recognition separately and pass the result of the current step to the next step. In other words, we will chain together several machine learning algorithms:\nLet‚Äôs tackle this problem one step at a time. For each step, we‚Äôll learn about a different machine learning algorithm. I‚Äôm not going to explain every single algorithm completely to keep this from turning into a book, but you‚Äôll learn the main ideas behind each one and you‚Äôll learn how you can build your own facial recognition system in Python using OpenFace and dlib.\nThe first step in our pipeline is face detection. Obviously we need to locate the faces in a photograph before we can try to tell them apart!\nIf you‚Äôve used any camera in the last 10 years, you‚Äôve probably seen face detection in action:\nFace detection is a great feature for cameras. When the camera can automatically pick out faces, it can make sure that all the faces are in focus before it takes the picture. But we‚Äôll use it for a different purpose ‚Äî finding the areas of the image we want to pass on to the next step in our pipeline.\nFace detection went mainstream in the early 2000's when Paul Viola and Michael Jones invented a way to detect faces that was fast enough to run on cheap cameras. However, much more reliable solutions exist now. We‚Äôre going to use a method invented in 2005 called Histogram of Oriented Gradients ‚Äî or just HOG for short.\nTo find faces in an image, we‚Äôll start by making our image black and white because we don‚Äôt need color data to find faces:\nThen we‚Äôll look at every single pixel in our image one at a time. For every single pixel, we want to look at the pixels that directly surrounding it:\nOur goal is to figure out how dark the current pixel is compared to the pixels directly surrounding it. Then we want to draw an arrow showing in which direction the image is getting darker:\nIf you repeat that process for every single pixel in the image, you end up with every pixel being replaced by an arrow. These arrows are called gradients and they show the flow from light to dark across the entire image:\nThis might seem like a random thing to do, but there‚Äôs a really good reason for replacing the pixels with gradients. If we analyze pixels directly, really dark images and really light images of the same person will have totally different pixel values. But by only considering the direction that brightness changes, both really dark images and really bright images will end up with the same exact representation. That makes the problem a lot easier to solve!\nBut saving the gradient for every single pixel gives us way too much detail. We end up missing the forest for the trees. It would be better if we could just see the basic flow of lightness/darkness at a higher level so we could see the basic pattern of the image.\nTo do this, we‚Äôll break up the image into small squares of 16x16 pixels each. In each square, we‚Äôll count up how many gradients point in each major direction (how many point up, point up-right, point right, etc...). Then we‚Äôll replace that square in the image with the arrow directions that were the strongest.\nThe end result is we turn the original image into a very simple representation that captures the basic structure of a face in a simple way:\nTo find faces in this HOG image, all we have to do is find the part of our image that looks the most similar to a known HOG pattern that was extracted from a bunch of other training faces:\nUsing this technique, we can now easily find faces in any image:\nIf you want to try this step out yourself using Python and dlib, here‚Äôs code showing how to generate and view HOG representations of images.\nWhew, we isolated the faces in our image. But now we have to deal with the problem that faces turned different directions look totally different to a computer:\nTo account for this, we will try to warp each picture so that the eyes and lips are always in the sample place in the image. This will make it a lot easier for us to compare faces in the next steps.\nTo do this, we are going to use an algorithm called face landmark estimation. There are lots of ways to do this, but we are going to use the approach invented in 2014 by Vahid Kazemi and Josephine Sullivan.\nThe basic idea is we will come up with 68 specific points (called landmarks) that exist on every face ‚Äî the top of the chin, the outside edge of each eye, the inner edge of each eyebrow, etc. Then we will train a machine learning algorithm to be able to find these 68 specific points on any face:\nHere‚Äôs the result of locating the 68 face landmarks on our test image:\nNow that we know were the eyes and mouth are, we‚Äôll simply rotate, scale and shear the image so that the eyes and mouth are centered as best as possible. We won‚Äôt do any fancy 3d warps because that would introduce distortions into the image. We are only going to use basic image transformations like rotation and scale that preserve parallel lines (called affine transformations):\nNow no matter how the face is turned, we are able to center the eyes and mouth are in roughly the same position in the image. This will make our next step a lot more accurate.\nIf you want to try this step out yourself using Python and dlib, here‚Äôs the code for finding face landmarks and here‚Äôs the code for transforming the image using those landmarks.\nNow we are to the meat of the problem ‚Äî actually telling faces apart. This is where things get really interesting!\nThe simplest approach to face recognition is to directly compare the unknown face we found in Step 2 with all the pictures we have of people that have already been tagged. When we find a previously tagged face that looks very similar to our unknown face, it must be the same person. Seems like a pretty good idea, right?\nThere‚Äôs actually a huge problem with that approach. A site like Facebook with billions of users and a trillion photos can‚Äôt possibly loop through every previous-tagged face to compare it to every newly uploaded picture. That would take way too long. They need to be able to recognize faces in milliseconds, not hours.\nWhat we need is a way to extract a few basic measurements from each face. Then we could measure our unknown face the same way and find the known face with the closest measurements. For example, we might measure the size of each ear, the spacing between the eyes, the length of the nose, etc. If you‚Äôve ever watched a bad crime show like CSI, you know what I am talking about:\nOk, so which measurements should we collect from each face to build our known face database? Ear size? Nose length? Eye color? Something else?\nIt turns out that the measurements that seem obvious to us humans (like eye color) don‚Äôt really make sense to a computer looking at individual pixels in an image. Researchers have discovered that the most accurate approach is to let the computer figure out the measurements to collect itself. Deep learning does a better job than humans at figuring out which parts of a face are important to measure.\nThe solution is to train a Deep Convolutional Neural Network (just like we did in Part 3). But instead of training the network to recognize pictures objects like we did last time, we are going to train it to generate 128 measurements for each face.\nThe training process works by looking at 3 face images at a time:\nThen the algorithm looks at the measurements it is currently generating for each of those three images. It then tweaks the neural network slightly so that it makes sure the measurements it generates for #1 and #2 are slightly closer while making sure the measurements for #2 and #3 are slightly further apart:\nAfter repeating this step millions of times for millions of images of thousands of different people, the neural network learns to reliably generate 128 measurements for each person. Any ten different pictures of the same person should give roughly the same measurements.\nMachine learning people call the 128 measurements of each face an embedding. The idea of reducing complicated raw data like a picture into a list of computer-generated numbers comes up a lot in machine learning (especially in language translation). The exact approach for faces we are using was invented in 2015 by researchers at Google but many similar approaches exist.\nThis process of training a convolutional neural network to output face embeddings requires a lot of data and computer power. Even with an expensive NVidia Telsa video card, it takes about 24 hours of continuous training to get good accuracy.\nBut once the network has been trained, it can generate measurements for any face, even ones it has never seen before! So this step only needs to be done once. Lucky for us, the fine folks at OpenFace already did this and they published several trained networks which we can directly use. Thanks Brandon Amos and team!\nSo all we need to do ourselves is run our face images through their pre-trained network to get the 128 measurements for each face. Here‚Äôs the measurements for our test image:\nSo what parts of the face are these 128 numbers measuring exactly? It turns out that we have no idea. It doesn‚Äôt really matter to us. All that we care is that the network generates nearly the same numbers when looking at two different pictures of the same person.\nIf you want to try this step yourself, OpenFace provides a lua script that will generate embeddings all images in a folder and write them to a csv file. You run it like this.\nThis last step is actually the easiest step in the whole process. All we have to do is find the person in our database of known people who has the closest measurements to our test image.\nYou can do that by using any basic machine learning classification algorithm. No fancy deep learning tricks are needed. We‚Äôll use a simple linear SVM classifier, but lots of classification algorithms could work.\nAll we need to do is train a classifier that can take in the measurements from a new test image and tells which known person is the closest match. Running this classifier takes milliseconds. The result of the classifier is the name of the person!\nSo let‚Äôs try out our system. First, I trained a classifier with the embeddings of about 20 pictures each of Will Ferrell, Chad Smith and Jimmy Falon:\nThen I ran the classifier on every frame of the famous youtube video of Will Ferrell and Chad Smith pretending to be each other on the Jimmy Fallon show:\nIt works! And look how well it works for faces in different poses ‚Äî even sideways faces!\nLet‚Äôs review the steps we followed:\nNow that you know how this all works, here‚Äôs instructions from start-to-finish of how run this entire face recognition pipeline on your own computer:\nUPDATE 4/9/2017: You can still follow the steps below to use OpenFace. However, I‚Äôve released a new Python-based face recognition library called face_recognition that is much easier to install and use. So I‚Äôd recommend trying out face_recognition first instead of continuing below!\nI even put together a pre-configured virtual machine with face_recognition, OpenCV, TensorFlow and lots of other deep learning tools pre-installed. You can download and run it on your computer very easily. Give the virtual machine a shot if you don‚Äôt want to install all these libraries yourself!\nOriginal OpenFace instructions:\nIf you liked this article, please consider signing up for my Machine Learning is Fun! newsletter:\nYou can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I‚Äôd love to hear from you if I can help you or your team with machine learning.\nNow continue on to Machine Learning is Fun Part 5!\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\nInterested in computers and machine learning. Likes to write about it."
    },
    {
        "author": "Xiaohan Zeng",
        "claps": "48K",
        "reading_time": 13,
        "link": "https://medium.com/@XiaohanZeng/i-interviewed-at-five-top-companies-in-silicon-valley-in-five-days-and-luckily-got-five-job-offers-25178cf74e0f?source=tag_archive---------4----------------",
        "title": "I interviewed at five top companies in Silicon Valley in five days, and luckily got five job offers",
        "text": "In the five days from July 24th to 28th 2017, I interviewed at LinkedIn, Salesforce Einstein, Google, Airbnb, and Facebook, and got all five job offers.\nIt was a great experience, and I feel fortunate that my efforts paid off, so I decided to write something about it. I will discuss how I prepared, review the interview process, and share my impressions about the five companies.\nI had been at Groupon for almost three years. It‚Äôs my first job, and I have been working with an amazing team and on awesome projects. We‚Äôve been building cool stuff, making impact within the company, publishing papers and all that. But I felt my learning rate was being annealed (read: slowing down) yet my mind was craving more. Also as a software engineer in Chicago, there are so many great companies that all attract me in the Bay Area.\nLife is short, and professional life shorter still. After talking with my wife and gaining her full support, I decided to take actions and make my first ever career change.\nAlthough I‚Äôm interested in machine learning positions, the positions at the five companies are slightly different in the title and the interviewing process. Three are machine learning engineer (LinkedIn, Google, Facebook), one is data engineer (Salesforce), and one is software engineer in general (Airbnb). Therefore I needed to prepare for three different areas: coding, machine learning, and system design.\nSince I also have a full time job, it took me 2‚Äì3 months in total to prepare. Here is how I prepared for the three areas.\nWhile I agree that coding interviews might not be the best way to assess all your skills as a developer, there is arguably no better way to tell if you are a good engineer in a short period of time. IMO it is the necessary evil to get you that job.\nI mainly used Leetcode and Geeksforgeeks for practicing, but Hackerrank and Lintcode are also good places. I spent several weeks going over common data structures and algorithms, then focused on areas I wasn‚Äôt too familiar with, and finally did some frequently seen problems. Due to my time constraints I usually did two problems per day.\nHere are some thoughts:\nThis area is more closely related to the actual working experience. Many questions can be asked during system design interviews, including but not limited to system architecture, object oriented design,database schema design,distributed system design,scalability, etc.\nThere are many resources online that can help you with the preparation. For the most part I read articles on system design interviews, architectures of large-scale systems, and case studies.\nHere are some resources that I found really helpful:\nAlthough system design interviews can cover a lot of topics, there are some general guidelines for how to approach the problem:\nWith all that said, the best way to practice for system design interviews is to actually sit down and design a system, i.e. your day-to-day work. Instead of doing the minimal work, go deeper into the tools, frameworks, and libraries you use. For example, if you use HBase, rather than simply using the client to run some DDL and do some fetches, try to understand its overall architecture, such as the read/write flow, how HBase ensures strong consistency, what minor/major compactions do, and where LRU cache and Bloom Filter are used in the system. You can even compare HBase with Cassandra and see the similarities and differences in their design. Then when you are asked to design a distributed key-value store, you won‚Äôt feel ambushed.\nMany blogs are also a great source of knowledge, such as Hacker Noon and engineering blogs of some companies, as well as the official documentation of open source projects.\nThe most important thing is to keep your curiosity and modesty. Be a sponge that absorbs everything it is submerged into.\nMachine learning interviews can be divided into two aspects, theory and product design.\nUnless you are have experience in machine learning research or did really well in your ML course, it helps to read some textbooks. Classical ones such as the Elements of Statistical Learning and Pattern Recognition and Machine Learning are great choices, and if you are interested in specific areas you can read more on those.\nMake sure you understand basic concepts such as bias-variance trade-off, overfitting, gradient descent, L1/L2 regularization,Bayes Theorem,bagging/boosting,collaborative filtering,dimension reduction, etc. Familiarize yourself with common formulas such as Bayes Theorem and the derivation of popular models such as logistic regression and SVM. Try to implement simple models such as decision trees and K-means clustering. If you put some models on your resume, make sure you understand it thoroughly and can comment on its pros and cons.\nFor ML product design, understand the general process of building a ML product. Here‚Äôs what I tried to do:\nHere I want to emphasize again on the importance of remaining curious and learning continuously. Try not to merely using the API for Spark MLlib or XGBoost and calling it done, but try to understand why stochastic gradient descent is appropriate for distributed training, or understand how XGBoost differs from traditional GBDT, e.g. what is special about its loss function, why it needs to compute the second order derivative, etc.\nI started by replying to HR‚Äôs messages on LinkedIn, and asking for referrals. After a failed attempt at a rock star startup (which I will touch upon later), I prepared hard for several months, and with help from my recruiters, I scheduled a full week of onsites in the Bay Area. I flew in on Sunday, had five full days of interviews with around 30 interviewers at some best tech companies in the world, and very luckily, got job offers from all five of them.\nAll phone screenings are standard. The only difference is in the duration: For some companies like LinkedIn it‚Äôs one hour, while for Facebook and Airbnb it‚Äôs 45 minutes.\nProficiency is the key here, since you are under the time gun and usually you only get one chance. You would have to very quickly recognize the type of problem and give a high-level solution. Be sure to talk to the interviewer about your thinking and intentions. It might slow you down a little at the beginning, but communication is more important than anything and it only helps with the interview. Do not recite the solution as the interviewer would almost certainly see through it.\nFor machine learning positions some companies would ask ML questions. If you are interviewing for those make sure you brush up your ML skills as well.\nTo make better use of my time, I scheduled three phone screenings in the same afternoon, one hour apart from each. The upside is that you might benefit from the hot hand and the downside is that the later ones might be affected if the first one does not go well, so I don‚Äôt recommend it for everyone.\nOne good thing about interviewing with multiple companies at the same time is that it gives you certain advantages. I was able to skip the second round phone screening with Airbnb and Salesforce because I got the onsite at LinkedIn and Facebook after only one phone screening.\nMore surprisingly, Google even let me skip their phone screening entirely and schedule my onsite to fill the vacancy after learning I had four onsites coming in the next week. I knew it was going to make it extremely tiring, but hey, nobody can refuse a Google onsite invitation!\nLinkedIn\nThis is my first onsite and I interviewed at the Sunnyvale location. The office is very neat and people look very professional, as always.\nThe sessions are one hour each. Coding questions are standard, but the ML questions can get a bit tough. That said, I got an email from my HR containing the preparation material which was very helpful, and in the end I did not see anything that was too surprising. I heard the rumor that LinkedIn has the best meals in the Silicon Valley, and from what I saw if it‚Äôs not true, it‚Äôs not too far from the truth.\nAcquisition by Microsoft seems to have lifted the financial burden from LinkedIn, and freed them up to do really cool things. New features such as videos and professional advertisements are exciting. As a company focusing on professional development, LinkedIn prioritizes the growth of its own employees. A lot of teams such as ads relevance and feed ranking are expanding, so act quickly if you want to join.\nSalesforce Einstein\nRock star project by rock star team. The team is pretty new and feels very much like a startup. The product is built on the Scala stack, so type safety is a real thing there! Great talks on the Optimus Prime library by Matthew Tovbin at Scala Days Chicago 2017 and Leah McGuire at Spark Summit West 2017.\nI interviewed at their Palo Alto office. The team has a cohesive culture and work life balance is great there. Everybody is passionate about what they are doing and really enjoys it. With four sessions it is shorter compared to the other onsite interviews, but I wish I could have stayed longer. After the interview Matthew even took me for a walk to the HP garage :)\nGoogle\nAbsolutely the industry leader, and nothing to say about it that people don‚Äôt already know. But it‚Äôs huge. Like, really, really HUGE. It took me 20 minutes to ride a bicycle to meet my friends there. Also lines for food can be too long. Forever a great place for developers.\nI interviewed at one of the many buildings on the Mountain View campus, and I don‚Äôt know which one it is because it‚Äôs HUGE.\nMy interviewers all look very smart, and once they start talking they are even smarter. It would be very enjoyable to work with these people.\nOne thing that I felt special about Google‚Äôs interviews is that the analysis of algorithm complexity is really important. Make sure you really understand what Big O notation means!\nAirbnb\nFast expanding unicorn with a unique culture and arguably the most beautiful office in the Silicon Valley. New products such as Experiences and restaurant reservation, high end niche market, and expansion into China all contribute to a positive prospect. Perfect choice if you are risk tolerant and want a fast growing, pre-IPO experience.\nAirbnb‚Äôs coding interview is a bit unique because you‚Äôll be coding in an IDE instead of whiteboarding, so your code needs to compile and give the right answer. Some problems can get really hard.\nAnd they‚Äôve got the one-of-a-kind cross functional interviews. This is how Airbnb takes culture seriously, and being technically excellent doesn‚Äôt guarantee a job offer. For me the two cross functionals were really enjoyable. I had casual conversations with the interviewers and we all felt happy at the end of the session.\nOverall I think Airbnb‚Äôs onsite is the hardest due to the difficulty of the problems, longer duration, and unique cross-functional interviews. If you are interested, be sure to understand their culture and core values.\nFacebook\nAnother giant that is still growing fast, and smaller and faster-paced compared to Google. With its product lines dominating the social network market and big investments in AI and VR, I can only see more growth potential for Facebook in the future. With stars like Yann LeCun and Yangqing Jia, it‚Äôs the perfect place if you are interested in machine learning.\nI interviewed at Building 20, the one with the rooftop garden and ocean view and also where Zuckerberg‚Äôs office is located.\nI‚Äôm not sure if the interviewers got instructions, but I didn‚Äôt get clear signs whether my solutions were correct, although I believed they were.\nBy noon the prior four days started to take its toll, and I was having a headache. I persisted through the afternoon sessions but felt I didn‚Äôt do well at all. I was a bit surprised to learn that I was getting an offer from them as well.\nGenerally I felt people there believe the company‚Äôs vision and are proud of what they are building. Being a company with half a trillion market cap and growing, Facebook is a perfect place to grow your career at.\nThis is a big topic that I won‚Äôt cover in this post, but I found this article to be very helpful.\nSome things that I do think are important:\nAll successes start with failures, including interviews. Before I started interviewing for these companies, I failed my interview at Databricks in May.\nBack in April, Xiangrui contacted me via LinkedIn asking me if I was interested in a position on the Spark MLlib team. I was extremely thrilled because 1) I use Spark and love Scala, 2) Databricks engineers are top-notch, and 3) Spark is revolutionizing the whole big data world. It is an opportunity I couldn‚Äôt miss, so I started interviewing after a few days.\nThe bar is very high and the process is quite long, including one pre-screening questionnaire, one phone screening, one coding assignment, and one full onsite.\nI managed to get the onsite invitation, and visited their office in downtown San Francisco, where Treasure Island can be seen.\nMy interviewer were incredibly intelligent yet equally modest. During the interviews I often felt being pushed to the limits. It was fine until one disastrous session, where I totally messed up due to insufficient skills and preparation, and it ended up a fiasco. Xiangrui was very kind and walked me to where I wanted to go after the interview was over, and I really enjoyed talking to him.\nI got the rejection several days later. It was expected but I felt frustrated for a few days nonetheless. Although I missed the opportunity to work there, I wholeheartedly wish they will continue to make greater impact and achievements.\nFrom the first interview in May to finally accepting the job offer in late September, my first career change was long and not easy.\nIt was difficult for me to prepare because I needed to keep doing well at my current job. For several weeks I was on a regular schedule of preparing for the interview till 1am, getting up at 8:30am the next day and fully devoting myself to another day at work.\nInterviewing at five companies in five days was also highly stressful and risky, and I don‚Äôt recommend doing it unless you have a very tight schedule. But it does give you a good advantage during negotiation should you secure multiple offers.\nI‚Äôd like to thank all my recruiters who patiently walked me through the process, the people who spend their precious time talking to me, and all the companies that gave me the opportunities to interview and extended me offers.\nLastly but most importantly, I want to thank my family for their love and support ‚Äî my parents for watching me taking the first and every step, my dear wife for everything she has done for me, and my daughter for her warming smile.\nThanks for reading through this long post.\nYou can find me on LinkedIn or Twitter.\nXiaohan Zeng\n10/22/17\nPS: Since the publication of this post, it has (unexpectedly) received some attention. I would like to thank everybody for the congratulations and shares, and apologize for not being able to respond to each of them.\nThis post has been translated into some other languages:\nIt has been reposted in Tech In Asia.\nBreaking Into Startups invited me to a live video streaming, together with Sophia Ciocca.\nCoverShr did a short QnA with me.\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\nCritical Mind & Romantic Heart"
    },
    {
        "author": "Gil Fewster",
        "claps": "3.3K",
        "reading_time": 5,
        "link": "https://medium.freecodecamp.org/the-mind-blowing-ai-announcement-from-google-that-you-probably-missed-2ffd31334805?source=tag_archive---------5----------------",
        "title": "The mind-blowing AI announcement from Google that you probably missed.",
        "text": "Disclaimer: I‚Äôm not an expert in neural networks or machine learning. Since originally writing this article, many people with far more expertise in these fields than myself have indicated that, while impressive, what Google have achieved is evolutionary, not revolutionary. In the very least, it‚Äôs fair to say that I‚Äôm guilty of anthropomorphising in parts of the text.\nI‚Äôve left the article‚Äôs content unchanged, because I think it‚Äôs interesting to compare the gut reaction I had with the subsequent comments of experts in the field. I strongly encourage readers to browse the comments after reading the article for some perspectives more sober and informed than my own.\nIn the closing weeks of 2016, Google published an article that quietly sailed under most people‚Äôs radars. Which is a shame, because it may just be the most astonishing article about machine learning that I read last year.\nDon‚Äôt feel bad if you missed it. Not only was the article competing with the pre-Christmas rush that most of us were navigating ‚Äî it was also tucked away on Google‚Äôs Research Blog, beneath the geektastic headline Zero-Shot Translation with Google‚Äôs Multilingual Neural Machine Translation System.\nThis doesn‚Äôt exactly scream must read, does it? Especially when you‚Äôve got projects to wind up, gifts to buy, and family feuds to be resolved ‚Äî all while the advent calendar relentlessly counts down the days until Christmas like some kind of chocolate-filled Yuletide doomsday clock.\nLuckily, I‚Äôm here to bring you up to speed. Here‚Äôs the deal.\nUp until September of last year, Google Translate used phrase-based translation. It basically did the same thing you and I do when we look up key words and phrases in our Lonely Planet language guides. It‚Äôs effective enough, and blisteringly fast compared to awkwardly thumbing your way through a bunch of pages looking for the French equivalent of ‚Äúplease bring me all of your cheese and don‚Äôt stop until I fall over.‚Äù But it lacks nuance.\nPhrase-based translation is a blunt instrument. It does the job well enough to get by. But mapping roughly equivalent words and phrases without an understanding of linguistic structures can only produce crude results.\nThis approach is also limited by the extent of an available vocabulary. Phrase-based translation has no capacity to make educated guesses at words it doesn‚Äôt recognize, and can‚Äôt learn from new input.\nAll that changed in September, when Google gave their translation tool a new engine: the Google Neural Machine Translation system (GNMT). This new engine comes fully loaded with all the hot 2016 buzzwords, like neural network and machine learning.\nThe short version is that Google Translate got smart. It developed the ability to learn from the people who used it. It learned how to make educated guesses about the content, tone, and meaning of phrases based on the context of other words and phrases around them. And ‚Äî here‚Äôs the bit that should make your brain explode ‚Äî it got creative.\nGoogle Translate invented its own language to help it translate more effectively.\nWhat‚Äôs more, nobody told it to. It didn‚Äôt develop a language (or interlingua, as Google call it) because it was coded to. It developed a new language because the software determined over time that this was the most efficient way to solve the problem of translation.\nStop and think about that for a moment. Let it sink in. A neural computing system designed to translate content from one human language into another developed its own internal language to make the task more efficient. Without being told to do so. In a matter of weeks. (I‚Äôve added a correction/retraction of this paragraph in the notes)\nTo understand what‚Äôs going on, we need to understand what zero-shot translation capability is. Here‚Äôs Google‚Äôs Mike Schuster, Nikhil Thorat, and Melvin Johnson from the original blog post:\nHere you can see an advantage of Google‚Äôs new neural machine over the old phrase-based approach. The GMNT is able to learn how to translate between two languages without being explicitly taught. This wouldn‚Äôt be possible in a phrase-based model, where translation is dependent upon an explicit dictionary to map words and phrases between each pair of languages being translated.\nAnd this leads the Google engineers onto that truly astonishing discovery of creation:\nSo there you have it. In the last weeks of 2016, as journos around the world started penning their ‚Äúwas this the worst year in living memory‚Äù thinkpieces, Google engineers were quietly documenting a genuinely astonishing breakthrough in software engineering and linguistics.\nI just thought maybe you‚Äôd want to know.\nOk, to really understand what‚Äôs going on we probably need multiple computer science and linguistics degrees. I‚Äôm just barely scraping the surface here. If you‚Äôve got time to get a few degrees (or if you‚Äôve already got them) please drop me a line and explain it all me to. Slowly.\nUpdate 1: in my excitement, it‚Äôs fair to say that I‚Äôve exaggerated the idea of this as an ‚Äòintelligent‚Äô system ‚Äî at least so far as we would think about human intelligence and decision making. Make sure you read Chris McDonald‚Äôs comment after the article for a more sober perspective.\nUpdate 2: Nafrondel‚Äôs excellent, detailed reply is also a must read for an expert explanation of how neural networks function.\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\nA tinkerer\nOur community publishes stories worth reading on development, design, and data science."
    },
    {
        "author": "Adam Geitgey",
        "claps": "10.4K",
        "reading_time": 15,
        "link": "https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3?source=tag_archive---------6----------------",
        "title": "Machine Learning is Fun! Part 2 ‚Äì Adam Geitgey ‚Äì Medium",
        "text": "Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8!\nYou can also read this article in Italiano, EspanÃÉol, FrancÃßais, TuÃàrkcÃße, –†—É—Å—Å–∫–∏–∏ÃÜ, ·Ñí·Ö°·Ü´·ÑÄ·ÖÆ·Ü®·Ñã·Ö• PortugueÃÇs, ŸÅÿßÿ±ÿ≥€å, TieÃÇÃÅng VieÃ£ÃÇt or ÊôÆÈÄöËØù.\nIn Part 1, we said that Machine Learning is using generic algorithms to tell you something interesting about your data without writing any code specific to the problem you are solving. (If you haven‚Äôt already read part 1, read it now!).\nThis time, we are going to see one of these generic algorithms do something really cool ‚Äî create video game levels that look like they were made by humans. We‚Äôll build a neural network, feed it existing Super Mario levels and watch new ones pop out!\nJust like Part 1, this guide is for anyone who is curious about machine learning but has no idea where to start. The goal is be accessible to anyone ‚Äî which means that there‚Äôs a lot of generalizations and we skip lots of details. But who cares? If this gets anyone more interested in ML, then mission accomplished.\nBack in Part 1, we created a simple algorithm that estimated the value of a house based on its attributes. Given data about a house like this:\nWe ended up with this simple estimation function:\nIn other words, we estimated the value of the house by multiplying each of its attributes by a weight. Then we just added those numbers up to get the house‚Äôs value.\nInstead of using code, let‚Äôs represent that same function as a simple diagram:\nHowever this algorithm only works for simple problems where the result has a linear relationship with the input. What if the truth behind house prices isn‚Äôt so simple? For example, maybe the neighborhood matters a lot for big houses and small houses but doesn‚Äôt matter at all for medium-sized houses. How could we capture that kind of complicated detail in our model?\nTo be more clever, we could run this algorithm multiple times with different of weights that each capture different edge cases:\nNow we have four different price estimates. Let‚Äôs combine those four price estimates into one final estimate. We‚Äôll run them through the same algorithm again (but using another set of weights)!\nOur new Super Answer combines the estimates from our four different attempts to solve the problem. Because of this, it can model more cases than we could capture in one simple model.\nLet‚Äôs combine our four attempts to guess into one big diagram:\nThis is a neural network! Each node knows how to take in a set of inputs, apply weights to them, and calculate an output value. By chaining together lots of these nodes, we can model complex functions.\nThere‚Äôs a lot that I‚Äôm skipping over to keep this brief (including feature scaling and the activation function), but the most important part is that these basic ideas click:\nIt‚Äôs just like LEGO! We can‚Äôt model much with one single LEGO block, but we can model anything if we have enough basic LEGO blocks to stick together:\nThe neural network we‚Äôve seen always returns the same answer when you give it the same inputs. It has no memory. In programming terms, it‚Äôs a stateless algorithm.\nIn many cases (like estimating the price of house), that‚Äôs exactly what you want. But the one thing this kind of model can‚Äôt do is respond to patterns in data over time.\nImagine I handed you a keyboard and asked you to write a story. But before you start, my job is to guess the very first letter that you will type. What letter should I guess?\nI can use my knowledge of English to increase my odds of guessing the right letter. For example, you will probably type a letter that is common at the beginning of words. If I looked at stories you wrote in the past, I could narrow it down further based on the words you usually use at the beginning of your stories. Once I had all that data, I could use it to build a neural network to model how likely it is that you would start with any given letter.\nOur model might look like this:\nBut let‚Äôs make the problem harder. Let‚Äôs say I need to guess the next letter you are going to type at any point in your story. This is a much more interesting problem.\nLet‚Äôs use the first few words of Ernest Hemingway‚Äôs The Sun Also Rises as an example:\nWhat letter is going to come next?\nYou probably guessed ‚Äôn‚Äô ‚Äî the word is probably going to be boxing. We know this based on the letters we‚Äôve already seen in the sentence and our knowledge of common words in English. Also, the word ‚Äòmiddleweight‚Äô gives us an extra clue that we are talking about boxing.\nIn other words, it‚Äôs easy to guess the next letter if we take into account the sequence of letters that came right before it and combine that with our knowledge of the rules of English.\nTo solve this problem with a neural network, we need to add state to our model. Each time we ask our neural network for an answer, we also save a set of our intermediate calculations and re-use them the next time as part of our input. That way, our model will adjust its predictions based on the input that it has seen recently.\nKeeping track of state in our model makes it possible to not just predict the most likely first letter in the story, but to predict the most likely next letter given all previous letters.\nThis is the basic idea of a Recurrent Neural Network. We are updating the network each time we use it. This allows it to update its predictions based on what it saw most recently. It can even model patterns over time as long as we give it enough of a memory.\nPredicting the next letter in a story might seem pretty useless. What‚Äôs the point?\nOne cool use might be auto-predict for a mobile phone keyboard:\nBut what if we took this idea to the extreme? What if we asked the model to predict the next most likely character over and over ‚Äî forever? We‚Äôd be asking it to write a complete story for us!\nWe saw how we could guess the next letter in Hemingway‚Äôs sentence. Let‚Äôs try generating a whole story in the style of Hemingway.\nTo do this, we are going to use the Recurrent Neural Network implementation that Andrej Karpathy wrote. Andrej is a Deep-Learning researcher at Stanford and he wrote an excellent introduction to generating text with RNNs, You can view all the code for the model on github.\nWe‚Äôll create our model from the complete text of The Sun Also Rises ‚Äî 362,239 characters using 84 unique letters (including punctuation, uppercase/lowercase, etc). This data set is actually really small compared to typical real-world applications. To generate a really good model of Hemingway‚Äôs style, it would be much better to have at several times as much sample text. But this is good enough to play around with as an example.\nAs we just start to train the RNN, it‚Äôs not very good at predicting letters. Here‚Äôs what it generates after a 100 loops of training:\nYou can see that it has figured out that sometimes words have spaces between them, but that‚Äôs about it.\nAfter about 1000 iterations, things are looking more promising:\nThe model has started to identify the patterns in basic sentence structure. It‚Äôs adding periods at the ends of sentences and even quoting dialog. A few words are recognizable, but there‚Äôs also still a lot of nonsense.\nBut after several thousand more training iterations, it looks pretty good:\nAt this point, the algorithm has captured the basic pattern of Hemingway‚Äôs short, direct dialog. A few sentences even sort of make sense.\nCompare that with some real text from the book:\nEven by only looking for patterns one character at a time, our algorithm has reproduced plausible-looking prose with proper formatting. That is kind of amazing!\nWe don‚Äôt have to generate text completely from scratch, either. We can seed the algorithm by supplying the first few letters and just let it find the next few letters.\nFor fun, let‚Äôs make a fake book cover for our imaginary book by generating a new author name and a new title using the seed text of ‚ÄúEr‚Äù, ‚ÄúHe‚Äù, and ‚ÄúThe S‚Äù:\nNot bad!\nBut the really mind-blowing part is that this algorithm can figure out patterns in any sequence of data. It can easily generate real-looking recipes or fake Obama speeches. But why limit ourselves human language? We can apply this same idea to any kind of sequential data that has a pattern.\nIn 2015, Nintendo released Super Mario MakerTM for the Wii U gaming system.\nThis game lets you draw out your own Super Mario Brothers levels on the gamepad and then upload them to the internet so you friends can play through them. You can include all the classic power-ups and enemies from the original Mario games in your levels. It‚Äôs like a virtual LEGO set for people who grew up playing Super Mario Brothers.\nCan we use the same model that generated fake Hemingway text to generate fake Super Mario Brothers levels?\nFirst, we need a data set for training our model. Let‚Äôs take all the outdoor levels from the original Super Mario Brothers game released in 1985:\nThis game has 32 levels and about 70% of them have the same outdoor style. So we‚Äôll stick to those.\nTo get the designs for each level, I took an original copy of the game and wrote a program to pull the level designs out of the game‚Äôs memory. Super Mario Bros. is a 30-year-old game and there are lots of resources online that help you figure out how the levels were stored in the game‚Äôs memory. Extracting level data from an old video game is a fun programming exercise that you should try sometime.\nHere‚Äôs the first level from the game (which you probably remember if you ever played it):\nIf we look closely, we can see the level is made of a simple grid of objects:\nWe could just as easily represent this grid as a sequence of characters with one character representing each object:\nWe‚Äôve replaced each object in the level with a letter:\n...and so on, using a different letter for each different kind of object in the level.\nI ended up with text files that looked like this:\nLooking at the text file, you can see that Mario levels don‚Äôt really have much of a pattern if you read them line-by-line:\nThe patterns in a level really emerge when you think of the level as a series of columns:\nSo in order for the algorithm to find the patterns in our data, we need to feed the data in column-by-column. Figuring out the most effective representation of your input data (called feature selection) is one of the keys of using machine learning algorithms well.\nTo train the model, I needed to rotate my text files by 90 degrees. This made sure the characters were fed into the model in an order where a pattern would more easily show up:\nJust like we saw when creating the model of Hemingway‚Äôs prose, a model improves as we train it.\nAfter a little training, our model is generating junk:\nIt sort of has an idea that ‚Äò-‚Äôs and ‚Äò=‚Äôs should show up a lot, but that‚Äôs about it. It hasn‚Äôt figured out the pattern yet.\nAfter several thousand iterations, it‚Äôs starting to look like something:\nThe model has almost figured out that each line should be the same length. It has even started to figure out some of the logic of Mario: The pipes in mario are always two blocks wide and at least two blocks high, so the ‚ÄúP‚Äùs in the data should appear in 2x2 clusters. That‚Äôs pretty cool!\nWith a lot more training, the model gets to the point where it generates perfectly valid data:\nLet‚Äôs sample an entire level‚Äôs worth of data from our model and rotate it back horizontal:\nThis data looks great! There are several awesome things to notice:\nFinally, let‚Äôs take this level and recreate it in Super Mario Maker:\nPlay it yourself!\nIf you have Super Mario Maker, you can play this level by bookmarking it online or by looking it up using level code 4AC9‚Äì0000‚Äì0157-F3C3.\nThe recurrent neural network algorithm we used to train our model is the same kind of algorithm used by real-world companies to solve hard problems like speech detection and language translation. What makes our model a ‚Äòtoy‚Äô instead of cutting-edge is that our model is generated from very little data. There just aren‚Äôt enough levels in the original Super Mario Brothers game to provide enough data for a really good model.\nIf we could get access to the hundreds of thousands of user-created Super Mario Maker levels that Nintendo has, we could make an amazing model. But we can‚Äôt ‚Äî because Nintendo won‚Äôt let us have them. Big companies don‚Äôt give away their data for free.\nAs machine learning becomes more important in more industries, the difference between a good program and a bad program will be how much data you have to train your models. That‚Äôs why companies like Google and Facebook need your data so badly!\nFor example, Google recently open sourced TensorFlow, its software toolkit for building large-scale machine learning applications. It was a pretty big deal that Google gave away such important, capable technology for free. This is the same stuff that powers Google Translate.\nBut without Google‚Äôs massive trove of data in every language, you can‚Äôt create a competitor to Google Translate. Data is what gives Google its edge. Think about that the next time you open up your Google Maps Location History or Facebook Location History and notice that it stores every place you‚Äôve ever been.\nIn machine learning, there‚Äôs never a single way to solve a problem. You have limitless options when deciding how to pre-process your data and which algorithms to use. Often combining multiple approaches will give you better results than any single approach.\nReaders have sent me links to other interesting approaches to generating Super Mario levels:\nIf you liked this article, please consider signing up for my Machine Learning is Fun! email list. I‚Äôll only email you when I have something new and awesome to share. It‚Äôs the best way to find out when I write more articles like this.\nYou can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I‚Äôd love to hear from you if I can help you or your team with machine learning.\nNow continue on to Machine Learning is Fun Part 3!\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\nInterested in computers and machine learning. Likes to write about it."
    },
    {
        "author": "David Venturi",
        "claps": "10.6K",
        "reading_time": 20,
        "link": "https://medium.freecodecamp.org/every-single-machine-learning-course-on-the-internet-ranked-by-your-reviews-3c4a7b8026c0?source=tag_archive---------7----------------",
        "title": "Every single Machine Learning course on the internet, ranked by your reviews",
        "text": "A year and a half ago, I dropped out of one of the best computer science programs in Canada. I started creating my own data science master‚Äôs program using online resources. I realized that I could learn everything I needed through edX, Coursera, and Udacity instead. And I could learn it faster, more efficiently, and for a fraction of the cost.\nI‚Äôm almost finished now. I‚Äôve taken many data science-related courses and audited portions of many more. I know the options out there, and what skills are needed for learners preparing for a data analyst or data scientist role. So I started creating a review-driven guide that recommends the best courses for each subject within data science.\nFor the first guide in the series, I recommended a few coding classes for the beginner data scientist. Then it was statistics and probability classes. Then introductions to data science. Also, data visualization.\nFor this guide, I spent a dozen hours trying to identify every online machine learning course offered as of May 2017, extracting key bits of information from their syllabi and reviews, and compiling their ratings. My end goal was to identify the three best courses available and present them to you, below.\nFor this task, I turned to none other than the open source Class Central community, and its database of thousands of course ratings and reviews.\nSince 2011, Class Central founder Dhawal Shah has kept a closer eye on online courses than arguably anyone else in the world. Dhawal personally helped me assemble this list of resources.\nEach course must fit three criteria:\nWe believe we covered every notable course that fits the above criteria. Since there are seemingly hundreds of courses on Udemy, we chose to consider the most-reviewed and highest-rated ones only.\nThere‚Äôs always a chance that we missed something, though. So please let us know in the comments section if we left a good course out.\nWe compiled average ratings and number of reviews from Class Central and other review sites to calculate a weighted average rating for each course. We read text reviews and used this feedback to supplement the numerical ratings.\nWe made subjective syllabus judgment calls based on three factors:\nA popular definition originates from Arthur Samuel in 1959: machine learning is a subfield of computer science that gives ‚Äúcomputers the ability to learn without being explicitly programmed.‚Äù In practice, this means developing computer programs that can make predictions based on data. Just as humans can learn from experience, so can computers, where data = experience.\nA machine learning workflow is the process required for carrying out a machine learning project. Though individual projects can differ, most workflows share several common tasks: problem evaluation, data exploration, data preprocessing, model training/testing/deployment, etc. Below you‚Äôll find helpful visualization of these core steps:\nThe ideal course introduces the entire process and provides interactive examples, assignments, and/or quizzes where students can perform each task themselves.\nFirst off, let‚Äôs define deep learning. Here is a succinct description:\nAs would be expected, portions of some of the machine learning courses contain deep learning content. I chose not to include deep learning-only courses, however. If you are interested in deep learning specifically, we‚Äôve got you covered with the following article:\nMy top three recommendations from that list would be:\nSeveral courses listed below ask students to have prior programming, calculus, linear algebra, and statistics experience. These prerequisites are understandable given that machine learning is an advanced discipline.\nMissing a few subjects? Good news! Some of this experience can be acquired through our recommendations in the first two articles (programming, statistics) of this Data Science Career Guide. Several top-ranked courses below also provide gentle calculus and linear algebra refreshers and highlight the aspects most relevant to machine learning for those less familiar.\nStanford University‚Äôs Machine Learning on Coursera is the clear current winner in terms of ratings, reviews, and syllabus fit. Taught by the famous Andrew Ng, Google Brain founder and former chief scientist at Baidu, this was the class that sparked the founding of Coursera. It has a 4.7-star weighted average rating over 422 reviews.\nReleased in 2011, it covers all aspects of the machine learning workflow. Though it has a smaller scope than the original Stanford class upon which it is based, it still manages to cover a large number of techniques and algorithms. The estimated timeline is eleven weeks, with two weeks dedicated to neural networks and deep learning. Free and paid options are available.\nNg is a dynamic yet gentle instructor with a palpable experience. He inspires confidence, especially when sharing practical implementation tips and warnings about common pitfalls. A linear algebra refresher is provided and Ng highlights the aspects of calculus most relevant to machine learning.\nEvaluation is automatic and is done via multiple choice quizzes that follow each lesson and programming assignments. The assignments (there are eight of them) can be completed in MATLAB or Octave, which is an open-source version of MATLAB. Ng explains his language choice:\nThough Python and R are likely more compelling choices in 2017 with the increased popularity of those languages, reviewers note that that shouldn‚Äôt stop you from taking the course.\nA few prominent reviewers noted the following:\nColumbia University‚Äôs Machine Learning is a relatively new offering that is part of their Artificial Intelligence MicroMasters on edX. Though it is newer and doesn‚Äôt have a large number of reviews, the ones that it does have are exceptionally strong. Professor John Paisley is noted as brilliant, clear, and clever. It has a 4.8-star weighted average rating over 10 reviews.\nThe course also covers all aspects of the machine learning workflow and more algorithms than the above Stanford offering. Columbia‚Äôs is a more advanced introduction, with reviewers noting that students should be comfortable with the recommended prerequisites (calculus, linear algebra, statistics, probability, and coding).\nQuizzes (11), programming assignments (4), and a final exam are the modes of evaluation. Students can use either Python, Octave, or MATLAB to complete the assignments. The course‚Äôs total estimated timeline is eight to ten hours per week over twelve weeks. It is free with a verified certificate available for purchase.\nBelow are a few of the aforementioned sparkling reviews:\nMachine Learning A-ZTM on Udemy is an impressively detailed offering that provides instruction in both Python and R, which is rare and can‚Äôt be said for any of the other top courses. It has a 4.5-star weighted average rating over 8,119 reviews, which makes it the most reviewed course of the ones considered.\nIt covers the entire machine learning workflow and an almost ridiculous (in a good way) number of algorithms through 40.5 hours of on-demand video. The course takes a more applied approach and is lighter math-wise than the above two courses. Each section starts with an ‚Äúintuition‚Äù video from Eremenko that summarizes the underlying theory of the concept being taught. de Ponteves then walks through implementation with separate videos for both Python and R.\nAs a ‚Äúbonus,‚Äù the course includes Python and R code templates for students to download and use on their own projects. There are quizzes and homework challenges, though these aren‚Äôt the strong points of the course.\nEremenko and the SuperDataScience team are revered for their ability to ‚Äúmake the complex simple.‚Äù Also, the prerequisites listed are ‚Äújust some high school mathematics,‚Äù so this course might be a better option for those daunted by the Stanford and Columbia offerings.\nA few prominent reviewers noted the following:\nOur #1 pick had a weighted average rating of 4.7 out of 5 stars over 422 reviews. Let‚Äôs look at the other alternatives, sorted by descending rating. A reminder that deep learning-only courses are not included in this guide ‚Äî you can find those here.\nThe Analytics Edge (Massachusetts Institute of Technology/edX): More focused on analytics in general, though it does cover several machine learning topics. Uses R. Strong narrative that leverages familiar real-world examples. Challenging. Ten to fifteen hours per week over twelve weeks. Free with a verified certificate available for purchase. It has a 4.9-star weighted average rating over 214 reviews.\nPython for Data Science and Machine Learning Bootcamp (Jose Portilla/Udemy): Has large chunks of machine learning content, but covers the whole data science process. More of a very detailed intro to Python. Amazing course, though not ideal for the scope of this guide. 21.5 hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.6-star weighted average rating over 3316 reviews.\nData Science and Machine Learning Bootcamp with R (Jose Portilla/Udemy): The comments for Portilla‚Äôs above course apply here as well, except for R. 17.5 hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.6-star weighted average rating over 1317 reviews.\nMachine Learning Series (Lazy Programmer Inc./Udemy): Taught by a data scientist/big data engineer/full stack software engineer with an impressive resume, Lazy Programmer currently has a series of 16 machine learning-focused courses on Udemy. In total, the courses have 5000+ ratings and almost all of them have 4.6 stars. A useful course ordering is provided in each individual course‚Äôs description. Uses Python. Cost varies depending on Udemy discounts, which are frequent.\nMachine Learning (Georgia Tech/Udacity): A compilation of what was three separate courses: Supervised, Unsupervised and Reinforcement Learning. Part of Udacity‚Äôs Machine Learning Engineer Nanodegree and Georgia Tech‚Äôs Online Master‚Äôs Degree (OMS). Bite-sized videos, as is Udacity‚Äôs style. Friendly professors. Estimated timeline of four months. Free. It has a 4.56-star weighted average rating over 9 reviews.\nImplementing Predictive Analytics with Spark in Azure HDInsight (Microsoft/edX): Introduces the core concepts of machine learning and a variety of algorithms. Leverages several big data-friendly tools, including Apache Spark, Scala, and Hadoop. Uses both Python and R. Four hours per week over six weeks. Free with a verified certificate available for purchase. It has a 4.5-star weighted average rating over 6 reviews.\nData Science and Machine Learning with Python ‚Äî Hands On! (Frank Kane/Udemy): Uses Python. Kane has nine years of experience at Amazon and IMDb. Nine hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.5-star weighted average rating over 4139 reviews.\nScala and Spark for Big Data and Machine Learning (Jose Portilla/Udemy): ‚ÄúBig data‚Äù focus, specifically on implementation in Scala and Spark. Ten hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.5-star weighted average rating over 607 reviews.\nMachine Learning Engineer Nanodegree (Udacity): Udacity‚Äôs flagship Machine Learning program, which features a best-in-class project review system and career support. The program is a compilation of several individual Udacity courses, which are free. Co-created by Kaggle. Estimated timeline of six months. Currently costs $199 USD per month with a 50% tuition refund available for those who graduate within 12 months. It has a 4.5-star weighted average rating over 2 reviews.\nLearning From Data (Introductory Machine Learning) (California Institute of Technology/edX): Enrollment is currently closed on edX, but is also available via CalTech‚Äôs independent platform (see below). It has a 4.49-star weighted average rating over 42 reviews.\nLearning From Data (Introductory Machine Learning) (Yaser Abu-Mostafa/California Institute of Technology): ‚ÄúA real Caltech course, not a watered-down version.‚Äù Reviews note it is excellent for understanding machine learning theory. The professor, Yaser Abu-Mostafa, is popular among students and also wrote the textbook upon which this course is based. Videos are taped lectures (with lectures slides picture-in-picture) uploaded to YouTube. Homework assignments are .pdf files. The course experience for online students isn‚Äôt as polished as the top three recommendations. It has a 4.43-star weighted average rating over 7 reviews.\nMining Massive Datasets (Stanford University): Machine learning with a focus on ‚Äúbig data.‚Äù Introduces modern distributed file systems and MapReduce. Ten hours per week over seven weeks. Free. It has a 4.4-star weighted average rating over 30 reviews.\nAWS Machine Learning: A Complete Guide With Python (Chandra Lingam/Udemy): A unique focus on cloud-based machine learning and specifically Amazon Web Services. Uses Python. Nine hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.4-star weighted average rating over 62 reviews.\nIntroduction to Machine Learning & Face Detection in Python (Holczer Balazs/Udemy): Uses Python. Eight hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.4-star weighted average rating over 162 reviews.\nStatLearning: Statistical Learning (Stanford University): Based on the excellent textbook, ‚ÄúAn Introduction to Statistical Learning, with Applications in R‚Äù and taught by the professors who wrote it. Reviewers note that the MOOC isn‚Äôt as good as the book, citing ‚Äúthin‚Äù exercises and mediocre videos. Five hours per week over nine weeks. Free. It has a 4.35-star weighted average rating over 84 reviews.\nMachine Learning Specialization (University of Washington/Coursera): Great courses, but last two classes (including the capstone project) were canceled. Reviewers note that this series is more digestable (read: easier for those without strong technical backgrounds) than other top machine learning courses (e.g. Stanford‚Äôs or Caltech‚Äôs). Be aware that the series is incomplete with recommender systems, deep learning, and a summary missing. Free and paid options available. It has a 4.31-star weighted average rating over 80 reviews.\nFrom 0 to 1: Machine Learning, NLP & Python-Cut to the Chase (Loony Corn/Udemy): ‚ÄúA down-to-earth, shy but confident take on machine learning techniques.‚Äù Taught by four-person team with decades of industry experience together. Uses Python. Cost varies depending on Udemy discounts, which are frequent. It has a 4.2-star weighted average rating over 494 reviews.\nPrinciples of Machine Learning (Microsoft/edX): Uses R, Python, and Microsoft Azure Machine Learning. Part of the Microsoft Professional Program Certificate in Data Science. Three to four hours per week over six weeks. Free with a verified certificate available for purchase. It has a 4.09-star weighted average rating over 11 reviews.\nBig Data: Statistical Inference and Machine Learning (Queensland University of Technology/FutureLearn): A nice, brief exploratory machine learning course with a focus on big data. Covers a few tools like R, H2O Flow, and WEKA. Only three weeks in duration at a recommended two hours per week, but one reviewer noted that six hours per week would be more appropriate. Free and paid options available. It has a 4-star weighted average rating over 4 reviews.\nGenomic Data Science and Clustering (Bioinformatics V) (University of California, San Diego/Coursera): For those interested in the intersection of computer science and biology and how it represents an important frontier in modern science. Focuses on clustering and dimensionality reduction. Part of UCSD‚Äôs Bioinformatics Specialization. Free and paid options available. It has a 4-star weighted average rating over 3 reviews.\nIntro to Machine Learning (Udacity): Prioritizes topic breadth and practical tools (in Python) over depth and theory. The instructors, Sebastian Thrun and Katie Malone, make this class so fun. Consists of bite-sized videos and quizzes followed by a mini-project for each lesson. Currently part of Udacity‚Äôs Data Analyst Nanodegree. Estimated timeline of ten weeks. Free. It has a 3.95-star weighted average rating over 19 reviews.\nMachine Learning for Data Analysis (Wesleyan University/Coursera): A brief intro machine learning and a few select algorithms. Covers decision trees, random forests, lasso regression, and k-means clustering. Part of Wesleyan‚Äôs Data Analysis and Interpretation Specialization. Estimated timeline of four weeks. Free and paid options available. It has a 3.6-star weighted average rating over 5 reviews.\nProgramming with Python for Data Science (Microsoft/edX): Produced by Microsoft in partnership with Coding Dojo. Uses Python. Eight hours per week over six weeks. Free and paid options available. It has a 3.46-star weighted average rating over 37 reviews.\nMachine Learning for Trading (Georgia Tech/Udacity): Focuses on applying probabilistic machine learning approaches to trading decisions. Uses Python. Part of Udacity‚Äôs Machine Learning Engineer Nanodegree and Georgia Tech‚Äôs Online Master‚Äôs Degree (OMS). Estimated timeline of four months. Free. It has a 3.29-star weighted average rating over 14 reviews.\nPractical Machine Learning (Johns Hopkins University/Coursera): A brief, practical introduction to a number of machine learning algorithms. Several one/two-star reviews expressing a variety of concerns. Part of JHU‚Äôs Data Science Specialization. Four to nine hours per week over four weeks. Free and paid options available. It has a 3.11-star weighted average rating over 37 reviews.\nMachine Learning for Data Science and Analytics (Columbia University/edX): Introduces a wide range of machine learning topics. Some passionate negative reviews with concerns including content choices, a lack of programming assignments, and uninspiring presentation. Seven to ten hours per week over five weeks. Free with a verified certificate available for purchase. It has a 2.74-star weighted average rating over 36 reviews.\nRecommender Systems Specialization (University of Minnesota/Coursera): Strong focus one specific type of machine learning ‚Äî recommender systems. A four course specialization plus a capstone project, which is a case study. Taught using LensKit (an open-source toolkit for recommender systems). Free and paid options available. It has a 2-star weighted average rating over 2 reviews.\nMachine Learning With Big Data (University of California, San Diego/Coursera): Terrible reviews that highlight poor instruction and evaluation. Some noted it took them mere hours to complete the whole course. Part of UCSD‚Äôs Big Data Specialization. Free and paid options available. It has a 1.86-star weighted average rating over 14 reviews.\nPractical Predictive Analytics: Models and Methods (University of Washington/Coursera): A brief intro to core machine learning concepts. One reviewer noted that there was a lack of quizzes and that the assignments were not challenging. Part of UW‚Äôs Data Science at Scale Specialization. Six to eight hours per week over four weeks. Free and paid options available. It has a 1.75-star weighted average rating over 4 reviews.\nThe following courses had one or no reviews as of May 2017.\nMachine Learning for Musicians and Artists (Goldsmiths, University of London/Kadenze): Unique. Students learn algorithms, software tools, and machine learning best practices to make sense of human gesture, musical audio, and other real-time data. Seven sessions in length. Audit (free) and premium ($10 USD per month) options available. It has one 5-star review.\nApplied Machine Learning in Python (University of Michigan/Coursera): Taught using Python and the scikit learn toolkit. Part of the Applied Data Science with Python Specialization. Scheduled to start May 29th. Free and paid options available.\nApplied Machine Learning (Microsoft/edX): Taught using various tools, including Python, R, and Microsoft Azure Machine Learning (note: Microsoft produces the course). Includes hands-on labs to reinforce the lecture content. Three to four hours per week over six weeks. Free with a verified certificate available for purchase.\nMachine Learning with Python (Big Data University): Taught using Python. Targeted towards beginners. Estimated completion time of four hours. Big Data University is affiliated with IBM. Free.\nMachine Learning with Apache SystemML (Big Data University): Taught using Apache SystemML, which is a declarative style language designed for large-scale machine learning. Estimated completion time of eight hours. Big Data University is affiliated with IBM. Free.\nMachine Learning for Data Science (University of California, San Diego/edX): Doesn‚Äôt launch until January 2018. Programming examples and assignments are in Python, using Jupyter notebooks. Eight hours per week over ten weeks. Free with a verified certificate available for purchase.\nIntroduction to Analytics Modeling (Georgia Tech/edX): The course advertises R as its primary programming tool. Five to ten hours per week over ten weeks. Free with a verified certificate available for purchase.\nPredictive Analytics: Gaining Insights from Big Data (Queensland University of Technology/FutureLearn): Brief overview of a few algorithms. Uses Hewlett Packard Enterprise‚Äôs Vertica Analytics platform as an applied tool. Start date to be announced. Two hours per week over four weeks. Free with a Certificate of Achievement available for purchase.\nIntroduccioÃÅn al Machine Learning (Universitas TelefoÃÅnica/MiriÃÅada X): Taught in Spanish. An introduction to machine learning that covers supervised and unsupervised learning. A total of twenty estimated hours over four weeks.\nMachine Learning Path Step (Dataquest): Taught in Python using Dataquest‚Äôs interactive in-browser platform. Multiple guided projects and a ‚Äúplus‚Äù project where you build your own machine learning system using your own data. Subscription required.\nThe following six courses are offered by DataCamp. DataCamp‚Äôs hybrid teaching style leverages video and text-based instruction with lots of examples through an in-browser code editor. A subscription is required for full access to each course.\nIntroduction to Machine Learning (DataCamp): Covers classification, regression, and clustering algorithms. Uses R. Fifteen videos and 81 exercises with an estimated timeline of six hours.\nSupervised Learning with scikit-learn (DataCamp): Uses Python and scikit-learn. Covers classification and regression algorithms. Seventeen videos and 54 exercises with an estimated timeline of four hours.\nUnsupervised Learning in R (DataCamp): Provides a basic introduction to clustering and dimensionality reduction in R. Sixteen videos and 49 exercises with an estimated timeline of four hours.\nMachine Learning Toolbox (DataCamp): Teaches the ‚Äúbig ideas‚Äù in machine learning. Uses R. 24 videos and 88 exercises with an estimated timeline of four hours.\nMachine Learning with the Experts: School Budgets (DataCamp): A case study from a machine learning competition on DrivenData. Involves building a model to automatically classify items in a school‚Äôs budget. DataCamp‚Äôs ‚ÄúSupervised Learning with scikit-learn‚Äù is a prerequisite. Fifteen videos and 51 exercises with an estimated timeline of four hours.\nUnsupervised Learning in Python (DataCamp): Covers a variety of unsupervised learning algorithms using Python, scikit-learn, and scipy. The course ends with students building a recommender system to recommend popular musical artists. Thirteen videos and 52 exercises with an estimated timeline of four hours.\nMachine Learning (Tom Mitchell/Carnegie Mellon University): Carnegie Mellon‚Äôs graduate introductory machine learning course. A prerequisite to their second graduate level course, ‚ÄúStatistical Machine Learning.‚Äù Taped university lectures with practice problems, homework assignments, and a midterm (all with solutions) posted online. A 2011 version of the course also exists. CMU is one of the best graduate schools for studying machine learning and has a whole department dedicated to ML. Free.\nStatistical Machine Learning (Larry Wasserman/Carnegie Mellon University): Likely the most advanced course in this guide. A follow-up to Carnegie Mellon‚Äôs Machine Learning course. Taped university lectures with practice problems, homework assignments, and a midterm (all with solutions) posted online. Free.\nUndergraduate Machine Learning (Nando de Freitas/University of British Columbia): An undergraduate machine learning course. Lectures are filmed and put on YouTube with the slides posted on the course website. The course assignments are posted as well (no solutions, though). de Freitas is now a full-time professor at the University of Oxford and receives praise for his teaching abilities in various forums. Graduate version available (see below).\nMachine Learning (Nando de Freitas/University of British Columbia): A graduate machine learning course. The comments in de Freitas‚Äô undergraduate course (above) apply here as well.\nThis is the fifth of a six-piece series that covers the best online courses for launching yourself into the data science field. We covered programming in the first article, statistics and probability in the second article, intros to data science in the third article, and data visualization in the fourth.\nThe final piece will be a summary of those articles, plus the best online courses for other key topics such as data wrangling, databases, and even software engineering.\nIf you‚Äôre looking for a complete list of Data Science online courses, you can find them on Class Central‚Äôs Data Science and Big Data subject page.\nIf you enjoyed reading this, check out some of Class Central‚Äôs other pieces:\nIf you have suggestions for courses I missed, let me know in the responses!\nIf you found this helpful, click the üíö so more people will see it here on Medium.\nThis is a condensed version of my original article published on Class Central, where I‚Äôve included detailed course syllabi.\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\nCurriculum Lead, Projects @ DataCamp. I created my own data science master‚Äôs program.\nOur community publishes stories worth reading on development, design, and data science."
    },
    {
        "author": "Michael Jordan",
        "claps": "34K",
        "reading_time": 16,
        "link": "https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e7?source=tag_archive---------8----------------",
        "title": "Artificial Intelligence ‚Äî The Revolution Hasn‚Äôt Happened Yet",
        "text": "Artificial Intelligence (AI) is the mantra of the current era. The phrase is intoned by technologists, academicians, journalists and venture capitalists alike. As with many phrases that cross over from technical academic fields into general circulation, there is significant misunderstanding accompanying the use of the phrase. But this is not the classical case of the public not understanding the scientists ‚Äî here the scientists are often as befuddled as the public. The idea that our era is somehow seeing the emergence of an intelligence in silicon that rivals our own entertains all of us ‚Äî enthralling us and frightening us in equal measure. And, unfortunately, it distracts us.\nThere is a different narrative that one can tell about the current era. Consider the following story, which involves humans, computers, data and life-or-death decisions, but where the focus is something other than intelligence-in-silicon fantasies. When my spouse was pregnant 14 years ago, we had an ultrasound. There was a geneticist in the room, and she pointed out some white spots around the heart of the fetus. ‚ÄúThose are markers for Down syndrome,‚Äù she noted, ‚Äúand your risk has now gone up to 1 in 20.‚Äù She further let us know that we could learn whether the fetus in fact had the genetic modification underlying Down syndrome via an amniocentesis. But amniocentesis was risky ‚Äî the risk of killing the fetus during the procedure was roughly 1 in 300. Being a statistician, I determined to find out where these numbers were coming from. To cut a long story short, I discovered that a statistical analysis had been done a decade previously in the UK, where these white spots, which reflect calcium buildup, were indeed established as a predictor of Down syndrome. But I also noticed that the imaging machine used in our test had a few hundred more pixels per square inch than the machine used in the UK study. I went back to tell the geneticist that I believed that the white spots were likely false positives ‚Äî that they were literally ‚Äúwhite noise.‚Äù She said ‚ÄúAh, that explains why we started seeing an uptick in Down syndrome diagnoses a few years ago; it‚Äôs when the new machine arrived.‚Äù\nWe didn‚Äôt do the amniocentesis, and a healthy girl was born a few months later. But the episode troubled me, particularly after a back-of-the-envelope calculation convinced me that many thousands of people had gotten that diagnosis that same day worldwide, that many of them had opted for amniocentesis, and that a number of babies had died needlessly. And this happened day after day until it somehow got fixed. The problem that this episode revealed wasn‚Äôt about my individual medical care; it was about a medical system that measured variables and outcomes in various places and times, conducted statistical analyses, and made use of the results in other places and times. The problem had to do not just with data analysis per se, but with what database researchers call ‚Äúprovenance‚Äù ‚Äî broadly, where did data arise, what inferences were drawn from the data, and how relevant are those inferences to the present situation? While a trained human might be able to work all of this out on a case-by-case basis, the issue was that of designing a planetary-scale medical system that could do this without the need for such detailed human oversight.\nI‚Äôm also a computer scientist, and it occurred to me that the principles needed to build planetary-scale inference-and-decision-making systems of this kind, blending computer science with statistics, and taking into account human utilities, were nowhere to be found in my education. And it occurred to me that the development of such principles ‚Äî which will be needed not only in the medical domain but also in domains such as commerce, transportation and education ‚Äî were at least as important as those of building AI systems that can dazzle us with their game-playing or sensorimotor skills.\nWhether or not we come to understand ‚Äúintelligence‚Äù any time soon, we do have a major challenge on our hands in bringing together computers and humans in ways that enhance human life. While this challenge is viewed by some as subservient to the creation of ‚Äúartificial intelligence,‚Äù it can also be viewed more prosaically ‚Äî but with no less reverence ‚Äî as the creation of a new branch of engineering. Much like civil engineering and chemical engineering in decades past, this new discipline aims to corral the power of a few key ideas, bringing new resources and capabilities to people, and doing so safely. Whereas civil engineering and chemical engineering were built on physics and chemistry, this new engineering discipline will be built on ideas that the preceding century gave substance to ‚Äî ideas such as ‚Äúinformation,‚Äù ‚Äúalgorithm,‚Äù ‚Äúdata,‚Äù ‚Äúuncertainty,‚Äù ‚Äúcomputing,‚Äù ‚Äúinference,‚Äù and ‚Äúoptimization.‚Äù Moreover, since much of the focus of the new discipline will be on data from and about humans, its development will require perspectives from the social sciences and humanities.\nWhile the building blocks have begun to emerge, the principles for putting these blocks together have not yet emerged, and so the blocks are currently being put together in ad-hoc ways.\nThus, just as humans built buildings and bridges before there was civil engineering, humans are proceeding with the building of societal-scale, inference-and-decision-making systems that involve machines, humans and the environment. Just as early buildings and bridges sometimes fell to the ground ‚Äî in unforeseen ways and with tragic consequences ‚Äî many of our early societal-scale inference-and-decision-making systems are already exposing serious conceptual flaws.\nAnd, unfortunately, we are not very good at anticipating what the next emerging serious flaw will be. What we‚Äôre missing is an engineering discipline with its principles of analysis and design.\nThe current public dialog about these issues too often uses ‚ÄúAI‚Äù as an intellectual wildcard, one that makes it difficult to reason about the scope and consequences of emerging technology. Let us begin by considering more carefully what ‚ÄúAI‚Äù has been used to refer to, both recently and historically.\nMost of what is being called ‚ÄúAI‚Äù today, particularly in the public sphere, is what has been called ‚ÄúMachine Learning‚Äù (ML) for the past several decades. ML is an algorithmic field that blends ideas from statistics, computer science and many other disciplines (see below) to design algorithms that process data, make predictions and help make decisions. In terms of impact on the real world, ML is the real thing, and not just recently. Indeed, that ML would grow into massive industrial relevance was already clear in the early 1990s, and by the turn of the century forward-looking companies such as Amazon were already using ML throughout their business, solving mission-critical back-end problems in fraud detection and supply-chain prediction, and building innovative consumer-facing services such as recommendation systems. As datasets and computing resources grew rapidly over the ensuing two decades, it became clear that ML would soon power not only Amazon but essentially any company in which decisions could be tied to large-scale data. New business models would emerge. The phrase ‚ÄúData Science‚Äù began to be used to refer to this phenomenon, reflecting the need of ML algorithms experts to partner with database and distributed-systems experts to build scalable, robust ML systems, and reflecting the larger social and environmental scope of the resulting systems.\nThis confluence of ideas and technology trends has been rebranded as ‚ÄúAI‚Äù over the past few years. This rebranding is worthy of some scrutiny.\nHistorically, the phrase ‚ÄúAI‚Äù was coined in the late 1950‚Äôs to refer to the heady aspiration of realizing in software and hardware an entity possessing human-level intelligence. We will use the phrase ‚Äúhuman-imitative AI‚Äù to refer to this aspiration, emphasizing the notion that the artificially intelligent entity should seem to be one of us, if not physically at least mentally (whatever that might mean). This was largely an academic enterprise. While related academic fields such as operations research, statistics, pattern recognition, information theory and control theory already existed, and were often inspired by human intelligence (and animal intelligence), these fields were arguably focused on ‚Äúlow-level‚Äù signals and decisions. The ability of, say, a squirrel to perceive the three-dimensional structure of the forest it lives in, and to leap among its branches, was inspirational to these fields. ‚ÄúAI‚Äù was meant to focus on something different ‚Äî the ‚Äúhigh-level‚Äù or ‚Äúcognitive‚Äù capability of humans to ‚Äúreason‚Äù and to ‚Äúthink.‚Äù Sixty years later, however, high-level reasoning and thought remain elusive. The developments which are now being called ‚ÄúAI‚Äù arose mostly in the engineering fields associated with low-level pattern recognition and movement control, and in the field of statistics ‚Äî the discipline focused on finding patterns in data and on making well-founded predictions, tests of hypotheses and decisions.\nIndeed, the famous ‚Äúbackpropagation‚Äù algorithm that was rediscovered by David Rumelhart in the early 1980s, and which is now viewed as being at the core of the so-called ‚ÄúAI revolution,‚Äù first arose in the field of control theory in the 1950s and 1960s. One of its early applications was to optimize the thrusts of the Apollo spaceships as they headed towards the moon.\nSince the 1960s much progress has been made, but it has arguably not come about from the pursuit of human-imitative AI. Rather, as in the case of the Apollo spaceships, these ideas have often been hidden behind the scenes, and have been the handiwork of researchers focused on specific engineering challenges. Although not visible to the general public, research and systems-building in areas such as document retrieval, text classification, fraud detection, recommendation systems, personalized search, social network analysis, planning, diagnostics and A/B testing have been a major success ‚Äî these are the advances that have powered companies such as Google, Netflix, Facebook and Amazon.\nOne could simply agree to refer to all of this as ‚ÄúAI,‚Äù and indeed that is what appears to have happened. Such labeling may come as a surprise to optimization or statistics researchers, who wake up to find themselves suddenly referred to as ‚ÄúAI researchers.‚Äù But labeling of researchers aside, the bigger problem is that the use of this single, ill-defined acronym prevents a clear understanding of the range of intellectual and commercial issues at play.\nThe past two decades have seen major progress ‚Äî in industry and academia ‚Äî in a complementary aspiration to human-imitative AI that is often referred to as ‚ÄúIntelligence Augmentation‚Äù (IA). Here computation and data are used to create services that augment human intelligence and creativity. A search engine can be viewed as an example of IA (it augments human memory and factual knowledge), as can natural language translation (it augments the ability of a human to communicate). Computing-based generation of sounds and images serves as a palette and creativity enhancer for artists. While services of this kind could conceivably involve high-level reasoning and thought, currently they don‚Äôt ‚Äî they mostly perform various kinds of string-matching and numerical operations that capture patterns that humans can make use of.\nHoping that the reader will tolerate one last acronym, let us conceive broadly of a discipline of ‚ÄúIntelligent Infrastructure‚Äù (II), whereby a web of computation, data and physical entities exists that makes human environments more supportive, interesting and safe. Such infrastructure is beginning to make its appearance in domains such as transportation, medicine, commerce and finance, with vast implications for individual humans and societies. This emergence sometimes arises in conversations about an ‚ÄúInternet of Things,‚Äù but that effort generally refers to the mere problem of getting ‚Äúthings‚Äù onto the Internet ‚Äî not to the far grander set of challenges associated with these ‚Äúthings‚Äù capable of analyzing those data streams to discover facts about the world, and interacting with humans and other ‚Äúthings‚Äù at a far higher level of abstraction than mere bits.\nFor example, returning to my personal anecdote, we might imagine living our lives in a ‚Äúsocietal-scale medical system‚Äù that sets up data flows, and data-analysis flows, between doctors and devices positioned in and around human bodies, thereby able to aid human intelligence in making diagnoses and providing care. The system would incorporate information from cells in the body, DNA, blood tests, environment, population genetics and the vast scientific literature on drugs and treatments. It would not just focus on a single patient and a doctor, but on relationships among all humans ‚Äî just as current medical testing allows experiments done on one set of humans (or animals) to be brought to bear in the care of other humans. It would help maintain notions of relevance, provenance and reliability, in the way that the current banking system focuses on such challenges in the domain of finance and payment. And, while one can foresee many problems arising in such a system ‚Äî involving privacy issues, liability issues, security issues, etc ‚Äî these problems should properly be viewed as challenges, not show-stoppers.\nWe now come to a critical issue: Is working on classical human-imitative AI the best or only way to focus on these larger challenges? Some of the most heralded recent success stories of ML have in fact been in areas associated with human-imitative AI ‚Äî areas such as computer vision, speech recognition, game-playing and robotics. So perhaps we should simply await further progress in domains such as these. There are two points to make here. First, although one would not know it from reading the newspapers, success in human-imitative AI has in fact been limited ‚Äî we are very far from realizing human-imitative AI aspirations. Unfortunately the thrill (and fear) of making even limited progress on human-imitative AI gives rise to levels of over-exuberance and media attention that is not present in other areas of engineering.\nSecond, and more importantly, success in these domains is neither sufficient nor necessary to solve important IA and II problems. On the sufficiency side, consider self-driving cars. For such technology to be realized, a range of engineering problems will need to be solved that may have little relationship to human competencies (or human lack-of-competencies). The overall transportation system (an II system) will likely more closely resemble the current air-traffic control system than the current collection of loosely-coupled, forward-facing, inattentive human drivers. It will be vastly more complex than the current air-traffic control system, specifically in its use of massive amounts of data and adaptive statistical modeling to inform fine-grained decisions. It is those challenges that need to be in the forefront, and in such an effort a focus on human-imitative AI may be a distraction.\nAs for the necessity argument, it is sometimes argued that the human-imitative AI aspiration subsumes IA and II aspirations, because a human-imitative AI system would not only be able to solve the classical problems of AI (as embodied, e.g., in the Turing test), but it would also be our best bet for solving IA and II problems. Such an argument has little historical precedent. Did civil engineering develop by envisaging the creation of an artificial carpenter or bricklayer? Should chemical engineering have been framed in terms of creating an artificial chemist? Even more polemically: if our goal was to build chemical factories, should we have first created an artificial chemist who would have then worked out how to build a chemical factory?\nA related argument is that human intelligence is the only kind of intelligence that we know, and that we should aim to mimic it as a first step. But humans are in fact not very good at some kinds of reasoning ‚Äî we have our lapses, biases and limitations. Moreover, critically, we did not evolve to perform the kinds of large-scale decision-making that modern II systems must face, nor to cope with the kinds of uncertainty that arise in II contexts. One could argue that an AI system would not only imitate human intelligence, but also ‚Äúcorrect‚Äù it, and would also scale to arbitrarily large problems. But we are now in the realm of science fiction ‚Äî such speculative arguments, while entertaining in the setting of fiction, should not be our principal strategy going forward in the face of the critical IA and II problems that are beginning to emerge. We need to solve IA and II problems on their own merits, not as a mere corollary to a human-imitative AI agenda.\nIt is not hard to pinpoint algorithmic and infrastructure challenges in II systems that are not central themes in human-imitative AI research. II systems require the ability to manage distributed repositories of knowledge that are rapidly changing and are likely to be globally incoherent. Such systems must cope with cloud-edge interactions in making timely, distributed decisions and they must deal with long-tail phenomena whereby there is lots of data on some individuals and little data on most individuals. They must address the difficulties of sharing data across administrative and competitive boundaries. Finally, and of particular importance, II systems must bring economic ideas such as incentives and pricing into the realm of the statistical and computational infrastructures that link humans to each other and to valued goods. Such II systems can be viewed as not merely providing a service, but as creating markets. There are domains such as music, literature and journalism that are crying out for the emergence of such markets, where data analysis links producers and consumers. And this must all be done within the context of evolving societal, ethical and legal norms.\nOf course, classical human-imitative AI problems remain of great interest as well. However, the current focus on doing AI research via the gathering of data, the deployment of ‚Äúdeep learning‚Äù infrastructure, and the demonstration of systems that mimic certain narrowly-defined human skills ‚Äî with little in the way of emerging explanatory principles ‚Äî tends to deflect attention from major open problems in classical AI. These problems include the need to bring meaning and reasoning into systems that perform natural language processing, the need to infer and represent causality, the need to develop computationally-tractable representations of uncertainty and the need to develop systems that formulate and pursue long-term goals. These are classical goals in human-imitative AI, but in the current hubbub over the ‚ÄúAI revolution,‚Äù it is easy to forget that they are not yet solved.\nIA will also remain quite essential, because for the foreseeable future, computers will not be able to match humans in their ability to reason abstractly about real-world situations. We will need well-thought-out interactions of humans and computers to solve our most pressing problems. And we will want computers to trigger new levels of human creativity, not replace human creativity (whatever that might mean).\nIt was John McCarthy (while a professor at Dartmouth, and soon to take a position at MIT) who coined the term ‚ÄúAI,‚Äù apparently to distinguish his budding research agenda from that of Norbert Wiener (then an older professor at MIT). Wiener had coined ‚Äúcybernetics‚Äù to refer to his own vision of intelligent systems ‚Äî a vision that was closely tied to operations research, statistics, pattern recognition, information theory and control theory. McCarthy, on the other hand, emphasized the ties to logic. In an interesting reversal, it is Wiener‚Äôs intellectual agenda that has come to dominate in the current era, under the banner of McCarthy‚Äôs terminology. (This state of affairs is surely, however, only temporary; the pendulum swings more in AI than in most fields.)\nBut we need to move beyond the particular historical perspectives of McCarthy and Wiener.\nWe need to realize that the current public dialog on AI ‚Äî which focuses on a narrow subset of industry and a narrow subset of academia ‚Äî risks blinding us to the challenges and opportunities that are presented by the full scope of AI, IA and II.\nThis scope is less about the realization of science-fiction dreams or nightmares of super-human machines, and more about the need for humans to understand and shape technology as it becomes ever more present and influential in their daily lives. Moreover, in this understanding and shaping there is a need for a diverse set of voices from all walks of life, not merely a dialog among the technologically attuned. Focusing narrowly on human-imitative AI prevents an appropriately wide range of voices from being heard.\nWhile industry will continue to drive many developments, academia will also continue to play an essential role, not only in providing some of the most innovative technical ideas, but also in bringing researchers from the computational and statistical disciplines together with researchers from other disciplines whose contributions and perspectives are sorely needed ‚Äî notably the social sciences, the cognitive sciences and the humanities.\nOn the other hand, while the humanities and the sciences are essential as we go forward, we should also not pretend that we are talking about something other than an engineering effort of unprecedented scale and scope ‚Äî society is aiming to build new kinds of artifacts. These artifacts should be built to work as claimed. We do not want to build systems that help us with medical treatments, transportation options and commercial opportunities to find out after the fact that these systems don‚Äôt really work ‚Äî that they make errors that take their toll in terms of human lives and happiness. In this regard, as I have emphasized, there is an engineering discipline yet to emerge for the data-focused and learning-focused fields. As exciting as these latter fields appear to be, they cannot yet be viewed as constituting an engineering discipline.\nMoreover, we should embrace the fact that what we are witnessing is the creation of a new branch of engineering. The term ‚Äúengineering‚Äù is often invoked in a narrow sense ‚Äî in academia and beyond ‚Äî with overtones of cold, affectless machinery, and negative connotations of loss of control by humans. But an engineering discipline can be what we want it to be.\nIn the current era, we have a real opportunity to conceive of something historically new ‚Äî a human-centric engineering discipline.\nI will resist giving this emerging discipline a name, but if the acronym ‚ÄúAI‚Äù continues to be used as placeholder nomenclature going forward, let‚Äôs be aware of the very real limitations of this placeholder. Let‚Äôs broaden our scope, tone down the hype and recognize the serious challenges ahead.\nMichael I. Jordan\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\nMichael I. Jordan is a Professor in the Department of Electrical Engineering and Computer Sciences and the Department of Statistics at UC Berkeley."
    },
    {
        "author": "Eran Kampf",
        "claps": 57,
        "reading_time": 3,
        "link": "https://developerzen.com/data-mining-handling-missing-values-the-database-bd2241882e72?source=tag_archive---------0----------------",
        "title": "Data Mining ‚Äî Handling Missing Values the Database ‚Äì DeveloperZen",
        "text": "I‚Äôve recently answered Predicting missing data values in a database on StackOverflow and thought it deserved a mention on DeveloperZen.\nOne of the important stages of data mining is preprocessing, where we prepare the data for mining. Real-world data tends to be incomplete, noisy, and inconsistent and an important task when preprocessing the data is to fill in missing values, smooth out noise and correct inconsistencies.\nIf we specifically look at dealing with missing data, there are several techniques that can be used. Choosing the right technique is a choice that depends on the problem domain ‚Äî the data‚Äôs domain (sales data? CRM data? ...) and our goal for the data mining process.\nSo how can you handle missing values in your database?\nThis is usually done when the class label is missing (assuming your data mining goal is classification), or many attributes are missing from the row (not just one). However, you‚Äôll obviously get poor performance if the percentage of such rows is high.\nFor example, let‚Äôs say we have a database of students enrolment data (age, SAT score, state of residence, etc.) and a column classifying their success in college to ‚ÄúLow‚Äù, ‚ÄúMedium‚Äù and ‚ÄúHigh‚Äù. Let‚Äôs say our goal is to build a model predicting a student‚Äôs success in college. Data rows who are missing the success column are not useful in predicting success so they could very well be ignored and removed before running the algorithm.\nDecide on a new global constant value, like ‚Äúunknown‚Äú, ‚ÄúN/A‚Äù or minus infinity, that will be used to fill all the missing values. This technique is used because sometimes it just doesn‚Äôt make sense to try and predict the missing value.\nFor example, let‚Äôs look at the students enrollment database again. Assuming the state of residence attribute data is missing for some students. Filling it up with some state doesn‚Äôt really makes sense as opposed to using something like ‚ÄúN/A‚Äù.\nReplace missing values of an attribute with the mean (or median if its discrete) value for that attribute in the database.\nFor example, in a database of US family incomes, if the average income of a US family is X you can use that value to replace missing income values.\nInstead of using the mean (or median) of a certain attribute calculated by looking at all the rows in a database, we can limit the calculations to the relevant class to make the value more relevant to the row we‚Äôre looking at.\nLet‚Äôs say you have a cars pricing database that, among other things, classifies cars to ‚ÄúLuxury‚Äù and ‚ÄúLow budget‚Äù and you‚Äôre dealing with missing values in the cost field. Replacing missing cost of a luxury car with the average cost of all luxury cars is probably more accurate than the value you‚Äôd get if you factor in the low budget cars.\nThe value can be determined using regression, inference based tools using Bayesian formalism, decision trees, clustering algorithms (K-Mean\\Median etc.).\nFor example, we could use clustering algorithms to create clusters of rows which will then be used for calculating an attribute mean or median as specified in technique #3. Another example could be using a decision tree to try and predict the probable value in the missing attribute, according to other attributes in the data.\nI‚Äôd suggest looking into regression and decision trees first (ID3 tree generation) as they‚Äôre relatively easy and there are plenty of examples on the net...\nAdditional Notes\nOriginally published at www.developerzen.com on August 14, 2009.\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\nMaker of things. Big data geek. Food Lover.\nThe essence of Software Development ..."
    },
    {
        "author": "Oliver Lindberg",
        "claps": 1,
        "reading_time": 7,
        "link": "https://medium.com/the-lindberg-interviews/interview-with-googles-alfred-spector-on-voice-search-hybrid-intelligence-and-more-2f6216aa480c?source=tag_archive---------0----------------",
        "title": "Interview with Google‚Äôs Alfred Spector on voice search, hybrid intelligence and more",
        "text": "Google‚Äôs a pretty good search engine, right? Well, you ain‚Äôt seen nothing yet. VP of research Alfred Spector talks to Oliver Lindberg about the technologies emerging from Google Labs ‚Äî from voice search to hybrid intelligence and beyond\nThis article originally appeared in issue 198 of .net magazine in 2010 and was republished at www.techradar.com.\nGoogle has always been tight-lipped about products that haven‚Äôt launched yet. It‚Äôs no secret, however, that thanks to the company‚Äôs bottom-up culture, its engineers are working on tons of new projects at the same time. Following the mantra of ‚Äòrelease early, release often‚Äô, the speed at which the search engine giant is churning out tools is staggering. At the heart of it all is Alfred Spector, Google‚Äôs Vice President of Research and Special Initiatives.\nOne of the areas Google is making significant advances in is voice search. Spector is astounded by how rapidly it‚Äôs come along. The Google Mobile App features ‚Äòsearch by voice‚Äô capabilities that are available for the iPhone, BlackBerry, Windows Mobile and Android. All versions understand English (including US, UK, Australian and Indian-English accents) but the latest addition, for Nokia S60 phones, even introduces Mandarin speech recognition, which ‚Äî because of its many different accents and tonal characteristics ‚Äî posed a huge engineering challenge. It‚Äôs the most spoken language in the world, but as it isn‚Äôt exactly keyboard-friendly, voice search could become immensely popular in China.\n‚ÄúVoice is one of these grand technology challenges in computer science,‚Äù Spector explains. ‚ÄúCan a computer understand the human voice? It‚Äôs been worked on for many decades and what we‚Äôve realised over the last couple of years is that search, particularly on handheld devices, is amenable to voice as an import mechanism. ‚ÄúIt‚Äôs very valuable to be able to use voice. All of us know that no matter how good the keyboard, it‚Äôs tricky to type exactly the right thing into a searchbar, while holding your backpack and everything else.‚Äù\nTo get a computer to take account of your voice is no mean feat, of course. ‚ÄúOne idea is to take all of the voices that the system hears over time into one huge pan-human voice model. So, on the one hand we have a voice that‚Äôs higher and with an English accent, and on the other hand my voice, which is deeper and with an American accent. They both go into one model, or it just becomes personalised to the individual; voice scientists are a little unclear as to which is the best approach.‚Äù\nThe research department is also making progress in machine translation. Google Translate already features 51 languages, including Swahili and Yiddish. The latest version introduces instant, real-time translation, phonetic input and text-to-speech support (in English). ‚ÄúWe‚Äôre able to go from any language to any of the others, and there are 51 times 50, so 2,550 possibilities,‚Äù Spector explains. ‚ÄúWe‚Äôre focusing on increasing the number of languages because we‚Äôd like to handle even those languages where there‚Äôs not an enormous volume of usage. It will make the web far more valuable to more people if they can access the English-or Chinese language web, for example.\n‚ÄúBut we also continue to focus on quality because almost always the translations are valuable but imperfect. Sometimes it comes from training our translation system over more raw data, so we have, say, EU documents in English and French and can compare them and learn rules for translation. The other approach is to bring more knowledge into translation. For example, we‚Äôre using more syntactic knowledge today and doing automated parsing with language. It‚Äôs been a grand challenge of the field since the late 1950s. Now it‚Äôs finally achieved mass usage.‚Äù\nThe team, led by scientist Franz Josef Och, has been collecting data for more than 100 languages, and the Google Translator Toolkit, which makes use of the ‚Äòwisdom of the crowds‚Äô, now even supports 345 languages, many of which are minority languages. The editor enables users to translate text, correct the automatic translation and publish it.\nSpector thinks that this approach is the future. As computers become even faster, handling more and more data ‚Äî a lot of it in the cloud ‚Äî machines learn from users and thus become smarter. He calls this concept ‚Äòhybrid intelligence‚Äô. ‚ÄúIt‚Äôs very difficult to solve these technological problems without human input,‚Äù he says. ‚ÄúIt‚Äôs hard to create a robot that‚Äôs as clever, smart and knowledgeable of the world as we humans are. But it‚Äôs not as tough to build a computational system like Google, which extends what we do greatly and gradually learns something about the world from us, but that requires our interpretation to make it really successful. ‚ÄúWe need to get computers and people communicating in both directions, so the computer learns from the human and makes the human more effective.‚Äù\nExamples of ‚Äòhybrid intelligence‚Äô are Google Suggest, which instantly offers popular searches as you type a search query, and the ‚Äòdid you mean?‚Äô feature in Google search, which corrects you when you misspell a query in the search bar. The more you use it, the better the system gets.\nTraining computers to become seemingly more intelligent poses major hurdles for Google‚Äôs engineers. ‚ÄúComputers don‚Äôt train as efficiently as people do,‚Äù Spector explains. ‚ÄúLet‚Äôs take the chess example. If a Kasparov was the educator, we could count on almost anything he says as being accurate. But if you tried to learn from a million chess players, you learn from my children as well, who play chess but they‚Äôre 10 and eight. They‚Äôll be right sometimes and not right other times. There‚Äôs noise in that, and some of the noise is spam. One also has to have careful regard for privacy issues.‚Äù\nBy collecting enormous amounts of data, Google hopes to create a powerful database that eventually will understand the relationship between words (for example, ‚Äòa dog is an animal‚Äô and ‚Äòa dog has four legs‚Äô). The challenge is to try to establish these relationships automatically, using tons of information, instead of having experts teach the system. This database would then improve search results and language translations because it would have a better understanding of the meaning of the words.\nThere‚Äôs also a lot of research around ‚Äòconceptual search‚Äô. ‚ÄúLet‚Äôs take a video of a couple in front of the Empire State Building. We watch the video and it‚Äôs clear they‚Äôre on their honeymoon. But what is the video about? Is it about love or honeymoons, or is it about renting office space? It‚Äôs a fundamentally challenging problem.‚Äù\nOne example of conceptual search is Google Image Swirl, which was added to Labs in November. Enter a keyword and you get a list of 12 images; clicking on each one brings up a cluster of related pictures. Click on any of them to expand the ‚Äòwonder wheel‚Äô further. Google notes that they‚Äôre not just the most relevant images; the algorithm determines the most relevant group of images with similar appearance and meaning.\nTo improve the world‚Äôs data, Google continues to focus on the importance of the open internet. Another Labs project, Google Fusion Tables facilitates data management in the cloud. It enables users to create tables, filter and aggregate data, merge it with other data sources and visualise it with Google Maps or the Google Visualisation API. The data sets can then be published, shared or kept private and commented on by people around the world. ‚ÄúIt‚Äôs an example of open collaboration,‚Äù Spector says. ‚ÄúIf it‚Äôs public, we can crawl it to make it searchable and easily visible to people. We hired one of the best database researchers in the world, Alon Halevy, to lead it.‚Äù\nGoogle is aiming to make more information available more easily across multiple devices, whether it‚Äôs images, videos, speech or maps, no matter which language we‚Äôre using. Spector calls the impact ‚Äútotally transparent processing ‚Äî it revolutionises the role of computation in day-today life. The computer can break down all these barriers to communication and knowledge. No matter what device we‚Äôre using, we have access to things. We can do translations, there are books or government documents, and some day we hope to have medical records. Whatever you want, no matter where you are, you can find it.‚Äù\nSpector retired in early 2015 and now serves as the CTO of Two Sigma Investments\nThis article originally appeared in issue 198 of .net magazine in 2010 and was republished at www.techradar.com. Photography by Andy Short\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\nIndependent editor and content consultant. Founder and captain of @pixelpioneers. Co-founder and curator of www.GenerateConf.com. Former editor of @netmag.\nInterviews with leading tech entrepreneurs and web designers, conducted by @oliverlindberg at @netmag."
    },
    {
        "author": "Xu Wenhao",
        "claps": 1,
        "reading_time": 4,
        "link": "https://xuwenhao.com/%E5%BB%BA%E8%AE%AE%E7%9A%84%E7%A8%8B%E5%BA%8F%E5%91%98%E5%AD%A6%E4%B9%A0lda%E7%AE%97%E6%B3%95%E7%9A%84%E6%AD%A5%E9%AA%A4-54168e081bc1?source=tag_archive---------0----------------",
        "title": "Âª∫ËÆÆÁöÑÁ®ãÂ∫èÂëòÂ≠¶‰π†LDAÁÆóÊ≥ïÁöÑÊ≠•È™§ ‚Äì Ëí∏Ê±Ω‰∏éÈ≠îÊ≥ï",
        "text": "Ëøô‰∏ÄÈòµ‰∏∫‰∫ÜÂ∑•‰Ωú‰∏äÁöÑÂÖ≥Á≥ª,Ëä±‰∫ÜÁÇπÊó∂Èó¥Â≠¶‰π†‰∫Ü‰∏Ä‰∏ãLDAÁÆóÊ≥ï,ËØ¥ÂÆûËØù,ÂØπ‰∫éÊàëËøô‰∏™Â≠¶CSËÄåÈùûÂ≠¶Êï∞Â≠¶ÁöÑ‰∫∫Êù•ËØ¥,Èô§‰∫ÜÈõÜ‰ΩìÊô∫ÊÖßÁºñÁ®ãËøôÊú¨‰π¶‰πãÂ§ñÂü∫Êú¨Ê≤°ÊÄé‰πàÁúãËøáÊú∫Âô®Â≠¶‰π†ÁöÑ‰∫∫Êù•ËØ¥,‰∏ÄÂºÄÂßãËøòÁúüÊòØÊë∏‰∏çÂ§™Âà∞Èó®ÈÅì,ÂâçÂâçÂêéÂêéÂø´Ë¶ÅÂõõ‰∏™Êúà‰∫Ü,ÁÆóÊòØÂü∫Êú¨‰∫ÜËß£‰∫ÜËøô‰∏™ÁÆóÊ≥ïÁöÑÂÆûÁé∞,ËÆ∞ÂΩï‰∏Ä‰∏ã,‰πü‰æõÂêéÊù•‰∫∫Âø´ÈÄüÂÖ•Èó®ÂÅö‰∏™ÂèÇËÄÉ„ÄÇ\n‰∏ÄÂºÄÂßãÁõ¥Êé•Â∞±‰∏ã‰∫ÜBleiÁöÑÂéüÂßãÁöÑÈÇ£ÁØáËÆ∫ÊñáÊù•Áúã,‰ΩÜÊòØÁúã‰∫Ü‰∏™ÂºÄÂ§¥Â∞±Ë¢´DirichletÂàÜÂ∏ÉÂíåÂá†‰∏™Êï∞Â≠¶ÂÖ¨ÂºèÊâìÂÄí,ÁÑ∂ÂêéÂõ†‰∏∫‰∏ìÂøÉÂú®ÂÜôÈ°πÁõÆ‰∏≠ÁöÑÂÖ∑‰ΩìÁöÑ‰ª£Á†Å,‰πüÂ∞±ÂÖàÊîæ‰∏ã‰∫Ü„ÄÇ‰ΩÜÊòØÂõ†‰∏∫ÂèëÁé∞ÂÆåÂÖ®ÂøòËÆ∞‰∫ÜÊú¨ÁßëÂ≠¶ÁöÑÊ¶ÇÁéáÂíåÁªüËÆ°ÁöÑÂÜÖÂÆπ,Âè™Â•ΩÂõûÂ§¥ÂéªÁúãÂ§ßÂ≠¶Êó∂ÂÄôÊ¶ÇÁéáËÆ∫ÁöÑÊïôÊùê,ÂèëÁé∞Êó©‰∏çÁü•ÈÅìÂÄüÁªôË∞Å‰∫Ü,‰∫éÊòØ‰∏äÁΩë‰π∞‰∫ÜÊú¨,Ëä±‰∫ÜÂá†Â§©Êó∂Èó¥Â§ßËá¥ÂõûÈ°æ‰∫Ü‰∏ÄÈÅçÊ¶ÇÁéáËÆ∫ÁöÑÁü•ËØÜ,‰ªÄ‰πàË¥ùÂè∂ÊñØÂÖ®Ê¶ÇÁéáÂÖ¨Âºè,Ê≠£ÊÄÅÂàÜÂ∏É,‰∫åÈ°πÂàÜÂ∏É‰πãÁ±ªÁöÑ„ÄÇÂêéÊù•Êôö‰∏äÊ≤°‰∫ãÂÑøÁöÑÊó∂ÂÄô,ÂéªÊ∞¥Êú®ÁöÑAIÁâàËΩ¨‰∫ÜËΩ¨,‰∫ÜËß£Âà∞‰∫ÜMachine LearningÁöÑÂú£ÁªèPRML,ËÄÉËôëÂà∞ÂèçÊ≠£‰πüÊòØË¶ÅÈïøÊúüÂ≠¶‰π†‰∫Ü,Êêû‰∫ÜÁîµÂ≠êÁâà,ÂêåÊó∂‰∏äÊ∑òÂÆù‰π∞‰∫Ü‰∏™ÊâìÂç∞ËÉ∂Ë£ÖÁöÑÁâàÊú¨„ÄÇÊò•ËäÇÈáåÊØèÂ§©Êôö‰∏äÁúã‰∏ÄÁÇπÂÑø,Êâ´‰∫Ü‰∏Ä‰∏ãÂâç‰∏§Á´†,ÂÜçÊ¨°ÂõûÈ°æ‰∫Ü‰∏Ä‰∏ãÂü∫Êú¨Êï∞Â≠¶Áü•ËØÜ,ÁÑ∂Âêé‰∫ÜËß£‰∫Ü‰∏ãË¥ùÂè∂ÊñØÂ≠¶Ê¥æÈÇ£ÁßçÈááÁî®ÂÖ±ËΩ≠ÂÖàÈ™åÊù•Âª∫Ê®°ÁöÑÊñπÂºè„ÄÇ‰∫éÊòØÂÜçÊ¨°Â∞ùËØïÂõûÂ§¥ÂéªÁúãBleiÁöÑÈÇ£ÁØáËÆ∫Êñá,ÂèëÁé∞ËøòÊòØÁúã‰∏çÂ§™ÊáÇ,‰∫éÊòØÂèàÊîæ‰∏ã‰∫Ü„ÄÇÁÑ∂ÂêéÊüêÂ§©TonyËÆ©ÊàëÂáÜÂ§áÂáÜÂ§áÁªôÂ§çÊó¶ÁöÑÂêåÂ≠¶‰ª¨share‰∏Ä‰∏ãÊàë‰ª¨È°πÁõÆ‰∏≠LDAÁöÑ‰ΩøÁî®,‰∏∫‰∫Ü‰∏çÈú≤ÊÄØ,ÂèàÂéªÁøªËÆ∫Êñá,Ê≠£Â•ΩÁúãÂà∞Science‰∏äËøôÁØáTopic Models Vs. Unstructured DataÁöÑÁßëÊôÆÊÄßË¥®ÁöÑÊñáÁ´†,Áøª‰∫Ü‰∏ÄÈÅç‰πãÂêé,ÂÜçÂéªPRMLÈáåÁúã‰∫Ü‰∏ÄÈÅçGraphic ModelsÈÇ£‰∏ÄÂº†,ËßâÂæóÂØπ‰∫éLDAÊÉ≥Ëß£ÂÜ≥ÁöÑÈóÆÈ¢òÂíåÊñπÊ≥ï‰∫ÜËß£‰∫ÜÊõ¥Ê∏ÖÊ•ö‰∫Ü„ÄÇ‰πãÂêé‰ªésearch engineÈáåÊêúÂà∞ËøôÁØáÊñáÁ´†,ÁÑ∂ÂêéÊ†πÊçÆÊé®ËçêËØª‰∫Ü‰∏ÄÈÉ®ÂàÜÁöÑGibbs Sampling for the Uninitiated„ÄÇ‰πãÂêéÂøò‰∫ÜÊÄé‰πàÂèàÊêúÂà∞‰∫ÜMark SteyversÂíåTom GriffithsÂêàËëóÁöÑProbabilistic Topic Models,Âú®Êüê‰∏™Âë®Êú´ÂæÄËøîÂåó‰∫¨ÁöÑÈ£ûÊú∫‰∏äËØªÂÆå‰∫Ü,ËßâÂæóÂü∫Êú¨‰∏äÊ®°ÂûãËÆ≠ÁªÉËøáÁ®ã‰πüÊòéÁôΩ‰∫Ü„ÄÇÂÜç‰πãÂêéÂ∞±ÊòØËØª‰∫Ü‰∏Ä‰∏ãËøô‰∏™ÊúÄÁÆÄÁâàÁöÑLDA Gibbs SamplingÁöÑÂÆûÁé∞,ÂÜçÂõûËøáÂ§¥ËØª‰∫Ü‰∏Ä‰∏ãPLDAÁöÑÊ∫êÁ†Å,Âü∫Êú¨‰∏äÁÆóÊòØÂØπLDAÊúâ‰∫Ü‰∏™Áõ∏ÂØπÊ∏ÖÊ•öÁöÑ‰∫ÜËß£„ÄÇ\nËøôÊ†∑ÂâçÂâçÂêéÂêé,‰πüËøáÂéª‰∫Ü‰∏â‰∏™Êúà,ÂÖ∂ÂÆû‰∏çÂ∞ëÊó∂Èó¥ÈÉΩÊòØÊµ™Ë¥πÊéâÁöÑ,ÊØîÂ¶ÇBleiÁöÑËÆ∫ÊñáÂú®Ê≤°Êúâ‰ªª‰ΩïÁõ∏ÂÖ≥Áü•ËØÜÁöÑÊÉÖÂÜµ‰∏ã‰∏ÄÂºÄÂßãËØª‰∫ÜÂ•ΩÂá†Ê¨°,ÈÉΩÊ≤°ËØªÂÆåËÄå‰∏îÂæóÂà∞Âà∞‰ø°ÊÅØ‰πüÂæàÊúâÈôê,Â¶ÇÊûúÈáçÊñ∞ÊÄªÁªì‰∏Ä‰∏ã,ÊàëËßâÂæóÂØπ‰∫éÊàë‰ª¨Ëøô‰∫õÈó®Â§ñÊ±âÁ®ãÂ∫èÂëòÊù•ËØ¥,ÊÉ≥‰∫ÜËß£LDAÂ§ßÊ¶ÇÈúÄË¶ÅËøô‰∫õÁü•ËØÜ:\nÂü∫Êú¨‰∏äËøôÊ†∑‰∏ÄÂúà‰∏ãÊù•,Âü∫Êú¨Ê¶ÇÂøµÂíåÁÆóÊ≥ïÂÆûÁé∞ÈÉΩÂ∫îËØ•ÊêûÂÆö‰∫Ü,ÂΩìÁÑ∂,Êï∞Â≠¶ËØÅÊòéÂÖ∂ÂÆûÊ≤°ÈÇ£‰πàÂÆπÊòìÂ∞±ÊêûÂÆö,‰ΩÜÊòØÂØπ‰∫éÂ∑•Á®ãÂ∏àÊù•ËØ¥,ÂÖàÊääËøô‰∫õÊêûÂÆöÂ∞±ËÉΩÂπ≤Ê¥ª‰∫Ü,Ëøô‰∏™Ê≠•È™§Âπ∂‰∏çÈÄÇÂêàÂêÑ‰ΩçËØªÂçöÂ£´ÂèëËÆ∫ÊñáÁöÑÂêåÂ≠¶‰ª¨,‰ΩÜÊòØËøôÊ†∑ÂÖàÁúãÁúã‰πüÊØîËæÉÂÆπÊòìÂØπ‰∫éËøô‰∫õÊï∞Â≠¶ÈóÆÈ¢òÁöÑÂÖ¥Ë∂£,‰∏çÁÑ∂,ÊàêÂ§©ÂØπËøôÁ¨¶Âè∑ÂíåÊï∞Â≠¶ÂÖ¨Âºè,Ê≤°ÊúâÊï¥Âùó‰∏ö‰ΩôÊó∂Èó¥ÁöÑÊàëÊòØËßâÂæóËøòÊòØÂÆπÊòìÈÄÄÁº©ÊîæÂºÉÁöÑ„ÄÇ\nÂèëÁé∞‰Ωú‰∏∫Â∑•Á®ãÂ∏àÊù•ËØ¥,ËøòÊòØÁúã‰ª£Á†ÅÊØîËæÉÊúâÊÑüËßâ,ÁúãÂÆûÈôÖÂ∫îÁî®ÁöÑÂÆû‰æãÊØîËæÉÊúâÊÑüËßâ,ÁúãÊù•‰∏çËÉΩÊääÂ§ßÈÉ®ÂàÜÊó∂Èó¥Ëä±Âú®PRML‰∏ä,ËøòÊòØË¶ÅÂ§öÂØπÁÖßÁùÄ‰ª£Á†ÅÁúã„ÄÇ\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\nFacebook Messenger & Chatbot, Machine Learning & Big Data\nÁîüÂëΩÂ¶ÇÊ≠§Áü≠ÊöÇ,ÊéåÊè°ÊäÄËâ∫Âç¥Ë¶ÅÂ¶ÇÊ≠§Èïø‰πÖ"
    },
    {
        "author": "Netflix Technology Blog",
        "claps": 439,
        "reading_time": 9,
        "link": "https://medium.com/netflix-techblog/netflix-recommendations-beyond-the-5-stars-part-1-55838468f429?source=tag_archive---------0----------------",
        "title": "Netflix Recommendations: Beyond the 5 stars (Part 1)",
        "text": "by Xavier Amatriain and Justin Basilico (Personalization Science and Engineering)\nIn this two-part blog post, we will open the doors of one of the most valued Netflix assets: our recommendation system. In Part 1, we will relate the Netflix Prize to the broader recommendation challenge, outline the external components of our personalized service, and highlight how our task has evolved with the business. In Part 2, we will describe some of the data and models that we use and discuss our approach to algorithmic innovation that combines offline machine learning experimentation with online AB testing. Enjoy... and remember that we are always looking for more star talent to add to our great team, so please take a look at our jobs page.\nIn 2006 we announced the Netflix Prize, a machine learning and data mining competition for movie rating prediction. We offered $1 million to whoever improved the accuracy of our existing system called Cinematch by 10%. We conducted this competition to find new ways to improve the recommendations we provide to our members, which is a key part of our business. However, we had to come up with a proxy question that was easier to evaluate and quantify: the root mean squared error (RMSE) of the predicted rating. The race was on to beat our RMSE of 0.9525 with the finish line of reducing it to 0.8572 or less.\nA year into the competition, the Korbell team won the first Progress Prize with an 8.43% improvement. They reported more than 2000 hours of work in order to come up with the final combination of 107 algorithms that gave them this prize. And, they gave us the source code. We looked at the two underlying algorithms with the best performance in the ensemble: Matrix Factorization (which the community generally called SVD, Singular Value Decomposition) and Restricted Boltzmann Machines (RBM). SVD by itself provided a 0.8914 RMSE, while RBM alone provided a competitive but slightly worse 0.8990 RMSE. A linear blend of these two reduced the error to 0.88. To put these algorithms to use, we had to work to overcome some limitations, for instance that they were built to handle 100 million ratings, instead of the more than 5 billion that we have, and that they were not built to adapt as members added more ratings. But once we overcame those challenges, we put the two algorithms into production, where they are still used as part of our recommendation engine.\nIf you followed the Prize competition, you might be wondering what happened with the final Grand Prize ensemble that won the $1M two years later. This is a truly impressive compilation and culmination of years of work, blending hundreds of predictive models to finally cross the finish line. We evaluated some of the new methods offline but the additional accuracy gains that we measured did not seem to justify the engineering effort needed to bring them into a production environment. Also, our focus on improving Netflix personalization had shifted to the next level by then. In the remainder of this post we will explain how and why it has shifted.\nOne of the reasons our focus in the recommendation algorithms has changed is because Netflix as a whole has changed dramatically in the last few years. Netflix launched an instant streaming service in 2007, one year after the Netflix Prize began. Streaming has not only changed the way our members interact with the service, but also the type of data available to use in our algorithms. For DVDs our goal is to help people fill their queue with titles to receive in the mail over the coming days and weeks; selection is distant in time from viewing, people select carefully because exchanging a DVD for another takes more than a day, and we get no feedback during viewing. For streaming members are looking for something great to watch right now; they can sample a few videos before settling on one, they can consume several in one session, and we can observe viewing statistics such as whether a video was watched fully or only partially.\nAnother big change was the move from a single website into hundreds of devices. The integration with the Roku player and the Xbox were announced in 2008, two years into the Netflix competition. Just a year later, Netflix streaming made it into the iPhone. Now it is available on a multitude of devices that go from a myriad of Android devices to the latest AppleTV.\nTwo years ago, we went international with the launch in Canada. In 2011, we added 43 Latin-American countries and territories to the list. And just recently, we launched in UK and Ireland. Today, Netflix has more than 23 million subscribers in 47 countries. Those subscribers streamed 2 billion hours from hundreds of different devices in the last quarter of 2011. Every day they add 2 million movies and TV shows to the queue and generate 4 million ratings.\nWe have adapted our personalization algorithms to this new scenario in such a way that now 75% of what people watch is from some sort of recommendation. We reached this point by continuously optimizing the member experience and have measured significant gains in member satisfaction whenever we improved the personalization for our members. Let us now walk you through some of the techniques and approaches that we use to produce these recommendations.\nWe have discovered through the years that there is tremendous value to our subscribers in incorporating recommendations to personalize as much of Netflix as possible. Personalization starts on our homepage, which consists of groups of videos arranged in horizontal rows. Each row has a title that conveys the intended meaningful connection between the videos in that group. Most of our personalization is based on the way we select rows, how we determine what items to include in them, and in what order to place those items.\nTake as a first example the Top 10 row: this is our best guess at the ten titles you are most likely to enjoy. Of course, when we say ‚Äúyou‚Äù, we really mean everyone in your household. It is important to keep in mind that Netflix‚Äô personalization is intended to handle a household that is likely to have different people with different tastes. That is why when you see your Top10, you are likely to discover items for dad, mom, the kids, or the whole family. Even for a single person household we want to appeal to your range of interests and moods. To achieve this, in many parts of our system we are not only optimizing for accuracy, but also for diversity.\nAnother important element in Netflix‚Äô personalization is awareness. We want members to be aware of how we are adapting to their tastes. This not only promotes trust in the system, but encourages members to give feedback that will result in better recommendations. A different way of promoting trust with the personalization component is to provide explanations as to why we decide to recommend a given movie or show. We are not recommending it because it suits our business needs, but because it matches the information we have from you: your explicit taste preferences and ratings, your viewing history, or even your friends‚Äô recommendations.\nOn the topic of friends, we recently released our Facebook connect feature in 46 out of the 47 countries we operate ‚Äî all but the US because of concerns with the VPPA law. Knowing about your friends not only gives us another signal to use in our personalization algorithms, but it also allows for different rows that rely mostly on your social circle to generate recommendations.\nSome of the most recognizable personalization in our service is the collection of ‚Äúgenre‚Äù rows. These range from familiar high-level categories like ‚ÄúComedies‚Äù and ‚ÄúDramas‚Äù to highly tailored slices such as ‚ÄúImaginative Time Travel Movies from the 1980s‚Äù. Each row represents 3 layers of personalization: the choice of genre itself, the subset of titles selected within that genre, and the ranking of those titles. Members connect with these rows so well that we measure an increase in member retention by placing the most tailored rows higher on the page instead of lower. As with other personalization elements, freshness and diversity is taken into account when deciding what genres to show from the thousands possible.\nWe present an explanation for the choice of rows using a member‚Äôs implicit genre preferences ‚Äî recent plays, ratings, and other interactions ‚Äî , or explicit feedback provided through our taste preferences survey. We will also invite members to focus a row with additional explicit preference feedback when this is lacking.\nSimilarity is also an important source of personalization in our service. We think of similarity in a very broad sense; it can be between movies or between members, and can be in multiple dimensions such as metadata, ratings, or viewing data. Furthermore, these similarities can be blended and used as features in other models. Similarity is used in multiple contexts, for example in response to a member‚Äôs action such as searching or adding a title to the queue. It is also used to generate rows of ‚Äúadhoc genres‚Äù based on similarity to titles that a member has interacted with recently. If you are interested in a more in-depth description of the architecture of the similarity system, you can read about it in this past post on the blog.\nIn most of the previous contexts ‚Äî be it in the Top10 row, the genres, or the similars ‚Äî ranking, the choice of what order to place the items in a row, is critical in providing an effective personalized experience. The goal of our ranking system is to find the best possible ordering of a set of items for a member, within a specific context, in real-time. We decompose ranking into scoring, sorting, and filtering sets of movies for presentation to a member. Our business objective is to maximize member satisfaction and month-to-month subscription retention, which correlates well with maximizing consumption of video content. We therefore optimize our algorithms to give the highest scores to titles that a member is most likely to play and enjoy.\nNow it is clear that the Netflix Prize objective, accurate prediction of a movie‚Äôs rating, is just one of the many components of an effective recommendation system that optimizes our members enjoyment. We also need to take into account factors such as context, title popularity, interest, evidence, novelty, diversity, and freshness. Supporting all the different contexts in which we want to make recommendations requires a range of algorithms that are tuned to the needs of those contexts. In the next part of this post, we will talk in more detail about the ranking problem. We will also dive into the data and models that make all the above possible and discuss our approach to innovating in this space.\nOn to part 2:\nOriginally published at techblog.netflix.com on April 6, 2012.\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\nLearn more about how Netflix designs, builds, and operates our systems and engineering organizations\nLearn about Netflix‚Äôs world class engineering efforts, company culture, product developments and more."
    },
    {
        "author": "Netflix Technology Blog",
        "claps": 365,
        "reading_time": 10,
        "link": "https://medium.com/netflix-techblog/netflix-recommendations-beyond-the-5-stars-part-2-d9b96aa399f5?source=tag_archive---------1----------------",
        "title": "Netflix Recommendations: Beyond the 5 stars (Part 2)",
        "text": "by Xavier Amatriain and Justin Basilico (Personalization Science and Engineering)\nIn part one of this blog post, we detailed the different components of Netflix personalization. We also explained how Netflix personalization, and the service as a whole, have changed from the time we announced the Netflix Prize.\nThe $1M Prize delivered a great return on investment for us, not only in algorithmic innovation, but also in brand awareness and attracting stars (no pun intended) to join our team. Predicting movie ratings accurately is just one aspect of our world-class recommender system. In this second part of the blog post, we will give more insight into our broader personalization technology. We will discuss some of our current models, data, and the approaches we follow to lead innovation and research in this space.\nThe goal of recommender systems is to present a number of attractive items for a person to choose from. This is usually accomplished by selecting some items and sorting them in the order of expected enjoyment (or utility). Since the most common way of presenting recommended items is in some form of list, such as the various rows on Netflix, we need an appropriate ranking model that can use a wide variety of information to come up with an optimal ranking of the items for each of our members.\nIf you are looking for a ranking function that optimizes consumption, an obvious baseline is item popularity. The reason is clear: on average, a member is most likely to watch what most others are watching. However, popularity is the opposite of personalization: it will produce the same ordering of items for every member. Thus, the goal becomes to find a personalized ranking function that is better than item popularity, so we can better satisfy members with varying tastes.\nRecall that our goal is to recommend the titles that each member is most likely to play and enjoy. One obvious way to approach this is to use the member‚Äôs predicted rating of each item as an adjunct to item popularity. Using predicted ratings on their own as a ranking function can lead to items that are too niche or unfamiliar being recommended, and can exclude items that the member would want to watch even though they may not rate them highly. To compensate for this, rather than using either popularity or predicted rating on their own, we would like to produce rankings that balance both of these aspects. At this point, we are ready to build a ranking prediction model using these two features.\nThere are many ways one could construct a ranking function ranging from simple scoring methods, to pairwise preferences, to optimization over the entire ranking. For the purposes of illustration, let us start with a very simple scoring approach by choosing our ranking function to be a linear combination of popularity and predicted rating. This gives an equation of the form frank(u,v) = w1 p(v) + w2 r(u,v) + b, where u=user, v=video item, p=popularity and r=predicted rating. This equation defines a two-dimensional space like the one depicted below.\nOnce we have such a function, we can pass a set of videos through our function and sort them in descending order according to the score. You might be wondering how we can set the weights w1 and w2 in our model (the bias b is constant and thus ends up not affecting the final ordering). In other words, in our simple two-dimensional model, how do we determine whether popularity is more or less important than predicted rating? There are at least two possible approaches to this. You could sample the space of possible weights and let the members decide what makes sense after many A/B tests. This procedure might be time consuming and not very cost effective. Another possible answer involves formulating this as a machine learning problem: select positive and negative examples from your historical data and let a machine learning algorithm learn the weights that optimize your goal. This family of machine learning problems is known as ‚ÄúLearning to rank‚Äù and is central to application scenarios such as search engines or ad targeting. Note though that a crucial difference in the case of ranked recommendations is the importance of personalization: we do not expect a global notion of relevance, but rather look for ways of optimizing a personalized model.\nAs you might guess, apart from popularity and rating prediction, we have tried many other features at Netflix. Some have shown no positive effect while others have improved our ranking accuracy tremendously. The graph below shows the ranking improvement we have obtained by adding different features and optimizing the machine learning algorithm.\nMany supervised classification methods can be used for ranking. Typical choices include Logistic Regression, Support Vector Machines, Neural Networks, or Decision Tree-based methods such as Gradient Boosted Decision Trees (GBDT). On the other hand, a great number of algorithms specifically designed for learning to rank have appeared in recent years such as RankSVM or RankBoost. There is no easy answer to choose which model will perform best in a given ranking problem. The simpler your feature space is, the simpler your model can be. But it is easy to get trapped in a situation where a new feature does not show value because the model cannot learn it. Or, the other way around, to conclude that a more powerful model is not useful simply because you don‚Äôt have the feature space that exploits its benefits.\nThe previous discussion on the ranking algorithms highlights the importance of both data and models in creating an optimal personalized experience for our members. At Netflix, we are fortunate to have many relevant data sources and smart people who can select optimal algorithms to turn data into product features. Here are some of the data sources we can use to optimize our recommendations:\nSo, what about the models? One thing we have found at Netflix is that with the great availability of data, both in quantity and types, a thoughtful approach is required to model selection, training, and testing. We use all sorts of machine learning approaches: From unsupervised methods such as clustering algorithms to a number of supervised classifiers that have shown optimal results in various contexts. This is an incomplete list of methods you should probably know about if you are working in machine learning for personalization:\nConsumer Data Science\nThe abundance of source data, measurements and associated experiments allow us to operate a data-driven organization. Netflix has embedded this approach into its culture since the company was founded, and we have come to call it Consumer (Data) Science. Broadly speaking, the main goal of our Consumer Science approach is to innovate for members effectively. The only real failure is the failure to innovate; or as Thomas Watson Sr, founder of IBM, put it: ‚ÄúIf you want to increase your success rate, double your failure rate.‚Äù We strive for an innovation culture that allows us to evaluate ideas rapidly, inexpensively, and objectively. And, once we test something we want to understand why it failed or succeeded. This lets us focus on the central goal of improving our service for our members.\nSo, how does this work in practice? It is a slight variation over the traditional scientific process called A/B testing (or bucket testing):\nWhen we execute A/B tests, we track many different metrics. But we ultimately trust member engagement (e.g. hours of play) and retention. Tests usually have thousands of members and anywhere from 2 to 20 cells exploring variations of a base idea. We typically have scores of A/B tests running in parallel. A/B tests let us try radical ideas or test many approaches at the same time, but the key advantage is that they allow our decisions to be data-driven. You can read more about our approach to A/B Testing in this previous tech blog post or in some of the Quora answers by our Chief Product Officer Neil Hunt.\nAn interesting follow-up question that we have faced is how to integrate our machine learning approaches into this data-driven A/B test culture at Netflix. We have done this with an offline-online testing process that tries to combine the best of both worlds. The offline testing cycle is a step where we test and optimize our algorithms prior to performing online A/B testing. To measure model performance offline we track multiple metrics used in the machine learning community: from ranking measures such as normalized discounted cumulative gain, mean reciprocal rank, or fraction of concordant pairs, to classification metrics such as accuracy, precision, recall, or F-score. We also use the famous RMSE from the Netflix Prize or other more exotic metrics to track different aspects like diversity. We keep track of how well those metrics correlate to measurable online gains in our A/B tests. However, since the mapping is not perfect, offline performance is used only as an indication to make informed decisions on follow up tests.\nOnce offline testing has validated a hypothesis, we are ready to design and launch the A/B test that will prove the new feature valid from a member perspective. If it does, we will be ready to roll out in our continuous pursuit of the better product for our members. The diagram below illustrates the details of this process.\nAn extreme example of this innovation cycle is what we called the Top10 Marathon. This was a focused, 10-week effort to quickly test dozens of algorithmic ideas related to improving our Top10 row. Think of it as a 2-month hackathon with metrics. Different teams and individuals were invited to contribute ideas and code in this effort. We rolled out 6 different ideas as A/B tests each week and kept track of the offline and online metrics. The winning results are already part of our production system.\nThe Netflix Prize abstracted the recommendation problem to a proxy question of predicting ratings. But member ratings are only one of the many data sources we have and rating predictions are only part of our solution. Over time we have reformulated the recommendation problem to the question of optimizing the probability a member chooses to watch a title and enjoys it enough to come back to the service. More data availability enables better results. But in order to get those results, we need to have optimized approaches, appropriate metrics and rapid experimentation.\nTo excel at innovating personalization, it is insufficient to be methodical in our research; the space to explore is virtually infinite. At Netflix, we love choosing and watching movies and TV shows. We focus our research by translating this passion into strong intuitions about fruitful directions to pursue; under-utilized data sources, better feature representations, more appropriate models and metrics, and missed opportunities to personalize. We use data mining and other experimental approaches to incrementally inform our intuition, and so prioritize investment of effort. As with any scientific pursuit, there‚Äôs always a contribution from Lady Luck, but as the adage goes, luck favors the prepared mind. Finally, above all, we look to our members as the final judges of the quality of our recommendation approach, because this is all ultimately about increasing our members‚Äô enjoyment in their own Netflix experience. We are always looking for more people to join our team of ‚Äúprepared minds‚Äù. Make sure you take a look at our jobs page.\nOriginally published at techblog.netflix.com on June 20, 2012.\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\nLearn more about how Netflix designs, builds, and operates our systems and engineering organizations\nLearn about Netflix‚Äôs world class engineering efforts, company culture, product developments and more."
    },
    {
        "author": "Wolf Garbe",
        "claps": 6,
        "reading_time": 6,
        "link": "https://medium.com/@wolfgarbe/1000x-faster-spelling-correction-algorithm-2012-8701fcd87a5f?source=tag_archive---------2----------------",
        "title": "1000x Faster Spelling Correction algorithm (2012) ‚Äì Wolf Garbe ‚Äì Medium",
        "text": "Update1: An improved SymSpell implementation is now 1,000,000x faster.Update2: SymSpellCompound with Compound aware spelling correction. Update3: Benchmark of SymSpell, BK-Tree und Norvig‚Äôs spell-correct.\nRecently I answered a question on Quora about spelling correction for search engines. When I described our SymSpell algorithm I was pointed to Peter Norvig‚Äôs page where he outlined his approach.\nBoth algorithms are based on Edit distance (Damerau-Levenshtein distance). Both try to find the dictionary entries with smallest edit distance from the query term.\nIf the edit distance is 0 the term is spelled correctly, if the edit distance is <=2 the dictionary term is used as spelling suggestion. But SymSpell uses a different way to search the dictionary, resulting in a significant performance gain and language independence. Three ways to search for minimum edit distance in a dictionary:\n1. Naive approachThe obvious way of doing this is to compute the edit distance from the query term to each dictionary term, before selecting the string(s) of minimum edit distance as spelling suggestion. This exhaustive search is inordinately expensive.\nSource: Christopher D. Manning, Prabhakar Raghavan & Hinrich SchuÃàtze: Introduction to Information Retrieval.\nThe performance can be significantly improved by terminating the edit distance calculation as soon as a threshold of 2 or 3 has been reached.\n2. Peter NorvigGenerate all possible terms with an edit distance (deletes + transposes + replaces + inserts) from the query term and search them in the dictionary. For a word of length n, an alphabet size a, an edit distance d=1, there will be n deletions, n-1 transpositions, a*n alterations, and a*(n+1) insertions, for a total of 2n+2an+a-1 terms at search time.\nSource: Peter Norvig: How to Write a Spelling Corrector.\nThis is much better than the naive approach, but still expensive at search time (114,324 terms for n=9, a=36, d=2) and language dependent (because the alphabet is used to generate the terms, which is different in many languages and huge in Chinese: a=70,000 Unicode Han characters)\n3. Symmetric Delete Spelling Correction (SymSpell) Generate terms with an edit distance (deletes only) from each dictionary term and add them together with the original term to the dictionary. This has to be done only once during a pre-calculation step. Generate terms with an edit distance (deletes only) from the input term and search them in the dictionary. For a word of length n, an alphabet size of a, an edit distance of 1, there will be just n deletions, for a total of n terms at search time.\nThis is three orders of magnitude less expensive (36 terms for n=9 and d=2) and language independent (the alphabet is not required to generate deletes). The cost of this approach is the pre-calculation time and storage space of x deletes for every original dictionary entry, which is acceptable in most cases.\nThe number x of deletes for a single dictionary entry depends on the maximum edit distance: x=n for edit distance=1, x=n*(n-1)/2 for edit distance=2, x=n!/d!/(n-d)! for edit distance=d (combinatorics: k out of n combinations without repetitions, and k=n-d), E.g. for a maximum edit distance of 2 and an average word length of 5 and 100,000 dictionary entries we need to additionally store 1,500,000 deletes.\nRemark 1: During the precalculation, different words in the dictionary might lead to same delete term: delete(sun,1)==delete(sin,1)==sn. While we generate only one new dictionary entry (sn), inside we need to store both original terms as spelling correction suggestion (sun,sin)\nRemark 2: There are four different comparison pair types:\nThe last comparison type is required for replaces and transposes only. But we need to check whether the suggested dictionary term is really a replace or an adjacent transpose of the input term to prevent false positives of higher edit distance (bank==bnak and bank==bink, but bank!=kanb and bank!=xban and bank!=baxn).\nRemark 3: Instead of a dedicated spelling dictionary we are using the search engine index itself. This has several benefits:\nRemark 4: We have implemented query suggestions/completion in a similar fashion. This is a good way to prevent spelling errors in the first place. Every newly indexed word, whose frequency is over a certain threshold, is stored as a suggestion to all of its prefixes (they are created in the index if they do not yet exist). As we anyway provide an instant search feature the lookup for suggestions comes also at almost no extra cost. Multiple terms are sorted by the number of results stored in the index.\nReasoningThe SymSpell algorithm exploits the fact that the edit distance between two terms is symmetrical:\nWe are using variant 3, because the delete-only-transformation is language independent and three orders of magnitude less expensive.\nWhere does the speed come from?\nComputational Complexity The SymSpell algorithm is constant time ( O(1) time ), i.e. independent of the dictionary size (but depending on the average term length and maximum edit distance), because our index is based on a Hash Table which has an average search time complexity of O(1).\nComparison to other approaches BK-Trees have a search time of O(log dictionary_size), whereas the SymSpell algorithm is constant time ( O(1) time ), i.e. independent of the dictionary size. Tries have a comparable search performance to our approach. But a Trie is a prefix tree, which requires a common prefix. This makes it suitable for autocomplete or search suggestions, but not applicable for spell checking. If your typing error is e.g. in the first letter, than you have no common prefix, hence the Trie will not work for spelling correction.\nApplication Possible application fields of the SymSpell algorithm are those of fast approximate dictionary string matching: spell checkers for word processors and search engines, correction systems for optical character recognition, natural language translation based on translation memory, record linkage, de-duplication, matching DNA sequences, fuzzy string searching and fraud detection.\nSource codeThe C# implementation of the Symmetric Delete Spelling Correction algorithm is released on GitHub as Open Source under the MIT License:https://github.com/wolfgarbe/symspell\nPortsThere are ports in C++, Crystal, Go, Java, Javascript, Python, Ruby, Rust, Scala, Swift available.\nOriginally published at blog.faroo.com on June 7, 2012.\nFrom a quick cheer to a standing ovation, clap to show how much you enjoyed this story.\nFounder SeekStorm (Search-as-a-Service), FAROO (P2P Search) http://www.seekstorm.com https://github.com/wolfgarbe https://www.quora.com/profile/Wolf-Garbe"
    },
    {
        "author": "Paul Christiano",
        "claps": 43,
        "reading_time": 31,
        "link": "https://ai-alignment.com/a-formalization-of-indirect-normativity-7e44db640160?source=tag_archive---------3----------------",
        "title": "Formalizing indirect normativity ‚Äì AI Alignment",
        "text": "This post outlines a formalization of what Nick Bostrom calls ‚Äúindirect normativity.‚Äù I don‚Äôt think it‚Äôs an adequate solution to the AI control problem; but to my knowledge it was the first precise specification of a goal that meets the ‚Äúnot terrible‚Äù bar, i.e. which does not lead to terrible consequences if pursued without any caveats or restrictions. The proposal outlined here was sketched in early 2012 while I was visiting FHI, and was my first serious foray into AI control.\nWhen faced with the challenge of writing down precise moral principles, adhering to the standards demanded in mathematics, moral philosophers encounter two serious difficulties:\nIn light of these difficulties, a moral philosopher might simply declare: ‚ÄúIt is not my place to aspire to mathematical standards of precision. Ethics as a project inherently requires shared language, understanding, and experience; it becomes impossible or meaningless without them.‚Äù\nThis may be a defensible philosophical position, but unfortunately the issue is not entirely philosophical. In the interest of building institutions or machines which reliably pursue what we value, we may one day be forced to describe precisely ‚Äúwhat we value‚Äù in a way that does not depend on charitable or ‚Äúcommon sense‚Äù interpretation (in the same way that we today must describe ‚Äúwhat we want done‚Äù precisely to computers, often with considerable effort). If some aspects of our values cannot be described formally, then it may be more difficult to use institutions or machines to reliably satisfy them. This is not to say that describing our values formally is necessary to satisfying them, merely that it might make it easier.\nSince we are focusing on finding any precise and satisfactory moral theory, rather than resolving disputes in moral philosophy, we will adopt a consequentialist approach without justification and focus on axiology. Moreover, we will begin from the standpoint of expected utility maximization, and leave aside questions about how or over what space the maximization is performed.\nWe aim to mathematically define a utility function U such that we would be willing to build a hypothetical machine which exceptionlessly maximized U, possibly at the catastrophic expense of any other values. We will assume that the machine has an ability to reason which at least rivals that of humans, and is willing to tolerate arbitrarily complex definitions of U (within its ability to reason about them).\nWe adopt an indirect approach. Rather than specifying what exactly we want, we specify a process for determining what we want. This process is extremely complex, so that any computationally limited agent will always be uncertain about the process‚Äô output. However, by reasoning about the process it is possible to make judgments about which action has the highest expected utility in light of this uncertainty.\nFor example, I might adopt the principle: ‚Äúa state of affairs is valuable to the extent that I would judge it valuable after a century of reflection.‚Äù In general I will be uncertain about what I would say after a century, but I can act on the basis of my best guesses: after a century I will probably prefer worlds with more happiness, and so today I should prefer worlds with more happiness. After a century I have only a small probability of valuing trees‚Äô feelings, and so today I should go out of my way to avoid hurting them if it is either instrumentally useful or extremely easy. As I spend more time thinking, my beliefs about what I would say after a century may change, and I will start to pursue different states of affairs even though the formal definition of my values is static. Similarly, I might desire to think about the value of trees‚Äô feelings, if I expect that my opinions are unstable: if I spend a month thinking about trees, my current views will then be a much better predictor of my views after a hundred years, and if I know better whether or not trees‚Äô feelings are valuable, I can make better decisions.\nThis example is quite informal, but it communicates the main idea of the approach. We stress that the value of our contribution, if any, is in the possibility of a precise formulation. (Our proposal itself will be relatively informal; instead it is a description of how you would arrive at a precise formulation.) The use of indirection seems to be necessary to achieve the desired level of precision.\nOur proposal contains only two explicit steps:\nEach of these steps requires substantial elaboration, but we must also specify what we expect the human to do with these tools.\nThis proposal is best understood in the context of other fantastic-seeming proposals, such as ‚Äúmy utility is whatever I would write down if I reflected for a thousand years without interruption or biological decay.‚Äù The counterfactual events which take place within the definition are far beyond the realm our intuition recognizes as ‚Äúrealistic,‚Äù and have no place except in thought experiments. But to the extent that we can reason about these counterfactuals and change our behavior on the basis of that reasoning (if so motivated), we can already see how such fantastic situations could affect our more prosaic reality.\nThe remainder of this document consists of brief elaboration of some of these steps, and a few arguments about why this is a desirable process.\nThe first step of our proposal is a high-fidelity mathematical model of human cognition. We will set aside philosophical troubles, and assume that the human brain is a purely physical system which may be characterized mathematically. Even granting this, it is not clear how we can realistically obtain such a characterization.\nThe most obvious approach to characterizing a brain is to combine measurements of its behavior or architecture with an understanding of biology, chemistry, and physics. This project represents a massive engineering effort which is currently just beginning. Most pessimistically, our proposal could be postponed until this project‚Äôs completion. This could still be long before the mathematical characterization of the brain becomes useful for running experiments or automating human activities: because we are interested only in a definition, we do not care about having the computational resources necessary to simulate the brain.\nAn impractical mathematical definition, however, may be much easier to obtain. We can define a model of a brain in terms of exhaustive searches which could never be practically carried out. For example, given some observations of a neuron, we can formally define a brute force search for a model of that neuron. Similarly, given models of individual neurons we may be able to specify a brute force search over all ways of connecting those neurons which account for our observations of the brain (say, some data acquired through functional neuroimaging).\nIt may be possible to carry out this definition without exploiting any structural knowledge about the brain, beyond what is necessary to measure it effectively. By collecting imaging data for a human exposed to a wide variety of stimuli, we can recover a large corpus of data which must be explained by any model of a human brain. Moreover, by using our explicit knowledge of human cognition we can algorithmically generate an extensive range of tests which identify a successful simulation, by probing responses to questions or performance on games or puzzles.\nIn fact, this project may be possible using existing resources. The complexity of the human brain is not as unapproachable as it may at first appear: though it may contain 1014synapses, each described by many parameters, it can be specified much more compactly. A newborn‚Äôs brain can be specified by about 109bits of genetic information, together with a recipe for a physical simulation of development. The human brain appears to form new long-term memories at a rate of 1‚Äì2 bits per second, suggesting that it may be possible to specify an adult brain using 109additional bits of experiential information. This suggests that it may require only about 1010bits of information to specify a human brain, which is at the limits of what can be reasonably collected by existing technology for functional neuroimaging.\nThis discussion has glossed over at least one question: what do we mean by ‚Äòbrain emulation‚Äô? Human cognition does not reside in a physical system with sharp boundaries, and it is not clear how you would define or use a simulation of the ‚Äúinput-output‚Äù behavior of such an object.\nWe will focus on some system which does have precisely defined input-output behavior, and which captures the important aspects of human cognition. Consider a system containing a human, a keyboard, a monitor, and some auxiliary instruments, well-insulated from the environment except for some wires carrying inputs to the monitor and outputs from the keyboard and auxiliary instruments (and wires carrying power). The inputs to this system are simply screens to be displayed on the monitor (say delivered as a sequence to be displayed one after another at 30 frames per second), while the outputs are the information conveyed from the keyboard and the other measuring apparatuses (also delivered as a sequence of data dumps, each recording activity from the last 30th of a second).\nThis ‚Äúhuman in a box‚Äù system can be easily formally defined if a precise description of a human brain and coarse descriptions of the human body and the environment are available. Alternatively, the input-output behavior of the human in a box can be directly observed, and a computational model constructed for the entire system. Let H be a mathematical definition of the resulting (randomized) function from input sequences (In(1), In(2), ..., In(K)) to the next output Out(K). H is, by design, a good approximation to what the human ‚Äúwould output‚Äù if presented with any particular input sequence.\nUsing H, we can mathematically define what ‚Äúwould happen‚Äù if the human interacted with a wide variety of systems. For example, if we deliver Out(K) as the input to an abstract computer running some arbitrary software, and then define In(K+1) as what the screen would next display, we can mathematically define the distribution over transcripts which would have arisen if the human had interacted with the abstract computer. This computer could be running an interactive shell, a video game, or a messaging client.\nNote that H reflects the behavior of a particular human, in a particular mental state. This state is determined by the process used to design H, or the data used to learn it. In general, we can control H by choosing an appropriate human and providing appropriate instructions / training. More emulations could be produced by similar measures if necessary. Using only a single human may seem problematic, but we will not rely on this lone individual to make all relevant ethical judgments. Instead, we will try to select a human with the motivational stability to carry out the subsequent steps faithfully, which will define U using the judgment of a community consisting of many humans.\nThis discussion has been brief and has necessarily glossed over several important difficulties. One difficulty is the danger of using computationally unbounded brute force search, given the possibility of short programs which exhibit goal-oriented behavior. Another difficulty is that, unless the emulation project is extremely conservative, the models it produces are not likely to be fully-functional humans. Their thoughts may be blurred in various ways, they may be missing many memories or skills, and they may lack important functionalities such as long-term memory formation or emotional expression. The scope of these issues depends on the availability of data from which to learn the relevant aspects of human cognition. Realistic proposals along these lines will need to accommodate these shortcomings, relying on distorted emulations as a tool to construct increasingly accurate models.\nFor any idealized ‚Äúsoftware‚Äù, with a distinguished instruction return, we can use H to mathematically define the distribution over return values which would result, if the human were to interact with that software. We will informally define a particular program T which provides a rich environment, in which the remainder of our proposal can be implemented. From a technical perspective this will be the last step of our proposal. The remaining steps will be reflected only in the intentions and behavior of the human being simulated in H.\nFix a convenient and adequately expressive language (say a dialect of Python designed to run on an abstract machine). T implements a standard interface for an interactive shell in this language: the user can look through all of the past instructions that have been executed and their return values (rendered as strings) or execute a new instruction. We also provide symbols representing H and T themselves (as functions from sequences of K inputs to a value for the Kth output). We also provide some useful information (such as a snapshot of the Internet, and some information about the process used to create H and T), which we encode as a bit string and store in a single environment variable data. We assume that our language of choice has a return instruction, and we have T return whenever the user executes this instruction. Some care needs to be taken to define the behavior if T enters an infinite loop‚Äìwe want to minimize the probability that the human accidentally hangs the terminal, with catastrophic consequences, but we cannot provide a complete safety-net without running into unresolvable issues with self-reference.\nWe define U to be the value returned by H interacting with T. If H represented an unfortunate mental state, then this interaction could be short and unproductive: the simulated human could just decide to type ‚Äòreturn 0‚Äô and be done with it. However, by choosing an appropriate human to simulate and inculcating an appropriate mental state, we can direct the process further.\nWe intend for H to use the resources in T to initiate a larger deliberative process. For example, the first step of this process may be to instantiate many copies of H, interacting with variants of messaging clients which are in contact with each other. The return value from the original process could then be defined as the value returned by a designated ‚Äòleader‚Äô from this community, or as a majority vote amongst the copies of H, or so on. Another step might be to create appropriate realistic virtual environments for simulated brains, rather than confining them to boxes. For motivational stability, it may be helpful to design various coordination mechanisms, involving frameworks for interaction, ‚Äúcached‚Äù mental states which are frequently re-instantiated, or sanity checks whereby one copy of H monitors the behavior of another.\nThe resulting communities of simulated brains then engage in a protracted planning process, ensuring that subsequent steps can be carried out safely or developing alternative approaches. The main priority of this community is to reduce the probability of errors as far as possible (exactly what constitutes an ‚Äòerror‚Äô will be discussed at more length later). At the end of this process, we obtain a formal definition of a new protocol H+, which submits its inputs for consideration to a large community and then produces its outputs using some deliberation mechanism (democratic vote, one leader using the rest of the community as advisors, etc.)\nThe next step requires our community of simulated brains to construct a detailed simulation of Earth which they can observe and manipulate. Once they have such a simulation, they have access to all of the data which would have been available on Earth. In particular, they can now explore many possible futures and construct simulations for each living human.\nIn order to locate Earth, we will again leverage an exhaustive search. First, H+ decides on informal desiderata for an ‚ÄúEarth simulation.‚Äù These are likely to be as follows:\nOnce H+ has decided on the desiderata, it uses a brute force search to find a simulation satisfying them: for each possible program it instantiates a new copy of H+ tasked with evaluating whether that program is an acceptable simulation. We then define E to be a uniform distribution over programs which pass this evaluation.\nWe might have doubts about whether this process produces the ‚Äúreal‚Äù Earth‚Äìperhaps even once we have verified that it is identical according to a laundry list of measures, it may still be different in other important ways. There are two reasons why we might care about such differences. First, if the simulated Earth has a substantially different set of people than the real Earth, then a different set of people will be involved in the subsequent decision making. If we care particularly about the opinions of the people who actually exist (which the reader might well, being amongst such people!) then this may be unsatisfactory. Second, if events transpire significantly differently on the simulated Earth than the real Earth, value judgments designed to guide behavior appropriately in the simulated Earth may lead to less appropriate behaviors in the real Earth. (This will not be a problem if our ultimate definition of U consists of universalizable ethical principles, but we will see that U might take other forms.)\nThese concerns are addressed by a few broad arguments. First, checking a detailed but arbitrary ‚Äòlaundry list‚Äô actually provides a very strong guarantee. For example, if this laundry list includes verifying a snapshot of the Internet, then every event or person documented on the Internet must exist unchanged, and every keystroke of every person composing a document on the Internet must not be disturbed. If the world is well interconnected, then it may be very difficult to modify parts of the world without having substantial effects elsewhere, and so if a long enough arbitrary list of properties is fixed, we expect nearly all of the world to be the same as well. Second, if the essential character of the world is fixed but detailed are varied, we should expect the sort of moral judgments reached by consensus to be relatively constant. Finally, if the system whose behavior depends on these moral judgments is identical between the real and simulated worlds, then outputting a U which causes that system to behave a certain way in the simulated world will also cause that system to behave that way in the real world.\nOnce H+ has defined a simulation of the world which permits inspection and intervention, by careful trial and error H+ can inspect a variety of possible futures. In particular, they can find interventions which cause the simulated human society to conduct a real brain emulation project and produce high-fidelity brain scans for all living humans.\nOnce these scans have been obtained, H+ can use them to define U as the output of a new community, H++, which draws on the expertise of all living humans operating under ideal conditions. There are two important degrees of flexibility: how to arrange the community for efficient communication and deliberation, and how to delegate the authority to define U. In terms of organization, the distinction between different approaches is probably not very important. For example, it would probably be perfectly satisfactory to start from a community of humans interacting with each other over something like the existing Internet (but on abstract, secure infrastructure). More important are the safety measures which would be in place, and the mechanism for resolving differences of value between different simulated humans.\nThe basic approach to resolving disputes is to allow each human to independently create a utility function U, each bounded in the interval [0, 1], and then to return their average. This average can either be unweighted, or can be weighted by a measure of each individual‚Äôs influence in the real world, in accordance with a game-theoretic notion like the Shapley value applied to abstract games or simulations of the original world. More sophisticated mechanisms are also possible, and may be desirable. Of course these questions can and should be addressed in part by H+ during its deliberation in the previous step. After all, H+ has access to an unlimited length of time to deliberate and has infinitely powerful computational aids. The role of our reasoning at this stage is simply to suggest that we can reasonably expect H+ to discover effective solutions.\nAs when discussing discovering a brain simulation by brute force, we have skipped over some critical issues in this section. In general, brute force searches (particularly over programs which we would like to run) are quite dangerous, because such searches will discover many programs with destructive goal-oriented behaviors. To deal with these issues, in both cases, we must rely on patience and powerful safety measures.\nOnce we have a formal description of a community of interacting humans, given as much time as necessary to deliberate and equipped with infinitely powerful computational aids, it becomes increasingly difficult to make coherent predictions about their behavior. Critically, though, we can also become increasingly confident that the outcome of their behavior will reflect their intentions. We sketch some possibilities, to illustrate the degree of flexibility available.\nPerhaps the most natural possibility is for this community to solve some outstanding philosophical problems and to produce a utility function which directly captures their preferences. However, even if they quickly discovered a formulation which appeared to be attractive, they would still be wise to spend a great length of time and to leverage some of these other techniques to ensure that their proposed solution was really satisfactory.\nAnother natural possibility is to eschew a comprehensive theory of ethics, and define value in terms of the community‚Äôs judgment. We can define a utility function in terms of the hypothetical judgments of astronomical numbers of simulated humans, collaboratively evaluating the goodness of a state of affairs by examining its history at the atomic level, understanding the relevant higher-order structure, and applying human intuitions.\nIt seems quite likely that the community will gradually engage in self-modifications, enlarging their cognitive capacity along various dimensions as they come to understand the relevant aspects of cognition and judge such modifications to preserve their essential character. Either independently or as an outgrowth of this process, they may (gradually or abruptly) pass control to machine intelligences which they are suitably confident expresses their values. This process could be used to acquire the power necessary to define a utility function in one of the above frameworks, or understanding value-preserving self-modification or machine intelligence may itself prove an important ingredient in formalizing what it is we value. Any of these operations would be performed only after considerable analysis, when the original simulated humans were extremely confident in the desirability of the results.\nWhatever path they take and whatever coordination mechanisms they use, eventually they will output a utility function U‚Äô. We then define U = 0 if U‚Äô < 0, U = 1 if U‚Äô > 1, and U = U‚Äô otherwise.\nAt this point we have offered a proposal for formally defining a function U. We have made some general observations about what this definition entails. But now we may wonder to what extent U reflects our values, or more relevantly, to what extent our values are served by the creation of U-maximizers. Concerns may be divided into a few natural categories:\nWe respond to each of these objections in turn.\nIf the process works as intended, we will reach a stage in which a large community of humans reflects on their values, undergoes a process of discovery and potentially self-modification, and then outputs its result. We may be concerned that this dynamic does not adequately capture what we value.\nFor example, we may believe that some other extrapolation dynamic captures our values, or that it is morally desirable to act on the basis of our current beliefs without further reflection, or that the presence of realistic disruptions, such as the threat of catastrophe, has an important role in shaping our moral deliberation.\nThe important observation, in the defense of our proposal, is that whatever objections we could think of today, we could think of within the simulation. If, upon reflection, we decide that too much reflection is undesirable, we can simply change our plans appropriately. If we decide that realistic interference is important for moral deliberation, we can construct a simulation in which such interference occurs, or determine our moral principles by observing moral judgments in our own world‚Äôs possible futures.\nThere is some chance that this proposal is inadequate for some reason which won‚Äôt be apparent upon reflection, but then by definition this is a fact which we cannot possibly hope to learn by deliberating now. It therefore seems quite difficult to maintain objections to the proposal along these lines.\nOne aspect of the proposal does get ‚Äúlocked in,‚Äù however, after being considered by only one human rather than by a large civilization: the distribution of authority amongst different humans, and the nature of mechanisms for resolving differing value judgments.\nHere we have two possible defenses. One is that the mechanism for resolving such disagreements can be reflected on at length by the individual simulated in H. This individual can spend generations of subjective time, and greatly expand her own cognitive capacities, while attempting to determine the appropriate way to resolve such disagreements. However, this defense is not completely satisfactory: we may be able to rely on this individual to produce a very technically sound and generally efficient proposal, but the proposal itself is quite value laden and relying on one individual to make such a judgment is in some sense begging the question.\nA second, more compelling, defense, is that the structure of our world has already provided a mechanism for resolving value disagreements. By assigning decision-making weight in a way that depends on current influence (for example, as determined by the simulated ability of various coalitions to achieve various goals), we can generate a class of proposals which are at a minimum no worse than the status quo. Of course, these considerations will also be shaped by the conditions surrounding the creation or maintenance of systems which will be guided by U‚Äìfor example, if a nation were to create a U-maximizer, they might first adopt an internal policy for assigning influence on U. By performing this decision making in an idealized environment, we can also reduce the likelihood of destructive conflict and increase the opportunities for mutually beneficial bargaining. We may have moral objections to codifying this sort of ‚Äúmight makes right‚Äù policy, favoring a more democratic proposal or something else entirely, but as a matter of empirical fact a more ‚Äòcosmopolitan‚Äô proposal will be adopted only if it is supported by those with the appropriate forms of influence, a situation which is unchanged by precisely codifying existing power structure.\nFinally, the values of the simulations in this process may diverge from the values of the original human models, for one reaosn or another. For example, the simulated humans may predictably disagree with the original models about ethical questions by virtue of (probably) having no physical instantiation. That is, the output of this process is defined in terms of what a particular human would do, in a situation which that human knows will never come to pass. If I ask ‚ÄúWhat would I do, if I were to wake up in a featureless room and told that the future of humanity depended on my actions?‚Äù the answer might begin with ‚Äúbecome distressed that I am clearly inhabiting a hypothetical situation, and adjust my ethical views to take into account the fact that people in hypothetical situations apparently have relevant first-person experience.‚Äù Setting aside the question of whether such adjustments are justified, they at least raise the possibility that our values may diverge from those of the simulations in this process.\nThese changes might be minimized, by understanding their nature in advance and treating them on a case-by-case basis (if we can become convinced that our understanding is exhaustive). For example, we could try and use humans who robustly employ updateless decision theories which never undergo such predictable changes, or we could attempt to engineer a situation in which all of the humans being emulated do have physical instantiations, and naive self-interest for those emulations aligns roughly with the desired behavior (for example, by allowing the early emulations to ‚Äúwrite themselves into‚Äù our world).\nWe can imagine many ways in which this process can fail to work as intended‚Äìthe original brain emulations may accurately model human behavior, the original subject may deviate from the intended plans, or simulated humans can make an error when interacting with their virtual environment which causes the process to get hijacked by some unintended dynamic.\nWe can argue that the proposal is likely to succeed, and can bolster the argument in various ways (by reducing the number of assumptions necessary for succees, building in fault-tolerance, justifying each assumption more rigorously, and so on). However, we are unlikely to eliminate the possibility of error. Therefore we need to argue that if the process fails with some small probability, the resulting values will only be slightly disturbed.\nThis is the reason for requiring U to lie in the interval [0, 1]‚Äìwe will see that this restriction bounds the damage which may be done by an unlikely failure.\nIf the process fails with some small probability Œµ, then we can represent the resulting utility function as U = (1 ‚Äî Œµ) U1 + Œµ U2, where U1 is the intended utility function and U2 is a utility function produced by some arbitrary error process. Now consider two possible states of affairs A and B such that U1(A) > U1(B) + Œµ /(1 ‚Äî Œµ) ‚âà U1(B) + Œµ. Then since 0 ‚â§ U2 ‚â§ 1, we have:\nU(A) = (1 ‚Äî Œµ) U1(A) + Œµ U2(A) > (1 ‚Äî Œµ) U1(B) + Œµ ‚â• (1 ‚Äî Œµ) U1(B) + Œµ U2(B) = U(B)\nThus if A is substantially better than B according to U1, then A is better than B according to U. This shows that a small probability of error, whether coming from the stochasticity of our process or an agent‚Äôs uncertainty about the process‚Äô output, has only a small effect on the resulting values.\nMoreover, the process contains a humans who have access to a simulation of our world. This implies, in particular, that they have access to a simulation of whatever U-maximizing agents exist in the world, and they have knowledge of those agents‚Äô beliefs about U. This allows them to choose U with perfect knowledge of the effects of error in these agents‚Äô judgments.\nIn some cases this will allow them to completely negate the effect of error terms. For example, if the randomness in our process causes a perfectly cooperate community of simulated humans to ‚Äúcontrol‚Äù U with probability 2‚ÅÑ3, and causes an arbitrary adversary to control it with probability 1‚ÅÑ3, then the simulated humans can spend half of their mass outputting a utility function which exactly counters the effect of the adversary.\nIn general, the situation is not quite so simple: the fraction of mass controlled by any particular coalition will vary as the system‚Äôs uncertainty about U varies, and so it will be impossible to counteract the effect of an error term in a way which is time-independent. Instead, we will argue later that an appropriate choice of a bounded and noisy U can be used to achieve a very wide variety of effective behaviors of U-maximizers, overcoming the limitations both of bounded utility maximization and of noisy specification of utility functions.\nMany possible problems with this scheme were described or implicitly addressed above. But that discussion was not exhaustive, and there are some classes of errors that fall through the cracks.\nOne interesting class of failures concerns changes in the values of the hypothetical human H. This human is in a very strange situation, and it seems quite possible that the physical universe we know c"
    },
]
